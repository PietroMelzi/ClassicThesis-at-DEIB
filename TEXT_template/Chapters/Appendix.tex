\markboth{\spacedlowsmallcaps{Appendix}}{\spacedlowsmallcaps{Appendix}}
\chapter{Appendix}

\section{Maximum Likelihood Problem in Double Integrator} \label{app:maxlike}
In this section we present the maximum likelihood problem considered in the \emph{Double Integrator} task to estimate the abstract transition function of the $\delta$-\ac{MDP}. Starting from the convex program defined in \mySubsec{sec:app2.3} and motivated by the nature of noise in the task, described in \mySec{sec:mass}, we solve the following problem:
\begin{align}
&\min_{\wt{P}\in\Reals^{|\Xspace|\times|\Aspace|\times|\Xspace|}} &&-\sum_{X,X'\in\Xspace_{\Dataset}, a\in\Aspace_{X}}\log\wt{P}(X'|X,a) \nonumber\\
&\text{subject to} &&\wt{P}(X'|X,a) \geq 0 &\forall X,X'\in\Xspace_{\Dataset}, a\in\Aspace_{X} \nonumber\\
& &&\sum_{X'\in\Xspace_{\Dataset}}\wt{P}(X'|X,a) = 1 &\forall X\in\Xspace_{\Dataset}, a\in\Aspace_{X} \nonumber\\
& &&\left|\sum_{j \in J}\wt{P}(X'_{ij}|X,a) - \sum_{j \in J}\wt{P}(X'_{ij}|X,\wt{a})\right|& \nonumber \\
& && \qquad \leq L_{\wt{P}_{i}}|a- \wt{a}| &\forall i \in I,X'\in\Xspace_{\Dataset}, a\in\Aspace_{X} \label{p:i}\\
& &&\left|\sum_{i \in I}\wt{P}(X'_{ij}|X,a) - \sum_{i \in I}\wt{P}(X'_{ij}|X,\wt{a})\right|& \nonumber \\
& && \qquad \leq L_{\wt{P}_{j}}|a- \wt{a}| &\forall j \in J,X'\in\Xspace_{\Dataset}, a\in\Aspace_{X}. \label{p:j}
\end{align}
In the Double Integrator task, the state is two-dimensional and the state abstraction is performed by considering an $I \times J$ grid. We represent the single dimensions with the indexes $i \in I$ and $j \in J$, $X_{ij} \in \Xspace$ is the abstract state corresponding to the combination of $i$-th (\wrt I) and $j$-th (\wrt J) dimensions. The action is one-dimensional, hence we consider the absolute value $|a-\wt{a}|$ as measure of the action distance. In the problem we can use two (possibly) different Lipschitz constants $L_{\wt{P}_{i}}$ and $L_{\wt{P}_{j}}$, one for each dimension.\\
\newline
%Starting from the Assumption \ref{ass:lipmdp2}, we can derive the Lipschitz constant $L_{TV}$ as:
The Lipschitz constant $L_{\wt{P}_{i}}$ for a generic dimension can be derived starting from the Assumption \ref{ass:lipmdp2}. We use the \emph{Pinsker's inequality} to bound the \ac{TV} distance:
\begin{align}
\mathop{TV}\left(P(\cdot|s_{i},a), P(\cdot| s_{i},\wt{a})\right) &\leq \sqrt{\frac{1}{2}\mathop{D}_{KL}\left(P(\cdot|s_{i},a), P(\cdot| s_{i},\wt{a})\right)}, \label{eq:pink}
\end{align}
where $\mathop{D}_{KL}$ is the \emph{Kullbackâ€“Leibler divergence}. In the case of two Gaussian distributions $\Gauss_1$ and $\Gauss_2$ with the same standard deviation $\sigma$ and different means $\mu_1$ and $\mu_2$, we have:
\begin{align}
	\mathop{D}_{KL}\left(\Gauss_1, \Gauss_2\right) &= \frac{\sigma^2 + (\mu_1 - \mu_2)^2}{2\sigma^2} - \frac{1}{2} \nonumber\\
	&= \frac{(\mu_1 - \mu_2)^2}{2\sigma^2}.
\end{align}
In the Double Integrator task, a Gaussian noise $\Gauss(0, \sigma_{i})$ is added to each dimension $i$ of the arriving state. If we consider each dimension separately, as we did in the maximum likelihood problem, $P(\cdot|s_{i},a)$ is a Gaussian distribution $\Gauss(s'_{i}, \sigma_{i})$, where $s'_{i}$ is the $i$-th dimension of the arriving state in absence of noise and $\sigma_{i}$ is the standard deviation of the Gaussian zero-mean noise added to $s'_{i}$. Going back to (\ref{eq:pink}) we write:
\begin{align}
	\mathop{TV}\left(P(\cdot|s_{i},a), P(\cdot| s_{i},\wt{a})\right) \leq \Big| \frac{s'_{i} - \wt{s'}_{i}}{2\sigma_{i}}\Big|,
\end{align}
where $\Big| \frac{1}{2\sigma_{i}} \Big| = L_{TV}$. Since $L_{\wt{P}_{i}}=2L_{TV}$, the Lipschitz constant of the problem, for the $i$-th dimension, is $L_{\wt{P}_{i}} = \Big| \frac{1}{\sigma_{i}} \Big|$.