\chapter{Deterministic Policy Optimization} \label{chap:dpo}
This is the most important chapter in our work, where we present the algorithm we designed, called \acf{DPO}. The chapter is divided in four sections. In \mySec{sec:deltamdp} we provide an explanation of the approach that we followed, motivated by the aim of this work (presented in \MyChap{ch:intro} and recalled in \mySec{sec:deltamdp}), in \mySec{sec:algo} we describe the algorithm according to high-level pseudocode of it. Details on the implementation of the different parts of the algorithm are provided in \mySec{sec:detail}, while in \mySec{sec:absmdp} we explain how to address the concrete issues that arise in the algorithm, related to the necessity of function estimations.
 
\section{Passive Exploration via $\delta$-MDPs}\label{sec:deltamdp}
%Passive exploration
As mentioned in \MyChap{ch:intro}, the goal of this work is to ensure that the agent is able to learn a deterministic policy avoiding the execution of dangerous actions throughout the learning phase (in this way the agent does not harm itself or the environment in which it evolves). We address this requirement on exploration by forcing the agent to gather the samples needed for improving the parametric policy \emph{without performing any random action}. A way to satisfy this strict constraint is to learn a deterministic policy and use it to collect samples so that no random actions can be performed at all. By doing this, the exact action that will be performed by the agent is known in every moment. 
This choice, unfortunately, prevents any form of active exploration and the possibility to update the parametric policy with existing algorithms. The fundamental reason is that, by observing a single action for each (continuous) state and without any probability measure over actions, the agent has no way of preferring one action over another for that particular state, which makes policy optimization impossible.\\
\newline 
However, if the environment is sufficiently regular, we can exploit a form of \emph{passive} exploration. 
The key idea is to aggregate states in such a way as to observe a variety of actions within these larger, abstract states. This variety is obtained by considering, for every abstract state, the set of actions chosen by our deterministic policy in the different states belonging to the same aggregate. In a sense, aggregation allows transferring the diversity of the states visited by the agent (both over different time-steps of the same episode and over independent episodes) into actions. Variety over states is encoded by the future-state distribution $\delta^{\vtheta}$, depending on the deterministic policy $\pi^{\vtheta}$, and is fueled by stochastic transitions and random restarts that allow to ideally cover the entire state space $\Sspace$.
The price to pay in order to consider state aggregation is basically a discretization error, that could be, in general, unbounded. However, in Lipschitz \ac{MDPs} we can keep this error under control, as we will show in \MyChap{chap:proofs}.\\
\newline
In a Lipschitz \ac{MDP} (see \mySubsec{sub:lipmdp} for details on it), if we know the effect of performing an action $a$ in a state $s$, we can exploit this knowledge to obtain information about the effect of performing the same action $a$ on a different state $\wt{s}$.
As an example we can consider a deterministic environment\footnote{An environment in which the transition function $P$ is a Dirac delta function.} in which we collect a sample $(s,a,s')$, where the next-state $s'$ is obtained from a state-action pair $(s, a)$. \hl{In deterministic environments, the next state $s'$ is computed as $s' = f(s,a)$ according to the function $f\colon\Sspace\times\Aspace\to\Sspace$ that conducts the state transition from $s$ to $s'$}. \hl{For a deterministic environment,} the assumption of Lipschitz regularity on the transition function $P$ defined in (\ref{eq:liptf}) can be rewritten as:
\begin{align}
	\norm{f(s,a)-f(\wt{s},\wt{a})} \leq L_{P}d_{\Sspace\Aspace}\left((s,a),(\wt{s},\wt{a})\right),
\end{align}
for some positive real constant $L_{P}$. From this inequality, the next-state $\wt{s'}$ resulting from an initial state-action pair $(\wt{s}, \wt{a})$ can be estimated even if the agent \hl{never executes the action} $\wt{a}$ in the state $\wt{s}$. \hl{The estimation of $\wt{s'}$ comes in the form of an interval, we show how it can be derived in the case of one-dimensional states and action spaces:}
\begin{align}
	-L_{P}d_{\Sspace\Aspace}\left(\cdot\right) &&\leq&& f(s,a)-f(\wt{s},\wt{a}) &&\leq&& &L_{P}d_{\Sspace\Aspace}\left(\cdot\right) \nonumber\\
	%
	f(s,a) -L_{P}d_{\Sspace\Aspace}\left(\cdot\right) &&\leq&&  \qquad f(\wt{s},\wt{a}) \qquad  &&\leq&& &f(s,a) + L_{P}d_{\Sspace\Aspace}\left(\cdot\right) \nonumber\\
	%
	s -L_{P}d_{\Sspace\Aspace}\left(\cdot\right) &&\leq&&  \qquad \wt{s'} \qquad  &&\leq&& &s + L_{P}d_{\Sspace\Aspace}\left(\cdot\right),
\end{align}
\hl{where $d_{\Sspace\Aspace}\left(\cdot\right) = d_{\Sspace\Aspace}\left((s,a),(\wt{s},\wt{a})\right)$ is the taxicab norm on $\Sspace \times \Aspace$ considered in} (\ref{eq:liptf}). \hl{Details on how to derive the intervals in the case of multi-dimensional states are provided in} \mySec{sec:absmdp}.
The \hl{tightness of the estimated intervals, and then their precision, increases according to the similarity between the state-action pairs $(s, a)$ and $(\widetilde{s}, \widetilde{a})$ used to build the intervals.
It appears to be a good idea, starting from an observed state-action pair $(s,a)$, to estimate the effect of executing the action $a$ in all the states $\wt{s}$ visited by the agent that are considered similar to $s$. In this way, the agent only performs the actions suggested by the deterministic policy and estimates intervals that can be deemed sufficiently accurate.} For a more detailed explanation of how these simple considerations are used to estimate \hl{the unknown information} required by the algorithm, refer to \mySec{sec:absmdp}.
%The information we gain from Lipschitz properties is in the form of bounds. For instance, if next-state $s'$ is obtained from observed state-action pair $(s, a)$, Lipschitz conditions provide a spherical region containing \emph{for sure} the state $\widetilde{s'}$ reached from unseen state-action pair $(\widetilde{s}, \widetilde{a})$, assuming deterministic transitions and Euclidean metric. The more similar the two pairs $(s, a)$ and $(\widetilde{s}, \widetilde{a})$ are, the more precise our region will be, and more data can be used to refine our knowledge by intersecting the corresponding regions. 
%In the continuous bandit (\ie single-state) setting,~\citet{kleinberg2019bandits} provide a way of constructing such regions via directed exploration that is optimal in terms of regret. Albeit deterministic, the resulting behavior would be erratic, and hardly representable with a simple parametric controller.
%The abstract MDP
\subsection{$\delta$-\ac{MDPs}}
We are going to exploit the ideas derived above by building an abstract \ac{MDP} in which the state space is composed by aggregates of states. We divide the original state space $\Sspace$ into a partition $\Xspace$. To take advantage of the Lipschitz continuity of our environment, we need to aggregate states taking their mutual distance into account. For this reason, we only consider \textit{tessellations} of the state space, such as grids. 
%
We denote as $D(X) = \sup_{s,\wt{s}\in X}\norm{s-\wt{s}}$ the diameter of an abstract state $X\in\Xspace$.
We want that the transition and the reward functions of our abstract MDP are able to model the overall effect of actions on the abstract states, so that the optimization in the abstract \ac{MDP} corresponds to the resolution of the original policy optimization problem. \hl{Doing this exactly would require additional assumptions, like bisimulation (details in} \mySec{sec:stdisc}\hl{), which we deem too restrictive. Indeed, bisimulation is an \emph{equivalence relation} that allows to obtain a partition of the state space composed by \emph{equivalence classes} where the optimal values and the optimal policies of the original} \ac{MDP} \hl{are preserved} [\cite{givan2003equivalence}], \hl{however this assumption is too stringent and difficult to obtain.} [\cite{ferns2012metrics}] \hl{shows with an example that in a simple} \ac{MDP} \hl{with only fours states, the constraints on the transition function $P$ needed to ensure bisimulation are already strict. Hence, we design a solution that approximates the resolution of the original problem.} To achieve this purpose, we consider the future-state distribution $\delta^{\vtheta}$ in order to define the weighting function \hl{for the state abstraction (details in} \mySec{sec:stdisc}), obtaining:
%
\begin{align}
	&\widetilde{R}_{\vtheta}(X,a) = \int_{X}\frac{\delta^{\vtheta}(s)}{Z_{\vtheta}(X)}R(s,a)\de s,\label{eq:abrew2} \\
	&\widetilde{P}_{\vtheta}(X'|X,a) = \int_{X} \frac{\delta^{\vtheta}(s)}{Z_{\vtheta}(X)} \int_{X'} P(s'|s,a)\de s\de s',\label{eq:abtran2}
\end{align}
%
\sloppy with weighting function $w(s)=\delta^{\vtheta}(s)/Z_{\vtheta}(X)$, where $Z_{\vtheta}(X)=\int_{X}\delta^{\vtheta}(s)\de s$ is a normalization factor necessary to ensure that the weights sum to one.
This completes the definition of our abstract \ac{MDP}, called $\delta$-MDP in the following to stress the fundamental role of the future-state distribution $\delta^{\vtheta}$ in its definition:
%
\begin{definition}\label{def:abmdp}
	Given an MDP $\langle\Sspace, \Aspace, P, R, \gamma\rangle$, a policy $\pi_{\vtheta}$, and a partition $\Xspace$ of $\Sspace$, the corresponding $\delta^{\vtheta}$-MDP is $\langle\Xspace, \Aspace, \wt{P}_{\vtheta}, \wt{R}_{\vtheta}, \gamma\rangle$, where $\widetilde{R}_{\vtheta}$ is defined as in Equation~\eqref{eq:abrew2} and $\wt{P}_{\vtheta}$ as in Equation~\eqref{eq:abtran2} for all $X,X'\in\Xspace$ and $a\in\Aspace$.
\end{definition}
%
\noindent An initial-state distribution for the $\delta$-\ac{MDP} can be defined as $\wt{p}_0(X) = \int_{X}p_0(s)\de s\quad\forall X\in\Xspace$, where $p_0$ is the initial-state distribution for the original \ac{MDP}.
%
A (stationary\footnote{The policy does not changes over time}, Markovian) abstract policy $\rho: \Xspace \to \Delta(\Aspace)$ is a (possibly stochastic) mapping from abstract states to actions. In order not to confuse them with concrete policies $\pi$, we denote them with the letter $\rho$. Fixed the abstract \ac{MDP}, we can define value functions $V^{\rho}$ and $Q^{\rho}$, future-state distribution $\delta^{\rho}$ (over $\Xspace$) and optimal policy $\rho^{*}$, as in any \ac{MDP}. Note that there is no direct correspondence, in general, between concrete and abstract policies, since the behavior of a concrete policy may appear non-Markovian in the abstract \ac{MDP}~\citep{lihong2006towards}.
However, we introduce a simple "concretization" operator $\Conc: \rho \mapsto \pi$ from abstract policies to concrete policies, defined as
%\begin{align}
$(\Conc\rho)(s) = \rho(\Gamma(s))$
%\end{align}
for any $\rho:\Xspace\to\Aspace$ and $s\in\Sspace$. The resulting concrete policy $\pi = \Conc\rho$ performs the same action $\rho(X)$ for all states $s\in X$, hence it is a piece-wise constant function.

\section{Deterministic Policy Optimization}\label{sec:algo}
In this section, we show how to use the $\delta$-\ac{MDP} defined in Section~\ref{sec:deltamdp} to approximately solve the policy optimization problem, under Lipschitz conditions but without access to random actions.
%
The proposed methodology is as follows: given the original \ac{MDP} and a deterministic parametric policy $\pi_{\vtheta}$, we build the corresponding  $\delta^{\vtheta}$-\ac{MDP} using only the data collected with $\pi_{\vtheta}$ itself. This requires estimating the abstract reward and transition functions. To do so, we can actively exploit the Lipschitz conditions, as discussed in the next sections (\mySec{sec:detail}, \mySec{sec:absmdp}).\\
\newline
The solution of the $\delta^{\vtheta}$-\ac{MDP} is a deterministic optimal abstract policy $\rho^{*}$, which is guaranteed to exist and maximizes $V^{\rho}(X)$ for all $X\in\Xspace$. This abstract policy can be emulated in the original \ac{MDP} by $\Conc\rho^{*}$, the piecewise-constant policy, even if in general $\Conc\rho^{*}$ \hl{is not optimal for the original} \ac{MDP} \hl{and} may not belong to the original parametric policy space. Hence, we need to project it back as $\pi_{\vtheta'} = \Proj_{\Pi_{\Theta}}(\Conc\rho^{*})$, where $\Proj$ is a projection operator. The new parameter vector $\vtheta'$ identifies the novel policy that we are going to run in the environment. The procedure can then be repeated. We call this general method Deterministic Policy Optimization (\ac{DPO} for short), and we outline (at high level) the different phases of it in Algorithm~\ref{alg:dpo}. Details on the implementation of the single phases are described in \mySec{sec:detail}.
We provide a theoretical justification of this approach in \MyChap{chap:proofs}.
%
%
\begin{algorithm}[t]
	\caption{DPO}
	\label{alg:dpo}
	\begin{algorithmic}[1]
		\STATE {\bfseries Input:} policy class $\Pi_{\Theta}$, initial policy parameter $\vtheta$, batch size $N$
		\STATE Partition the state space into $\Xspace$
		\FOR{$t=0,1,\dots$}
		\STATE Collect $N$ samples with $\pi_{\vtheta}$ 
		\STATE Estimate $\wt{R}_{\vtheta}$ and $\wt{P}_{\vtheta}$ over $\Xspace$
		\STATE Solve the $\delta^{\vtheta}$-MDP to find optimal abstract policy $\rho^{*}$
		\STATE Project $\Conc\rho^{*}$ back into $\Pi_{\Theta}$ to find $\vtheta'$
		\STATE $\vtheta\gets\vtheta'$
		\ENDFOR
	\end{algorithmic}
\end{algorithm}
%

\section{Algorithmic Details}\label{sec:detail}
In this section, we provide further details on our implementation of the different phases of \ac{DPO}, outlined in Algorithm~\ref{alg:dpo}: state aggregation (or state partition), estimation of $\wt{R}_{\vtheta}$ and $\wt{P}_{\vtheta}$ for the creation of the $\delta$-\ac{MDP}, solution of it and projection of $\Conc\rho^{*}$ into $\Pi_{\Theta}$.

%\subsection{State Discretization}
\subsection{State aggregation}
We discretize the state space $\Sspace\subseteq\Reals^N$ into a regular grid $\Xspace$, in which each hyperrectangular cell is an abstract state. 
We assume that the domain of each of the $N$ state variables is a continuous interval. If a variable is unbounded or the bounds are unknown, they are set equal to the minimum and maximum value on the corresponding dimension, observed among the collected samples. 
For each dimension $i \in N$, it is provided in input \hl{to the algorithm} a scalar $k$ that indicates the number of subsets in which the $i-th$ dimension of the state space $\Sspace$ has to be divided. We consider \hl{each dimension $i$ separately from the others} and we divide \hl{the} one-dimensional space in a partition of $k$ convex subsets with constant diameter. \hl{For instance, we consider a two-dimensional space in the domain $[2; 4] \times [-4;4] \subset \mathbb{R}^{2}$ and a vector $\boldsymbol{k} = [2,4]$ representing the number of subsets in which each dimension has to be divided. The first dimension is divided in two subsets having domains $[2;3)$ and $[3;4]$, the second dimension is divided in four subsets having domains $[-4;-2),[-2;0),[0;2)$ and $[2;4]$. Hence, the original state space $\Sspace$ is divided in $8$ subsets $X \in \Xspace$, the domain of each subset $X$ is the combination of one intervals from the first dimension and one from the second dimensions.} By partitioning in this way, the measure of \hl{the subsets'edges} can vary a lot among the different state-dimensions.
If the original state space $\Sspace$ contains an absorbing state, we add an absorbing abstract state to $\Xspace$. This can be useful for modeling indefinite-horizon tasks.\\
\newline
The state space partition is not required to be the same across the different iterations of the algorithm, since a new $\delta$-MDP is built at each iteration. For instance, \hl{if some state-dimensions are unbounded and we establish the minimum and maximum possible values for them based on the collected samples at each iteration, the size of the subsets in the partition changes between iterations, according to the minimum and maximum values considered.} \hl{An adaptive discretization of the state space,} \ie \hl{a discretization that changes between iterations based on the evolving situation, can provide benefits such as more efficient use of the collected samples.} However, for computational reasons we can avoid to compute a new partition of the state space at the beginning of each iteration and keep a fixed one across the multiple iterations. In the latter case, if we collect a samples whose starting state is outside from the grid, the sample can be assigned to the closest cell in the partition.
If naively implemented, this aggregation strategy is clearly subject to the \textit{curse of dimensionality}. However, only the abstract states that are actually visited by $\pi_{\vtheta}$ are considered in the next steps of the algorithm, so the size of the $\delta$-\ac{MDP} is polynomial in the number of collected samples\footnote{Efficiently exploring high-dimensional state spaces is still a difficult problem: \hl{considering only the visited abstract states allows to contain the computational complexity of the algorithm but does not reduce the difficulty of covering an high-dimensional state space with exploration.}}.  
More details on how the abstract state space $\Xspace$ is built \hl{in the experiments} and how the chosen aggregation strategy affects the performances of the algorithm are provided in \MyChap{chap:exp}.

%\subsection{$\delta$-MDP Estimation}
\subsection{Abstract MDP estimation}\label{sec:mdpest}
A fundamental aspect in the creation of the $\delta$-\ac{MDP} is the estimation of the $\gamma$-discounted future-state distribution $\delta^{\vtheta}(s)$\hl{, which we avoid to explicitly perform because it would require too many samples to obtain an accurate estimation.} \todo{specificare infinite examples to cover the state space? oppure basta un numero finito di samples, seppur alto per avere una stima accurata?}
To construct the $\delta^{\vtheta}$-\ac{MDP}, given the current policy $\pi_{\vtheta}$, we need to estimate the abstract reward function $\wt{R}_{\vtheta}$ and the abstract transition function $\wt{P}_{\vtheta}$. As we previously said, the weighting function $w(s)$ that weighs the abstract functions $\wt{R}_{\vtheta}(X, a)$ and $\wt{P}_{\vtheta}(X'|X, a)$ of the $\delta$-\ac{MDP} is defined according to $\delta^{\vtheta}(s)$. The exact computation of the abstract functions would require knowledge of the future state distribution $\delta^{\vtheta}$, which is out of reach. 
However, we can write the abstract functions as expected values:
\begin{align}
\wt{R}_{\vtheta}(X, a) &= \EV_{s \sim p(s | X)} R(s, a) \label{eq:mcr}\\
\wt{P}_{\vtheta}(X' | X, a) &= \EV_{s \sim p(s | X)} \int_{X'} P(s' | s, a) \de s'\label{eq:mcp}
\end{align}
where $p(s | X) = Z_{\vtheta}(X)^{-1}\delta^{\vtheta}(s)\Ind_X(s)$ is the probability of visiting state $s$ conditioned by $s\in X$, and $\Ind_X(\cdot)$ is the indicator function for set $X$.
In this way, we can simply estimate the abstract function via Monte Carlo estimation using the samples collected with $\pi_{\vtheta}$, since the visited states are distributed as $\delta^{\vtheta}$.
%\begin{align*}
%\wt{R}_{\vtheta}(X, a) &= \frac{1}{|\widetilde{s} \in X|} \sum_{\widetilde{s} \in X}R(\widetilde{s}, a)\\
%\wt{P}_{\vtheta}(X'|X,a) &= \frac{1}{|\widetilde{s} \in X|} \sum_{\widetilde{s} \in X} \int_{x'} P(s' | \widetilde{s}, a) \,ds
%\end{align*}
%The estimated $\delta^{\pi}(s)$ results to be equal to $\frac {|s|} {|\widetilde{s} \in X|}$, where $s \in X$ and we indicate with $\widetilde{s}$ a generic sampled state.
While this is enough for $\wt{R}_{\vtheta}(X, a)$, since $R(s, a)$ is usually known for all the state-action pairs $(s,a)$ or designed by humans, estimating $\wt{P}_{\vtheta}(X'|X, a)$ for all the abstract states and actions without any knowledge of $P(\cdot|s, a)$ requires more effort. All we get from the agent-environment interactions is a finite set of samples $(s,a,s')$. This means that, for almost all the actions in $\Aspace$, we do not have any samples. Fortunately, we can leverage our Lipschitz assumptions to fill in this missing information.\\
\newline
We first consider deterministic environments, where $P(\cdot|s, a)$ is a Dirac delta function that exactly identifies the next state $s'$ resulting from a state-action pair $(s,a)$, to be more precise:
\begin{align}
P(s'|s,a)= 
\begin{cases}
1 & \text{if } s'=f(s,a)\\
0 & \text{otherwise}
\end{cases}
\end{align}
For simplicity, we denote this deterministic mapping as $f:\Sspace\times\Aspace\to\Sspace$ and write $s'=f(s,a)$.
Let $\Delta(s, a) \coloneqq f(s,a) - s$ be the state difference obtained transitioning from $s$ to $s'$. From Assumption~\ref{ass:lipmdp} on Lipschitz \ac{MDPs} and fixed an action $a\in\Aspace$, we obtain:
%
\begin{align}\label{eq:deltas}
\norm{\Delta (s, a) - \Delta (\widetilde{s}, a)} \leq L_{\Delta}\norm{s - \widetilde{s}},
\end{align}
%
for all $s,\wt{s}\in\Sspace$, hence the state difference $\Delta$ is also Lipschitz continuous. A valid Lipschitz constant $L_{\Delta}$ that can be obtained from Assumption~\ref{ass:lipmdp} is $(1+L_{P})$, as we show below. Since the Kantorovich distance between two Dirac delta functions is equal to the distance of their locations, we can use Assumption~\ref{ass:lipmdp} to obtain a Lipschitz constant $L_{\Delta}$ for state-difference function $\Delta$. For every $a\in\Aspace$ and $s,\wt{s}\in\Sspace$:
\begin{align}
\norm{\Delta(s,a)-\Delta(\wt{s},a)} &=
\norm{f(s,a) - s - f(\wt{s},a) + \wt{s}} \nonumber\\
&\leq \norm{f(s,a)-f(\wt{s},a)} + \norm{s-\wt{s}} \label{p:23}\\
&= \Kant\left(P(\cdot|s,a), P(\cdot| \wt{s},a)\right) + \norm{s-\wt{s}} \nonumber\\
&\leq L_{P}\norm{s-\wt{s}} + \norm{s-\wt{s}} \label{p:22}\\
&\leq L_{\Delta} \norm{s-\wt{s}} \qquad \text{ with } L_{\Delta} = \left(1 + L_{P}\right), \label{p:18}
\end{align}
where~\eqref{p:23} is from the triangle inequality and~\eqref{p:22} is from Assumption~\ref{ass:lipmdp}.\\
\newline
Under additional assumptions, such as the linearity of the transition function $f(s,a)$, we can obtain constants lower than one or even equal to zero. The latter happens when the state difference depends only on the action, which can be realistic, or at least a good approximation, for some continuous control problems. We discuss these different settings and provide detailed computations of the Lipschitz constants in \mySec{sec:absmdp}.
%
When $L_{\Delta}=0$, we can easily generate extra "fictitious" samples. For instance, given sample $(s,a,s')$, we can generate $(\wt{s},a,\wt{s}')$ for any other state $\wt{s}\in\Sspace$ by setting $\wt{s}' = \wt{s} + (s'-s)$. \hl{Indeed, from the inequality in} (\ref{p:18}) \hl{and considering $L_{\Delta}=0$, we obtain $\Delta(s,a)=\Delta(\wt{s},a)$.} In this way, for each action $a$ performed by policy $\pi_{\vtheta}$ in abstract state $X$, we can have several (fictitious) samples (one for each visited state $s\in X$). With these samples, we can simply estimate $\wt{P}_{\vtheta}(X'|X,a)$ as the ratio of the next states that fall into $X'$ when we consider samples and fictitious samples involving any state $s \in X$ and the action $a$. \hl{To be more precise, given a batch $B^{*}$ of samples collected with $\pi_{\vtheta}$ and fictitious samples, the abstract transition function $\wt{P}_{\vtheta}$ is estimated as:}
\begin{align}
	\wt{P}_{\vtheta}(X'|X,a) = \frac{|(s,a,s') \in B^{*}: s \in X, s' \in X'|}{|(s,a,s') \in B^{*}: s \in X|}.
\end{align}
When $L_{\Delta}\neq 0$, we can still use~\eqref{eq:deltas} to get a region containing $\wt{s}'$.
%\begin{align}
%	\norm{\wt{s}' - \left(\wt{s}+(s-s')\right)} \leq L_{\Delta}\norm{s-\wt{s}}.
%\end{align}
From this, we can estimate lower and upper bounds on the transition function $\wt{P}$. As a result, the $\delta$-\ac{MDP} will be a \ac{BMDP} [\cite{givan2000bounded}] instead of a regular one.
%
In the case of stochastic environments, we follow a different approach for estimating the abstract transition kernel. We define a maximum-likelihood problem using data sampled from $\pi_{\vtheta}$ \textit{and} fictitious samples. We then add special constraints to this problem as to enforce the Lipschitz continuity of $\wt{P}_{\vtheta}(X'|X,a)$ \wrt actions. The resulting problem is still convex and can be solved with standard optimization tools.
%the estimation of $\wt{P}_{\vtheta}(X'|X, a)$ is performed by solving a constrained optimization problem whose goal is to maximize the likelihood of the samples collected. The constraints reflect the Lipschitz properties of $\wt{P}_{\vtheta}(X'|X,a)$: the distance between $\wt{P}_{\vtheta}(X'|X, a)$ and $\wt{P}_{\vtheta}(X'|X, \widetilde{a})$ is bound by a $f(d_A(a, \widetilde{a}))$.
Details on \ac{BMDP} are provided in \mySubsec{subsec:bmdp}, details on the algorithm involving \ac{BMDP}s and the constrained optimization approach are provided in \mySec{sec:absmdp}. 

%\subsection{Value Iteration on $\delta$-MDP}
\subsection{Solving the abstract MDP}
In the previous paragraph, we have proposed a way to estimate the abstract transition function for state-action pairs not experienced by the agent, by exploiting the assumptions about the regularity of the environment. However, this still applies only to the actions actually performed by the agent. As a result, our approximation of the $\delta$-\ac{MDP} has a finite action set. This introduces a further model bias, but allows us to employ dynamic programming to find the optimal abstract policy. This kind of error can be reduced by increasing the number of collected episodes, or the control frequency. Furthermore, under our Lipschitz assumptions, the finite actions actually performed by the agent are supposedly good representatives for the other ones.\\
\newline
We solve this approximate $\delta$-\ac{MDP} via value iteration (\mySubsec{subsec:dp}), with a stopping condition (introduced for computational reasons) on the max-norm distance between consecutive state-value functions.
The value iteration on the $\delta$-\ac{MDP} is stopped when, said $V_k(X)$ the value function of state $X$ at the $k-th$ iteration and $\epsilon$ the constant used as a threshold in the algorithm, 
\begin{align}
|V_k(X) - V_{k-1}(X)| < \epsilon \quad \forall X \in \Xspace.
\end{align}
In the case of multiple optimal policies, we keep track of the entire set of optimal actions for each abstract state \hl{$X$ so that we can set, for every state $s \in X$, a possibly different optimal action of $X$ as target in the regression problem described in} \mySubsec{subsec:proj}. \hl{We do this to minimize the difference between the deterministic policy $\pi_{\vtheta}$ before and after the update of its parameter $\vtheta$, while we consider for the update only actions that are optimal in $\Xspace$.}
We may need to evaluate abstract states for which no samples have been collected by the agent. We propose a risk-averse solution, which consists of considering these "unknown" abstract states as absorbing and set their value to the lowest known value. 
%This strategy follows a risk-minimization principle: to discourage actions that could bring the agent in unexplored macrostates. If the macrostate is worth it, the agent should be able to reach it in more time but in a safe manner. 

%\subsection{Projection on Parametric Space $\vtheta$}
\subsection{Projection} \label{subsec:proj}
Given the optimal abstract policy $\rho^*$, its concretization $\Conc\rho^*$ is easily obtained by copying the action selected by $\rho^{*}$ for all the states of the same abstract state. We then need to represent $\Conc\rho^{*}$ in the space $\Pi_{\Theta}$ of the original policy $\pi_{\vtheta}$. This projection phase can be approached as a regression problem, where the target for $\pi_{\vtheta'}(s)$ is $\Conc\rho^*(s)$. The problem consists in finding the parameter $\vtheta'$ that minimizes the following expression:
\begin{align}\label{eq:regress}
\norm[\delta^{\vtheta}]{\Conc\rho - \pi_{\vtheta'}} 
+ \lambda\norm{\vtheta'-\vtheta},
\end{align} 
where the expression is evaluated for all the collected samples $(s,a,s')$. 
When the abstract optimal policy $\rho^{*}$ prescribes multiple optimal actions for a certain abstract state $X \in \Xspace$, in order to facilitate the projection, we consider for each sample $(s,a,s')$, with $s \in X$, that $\Conc\rho(s)$ is the action with the smallest euclidean distance from the action prescribed by $\pi_{\vtheta}(s)$, among the optimal actions related to the abstract state $X$.\\
\newline
The first term from~\eqref{eq:regress} is the \acf{RMSE} loss, weighted by the future state distribution $\delta^{\vtheta}$. For differentiable policies (including neural networks), this loss can be minimized via gradient descent from the samples collected with $\pi_{\vtheta}$. The second term from~\eqref{eq:regress} penalizes solutions that are too far from the current policy parameters and it can be used as a regularization term in the regression problem. In practice, the regularization coefficient $\lambda$ can be tuned as a meta-parameter. Note that, without this projection step, there would be no room for further policy improvement, as the next $\delta$-\ac{MDP} would have just one action per abstract state. The projection error actually allows the agent to evaluate new actions and visit new regions of the state space. 

\section{Abstract Transition Function construction}\label{sec:absmdp}
In this section, we provide additional details on the \hl{estimation} of the $\delta$-MDP outlined in Section~\ref{sec:mdpest} \hl{and used to exploit passive exploration and update the deterministic policy of the agent.} We \hl{describe} the \hl{different approaches that we propose to estimate} the abstract transition function $\wt{P}$, \hl{these approaches} depend on the nature of the environment considered in the task. \hl{We inspect environments with an increasing level of difficulty: in} \mySubsec{app:b1} \hl{we derive two Lipschitz constants that can be useful to estimate the arriving state in deterministic linear environments, in} \mySubsec{app:b2} \hl{we propose to estimate the $\delta$-}\ac{MDP} with a \ac{BMDP} \hl{when the environment is deterministic but non-linear, in} \mySubsec{sec:app2.3} \hl{we build a constrained optimization problem to ensure a property of $\wt{P}$ in the case the environment is stochastic, at last in} \mySubsec{subsec:app2.4} \hl{we present a specific task involving a stochastic environment.}

\subsection{Deterministic linear environments}\label{app:b1}
\noindent We call \textit{linear} a deterministic environment in which $f(s, a)$ is linear \wrt the input variables $s$ and $a$. For these environments, we can write $f(s,a) = As + Ba$, where $A$ and $B$ are (possibly unknown) constant matrices. With $f(s,a)$ defined in this way, we can obtain a Lipschitz constant for $\Delta$ possibly smaller than the one derived in (\ref{p:18}):
\begin{align}
\norm{\Delta(s,a)-\Delta(\wt{s},a)} &= \norm{As+Ba-s - \left(A\wt{s}+Ba-\wt{s}\right)} \nonumber \\
&= L_{\Delta} \norm{s-\wt{s}} \qquad \text{ with } L_{\Delta}=\norm{A-\mathbb{I}}, \label{p:20}
\end{align}
\hl{where $\norm{A-\mathbb{I}}$ is the infinity matrix norm of $A-\mathbb{I}$.}
In general, the constant obtained in~\eqref{p:20} can be smaller than $\left(1 + L_{P}\right)$ and even equal to zero. When $\Delta(s,a)$ depends only on the performed action $a$ and not on the starting state $s$,  $A=\mathbb{I}$ and $L_{\Delta}=0$. From~\eqref{p:18}, if we consider $L_{\Delta} = 0$ instead of $L_{\Delta} = \left(1 + L_{P}\right)$, we obtain the equality $\Delta(s,a) = \Delta(\wt{s}, a)$ that allows to exactly predict the arriving state $\wt{s'}=f(\wt{s}, a)$ as $\wt{s'} = \wt{s} + \Delta(s,a)$.\\
\newline
Unfortunately, when $L_{\Delta} \neq 0$ we cannot generate "fictitious" samples exactly. However, we exploit the inequality in~\eqref{p:18} to obtain an interval (or a higher-dimensional region) including the unknown $\wt{s'}=f(\wt{s},a)$, instead of the exact point $\wt{s'}$. Here we show how to obtain the interval in the simple case of one-dimensional states:
\begin{align}
-L_{\Delta} \left|s - \wt{s}\right| &&\leq&& \Delta s (s,a) - \Delta s (\wt{s},a) &&\leq&& &L_{\Delta} \left|s - \wt{s}\right| \nonumber\\
%
\Delta(s,a) - L_{\Delta} \left|s - \wt{s}\right| &&\leq&& \qquad \Delta(\wt{s},a) \qquad &&\leq&& &\Delta(s,a) + L_{\Delta} \left|s - \wt{s}\right| \nonumber\\
%
\Delta(s,a) - L_{\Delta} \left|s - \wt{s}\right| &&\leq&& \qquad f(\wt{s},a) - \wt{s} \qquad &&\leq&& &\Delta(s,a) + L_{\Delta} \left|s - \wt{s}\right|. \label{p:19}
\end{align}
From~\eqref{p:19} we derive:
\begin{align}
	\wt{s'}=f(\wt{s}, a) \in \Big[\wt{s} + \Delta(s,a) - L_{\Delta} \left|s - \wt{s}\right|, \quad \wt{s} + \Delta(s,a) + L_{\Delta} \left|s - \wt{s}\right|\Big],
\end{align}
where $L_{\Delta} = \norm{A - \mathbb{I}}$.
When the state space is $n-$dimensional with $n\geq 2$, the interval that we estimate is an hyperrectangle.\\
%Each side of this hyperrectangle represents a mono-dimensional interval that includes the $i-th$ dimension of the state $\wt{s'}$, where $i \in \left[0, n-1\right]$.\\
\newline
In tasks where a lot of precision is required in order to estimate $\wt{s'}$, the computed intervals could be not tight enough to represent $\wt{s'}$ with the required accuracy. We can increase the precision by computing a second interval on each $\wt{s'}$ and considering the intersection of the two intervals on $\wt{s'}$ as the most precise estimate. The second interval is calculated by considering two state-action pairs having the \emph{same} state and different \emph{actions}:
\begin{align}
\norm{\Delta(\wt{s},a) - \Delta(\wt{s}, \wt{a})} &= \norm{A\wt{s}+Ba-\wt{s} - \left(A\wt{s}+B\wt{a}-\wt{s}\right)} \nonumber \\
&= L_{\Delta(a)} \norm{a-\wt{a}} \qquad \text{ with } L_{\Delta(a)}=\norm{B}. \label{p:21}
\end{align}
Similarly to~\eqref{p:19}, we obtain the new interval containing $\wt{s'} = f(\wt{s}, a)$:
\begin{align}
\wt{s'} \in \Big[\wt{s} + \Delta(\wt{s},\wt{a}) - L_{\Delta (a)}\norm{a - \wt{a}}, \quad \wt{s} + \Delta(\wt{s},\wt{a}) + L_{\Delta (a)}\norm{a - \wt{a}} \Big],\label{p:25}
\end{align}
with $L_{\Delta (a)}$ = $\norm{B}$.\\
\newline
Hence, given (real) samples $(s_1,a_1,s_1')$ and $(s_2, a_2, s_2')$, we can generate fictitious next states for both $(s_1,a_2)$ and $(s_2,a_1)$, intersecting two intervals for each of them.

\subsection{Deterministic non-linear environments} \label{app:b2}
%If the environment is non-linear, we can consider $L_{\Delta} = \left(1 + L_{P} \right)$ or we can estimate a smaller $L_{\Delta}$ from the samples and use it to compute the intervals on $\wt{s'}$ in the same way we showed for linear environments. Even if we can no longer consider $f(s,a)$ as a linear function in $s$ and $a$, the environment still satisfies the Lipschitz properties of regularity. If the constants $L_{\Delta}$ estimated from the samples are smaller than the real one, the intersection of two estimated intervals related to some $\wt{s'}$ can be empty. When it happens, we simply discard the intervals and do not generate the fictitious sample.
When the (deterministic) transition function is non-linear, we are not able to exactly predict the next state of non-sampled state-action pairs. However, we can still exploit the Lipschitz assumptions to obtain upper and lower bounds (or multi-dimensional regions) for the next-states. To do so, we can consider $L_{\Delta} = \left(1 + L_{P} \right)$, as shown in~\eqref{p:18} for general deterministic transitions, or we can estimate a smaller $L_{\Delta}$ from data.
The result is a \emph{bounded-parameter} abstract \ac{MDP} (see \mySubsec{subsec:bmdp} for details) instead of a regular one.\\
\newline
If we have $L_{\Delta} \neq 0$ and we estimate with~\eqref{p:18} the fictitious arriving state $\wt{s'} = f(\wt{s}, a)$, where $(\wt{s},a)$ is an unseen state-action pair, we obtain a region including $\wt{s'}$ instead of the exact point $\wt{s'}$. The information provided by the estimation of the fictitious arriving states is not sufficient to compute an exact $\wt{P}_{\vtheta}(X'|X,a)$ for each abstract state-action pair $(X,a)$ of the $\delta$-\ac{MDP}. The value of each $\wt{P}_{\vtheta}(X'|X,a)$ is a range of probabilities, whose lower and upper bounds are computed by evaluating all the estimated intervals related to some $\wt{s'}=f(\wt{s}, a)$, where $\wt{s} \in X$. The intervals estimated for fictitious arriving states can be entirely contained in a single abstract state $X' \in \Xspace$ or they can overlay multiple abstract states of $\Xspace$. We estimate the lower bound of $\wt{P}_{\vtheta}(X'|X,a)$ as the ratio of the intervals entirely contained into $X'$ and the upper bound of $\wt{P}_{\vtheta}(X'|X,a)$ as the ratio of the intervals that overlay $X'$. Once we have $\wt{P}_{\vtheta}(X'|X,a)$, we can build the $\delta$-\ac{MDP} as a \ac{BMDP}. According to ~\citep{givan2000bounded}, the \ac{BMDP} can be solved following an optimistic or a pessimistic criterion. Motivated by risk-aversion, we choose the second one, so as to obtain the solution of the exact \ac{MDP} whose transition function is the $\wt{P}_{\vtheta}(X'|X,a)$ that maximizes the probability of transitioning to the abstract states with the lowest value function.

\subsection{Stochastic environments}\label{sec:app2.3}
%When we estimate the arriving state $\wt{s'}$ resulting from an unsampled pair $(\wt{s},a)$, we exploit the information provided by some $(s,a)$ pairs sampled from a deterministic environment. 
%In stochastic environments, the estimation of fictitious $\wt{s'}$ (and consequently the estimation of the abstract transition function $\wt{P}(X'|X,a)$) cannot be done with the same approach we use in deterministic environments. Indeed, in stochastic environments the arriving state $s'$ of every collected sample contains an additive noise, independent from the noise of the other collected samples. Furthemore, in a stochastic environment the transition function $P(\cdot|s,a)$ is not a Dirac delta function and then we cannot derive $L_{\Delta}$ as in~\eqref{p:18}.
%we cannot easily estimate the arriving states $\wt{s'}$ for unseen pairs $(\wt{s},a)$: with a single sample $(s,a,s')$ we propagate its noise to the estimated $\wt{s'}$, with two samples we compute two different regions (that should contain $\wt{s'}$) whose intersection can be empty in many cases. 
For stochastic environments, we propose a different approach for estimating the abstract transition kernel from the collected data while exploiting regularities of the environment. To do so, we require a stronger assumption on the transition function $P$ \wrt actions:
%
\begin{assumption}\label{ass:lipmdp2}
	For all $s\in\Sspace$ and $a,\wt{a}\in\Aspace$:
	\begin{align}
	&\mathop{TV}\left(P(\cdot|s,a), P(\cdot| s,\wt{a})\right) \leq L_{TV}\norm{a - \wt{a}},
	\end{align}
	for some positive real constant $L_{TV}$, where $\mathop{TV}(\cdot,\cdot)$ denotes the total variation distance. \todo{definire sotto la Kantorovich e mettere un ref}
\end{assumption}
%
\noindent We can use this assumption to derive a Lipschitz constant for the abstract transition function:
\begin{align}
\Big|\wt{P}(X'|X,a) - \wt{P}(X'|X,\wt{a})\Big| &\leq \int_{X} w(s) \int_{X'} \Big|P(s'|s,a) - P(s'|s,\wt{a})\Big| \de s' \de s \nonumber\\
&\leq \int_{X}w(s)\int_{\Sspace} \Big|P(s'|s,a) - P(s'|s,\wt{a})\Big| \de s' \de s \label{p:24} \\
&= 2\int_{X}w(s)\mathop{TV}\left(P(\cdot|s,a), P(\cdot|s,\wt{a})\right)\de s\label{p:33}\\
&\leq L_{\wt{P}}\norm{a-\wt{a}}\qquad\text{with $L_{\wt{P}}=2L_{TV}$}\label{p:27},
\end{align}
where~\eqref{p:24} extends the inner integral to the entire state space $\Sspace$ (the integrands are all non-negative) and~\eqref{p:33} is from the definition of the total variation distance.
We formulate the estimation of $\wt{P}$ as a maximum likelihood problem, with additional constraints obtained from~\eqref{p:27}.
Let $\Dataset$ be the set of available (real or fictitious) samples, in the form of $(s,a,s')$ tuples. Let $\Xspace_{\Dataset}$ denote the set of visited abstract states, \ie for which there exists at least an $s\in\Xspace$ that appears in $\Dataset$. Moreover, let $\Aspace_{X}$ be the set of actions from the dataset that were performed in a state of $X$, for $X\in\Xspace_{\Dataset}$. 
The objective of our optimization problem is to maximize the likelihood of the samples:
%
\begin{align}
&\max_{\wt{P}\in\Reals^{|\Xspace|\times|\Aspace|\times|\Xspace|}} \prod_{X,X'\in\Xspace_{\Dataset}, a\in\Aspace_{X}}\wt{P}(X'|X,a).
\end{align}
%
This can be reformulated as a convex program as follows:
%
\begin{align}
&\min_{\wt{P}\in\Reals^{|\Xspace|\times|\Aspace|\times|\Xspace|}} &&-\sum_{X,X'\in\Xspace_{\Dataset}, a\in\Aspace_{X}}\log\wt{P}(X'|X,a) \nonumber\\
&\text{subject to} &&\wt{P}(X'|X,a) \geq 0 &\forall X,X'\in\Xspace_{\Dataset}, a\in\Aspace_{X} \nonumber\\
& &&\sum_{X'\in\Xspace_{\Dataset}}\wt{P}(X'|X,a) = 1 &\forall X\in\Xspace_{\Dataset}, a\in\Aspace_{X}.
\end{align}
%
We then add additional constraints that enforce in our estimate of $\wt{P}$ the regularity property we know to be true from~\eqref{p:27}:
%
\begin{align}
&\min_{\wt{P}\in\Reals^{|\Xspace|\times|\Aspace|\times|\Xspace|}} &&-\sum_{X,X'\in\Xspace_{\Dataset}, a\in\Aspace_{X}}\log\wt{P}(X'|X,a) \nonumber\\
&\text{subject to} &&\wt{P}(X'|X,a) \geq 0 &\forall X,X'\in\Xspace_{\Dataset}, a\in\Aspace_{X} \nonumber\\
& &&\sum_{X'\in\Xspace_{\Dataset}}\wt{P}(X'|X,a) = 1 &\forall X\in\Xspace_{\Dataset}, a\in\Aspace_{X} \nonumber\\
& &&\left|\wt{P}(X'|X,a) - \wt{P}(X'|X,\wt{a})\right| \leq L_{\wt{P}}\norm{a- \wt{a}} &\forall X,X'\in\Xspace_{\Dataset}, a\in\Aspace_{X}. \label{p:26}
\end{align}
%
This is still a convex program and can be solved with standard optimization tools.
In many cases, we can still generate fictitious samples as in the deterministic setting (see \mySubsec{subsec:app2.4}). These are simply added to the dataset $\Dataset$. 

\subsection{Linear-Quadratic Gaussian Regulator}\label{subsec:app2.4}
The Double Integrator task presented in Section~\ref{sec:mass} is a special case of Linear-Quadratic Gaussian Regulator~[\cite{peters2002policy}]. This means the transition function is linear plus an additive zero-mean Gaussian noise.
Since this kind of environment is stochastic, we adopt the constrained maximum-likelihood approach presented in~\ref{sec:app2.3}. 
However, we can exploit the underlying linearity of the system to generate fictitious samples.
For each abstract state $X$, we compute a fictitious sample for each unseen pair $(\wt{s}, a)$, where $\wt{s} \in X$ and $a$ is sampled from a state $s \in X$. The next states for fictitious samples are computed from the equation~\eqref{p:20} using $L_{\Delta}=0$, as if the noise was not present. 
In Double Integrator, the Gaussian noise is added to each dimension of the state independently from the other one. 
In this case, we can define a separate constraint~\eqref{p:26} for each of the two dimensions of the state space, for increased precision, possibly with a different constant $L_{\wt{P}}$ for each dimension. \todo{conviene esplicitare i constraints di questo caso particolare, che riguardano le sommatorie delle probabilitÃ  rispetto a ogni dimensione dello state space?}