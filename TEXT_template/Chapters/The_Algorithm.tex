\chapter{The algorithm}

\section{Passive Exploration via $\delta$-MDPs}\label{sec:deltamdp}
%Passive exploration
As mentioned in Section~\ref{sec:intro}, our goal is to gather the samples needed for improving a parametric policy \emph{without performing random actions}. A way to satisfy this strict requirement is to learn a deterministic policy and only perform the actions it prescribes. 
%By doing this, the sampling phase does not involve any random exploration and, in every moment, it is possible to know the exact action that will be performed by the agent. 
This, unfortunately, prevents any form of active exploration. 
However, if the environment is sufficiently regular, we can exploit a form of \emph{passive} exploration. The fundamental problem is that, by observing a single action for each (continuous) state and without any probability measure over actions, we have no way of preferring one action over another for that particular state, which makes policy optimization impossible.  
The key idea is to aggregate states in such a way as to observe a variety of actions within these larger, abstract states. These are simply the actions chosen by our deterministic policy in the different states belonging to the same aggregate. In a sense, aggregation allows transferring the variety of the states visited by our policy (both over different time-steps of the same episode and over independent episodes) into actions. Variety over states is encoded by the future-state distribution $\delta^{\vtheta}$, and is fueled by stochastic transitions and random restarts.
The price to pay is basically a discretization error. This could be, in general, unbounded. However, in Lipschitz MDPs we can keep this error under control, as we will show in Section~\ref{sec:algo}.
%
%Indeed, in a Lipschitz MDP, if we know the effect of taking an action $a$ in a state $s$, we can transfer this knowledge to obtain information about the effect of some unseen state-action pair $(\widetilde{s}, \widetilde{a})$. We can see this as the deterministic equivalent of using a random sample as a proxy for the distribution it was generated from. 
%For instance, if next-state $s'$ is obtained from sample $(s, a)$, Lipschitz conditions provide an interval in which the next state $\widetilde{s'}$ will be included  of a state-action pair $(\widetilde{s}, \widetilde{a})$ that doesn't appear in our samples. The estimated interval is as precise as the two pairs $(s, a)$ and $(\widetilde{s}, \widetilde{a})$ that we consider are similar, where similarity is measured according to a distance measure.
%It appears to be a good idea to consider a sample where an action $a$ is performed in a state $s$ and estimate the effect of taking the same action $a$ in the sampled states that are similar to $s$: in this way, only actions suggested by the deterministic policy are evaluated and the effect of unseen state-action pairs can be upper bounded with a precision that increases with the similarity of the considered states. 
%The information we gain from Lipschitz properties is in the form of bounds. For instance, if next-state $s'$ is obtained from observed state-action pair $(s, a)$, Lipschitz conditions provide a spherical region containing \emph{for sure} the state $\widetilde{s'}$ reached from unseen state-action pair $(\widetilde{s}, \widetilde{a})$, assuming deterministic transitions and Euclidean metric. The more similar the two pairs $(s, a)$ and $(\widetilde{s}, \widetilde{a})$ are, the more precise our region will be, and more data can be used to refine our knowledge by intersecting the corresponding regions. 
%In the continuous bandit (\ie single-state) setting,~\citet{kleinberg2019bandits} provide a way of constructing such regions via directed exploration that is optimal in terms of regret. Albeit deterministic, the resulting behavior would be erratic, and hardly representable with a simple parametric controller.
%The abstract MDP
We are going to implement this idea by building an abstract MDP over aggregated states. We divide the original state space $\Sspace$ into a partition $\Xspace$. To exploit the Lipschitz continuity of our environment, we need to aggregate states taking their mutual distance into account. For this reason, we only consider \textit{tessellations} of the state space, such as grids. 
%
We denote by $D(X) = \sup_{s,\wt{s}\in X}\norm{s-\wt{s}}$ the diameter of an abstract state $X\in\Xspace$.
We want the transition and reward functions of our abstract MDP to model the overall effect of actions on abstract states, so that the optimization in the abstract MDP corresponds, at least approximately, to the resolution of the original policy optimization problem\footnote{Doing this exactly would require additional assumptions, like bisimulation~\citep{givan2003equivalence}, which we deem too restrictive.}. To do so, we use the future-state distribution $\delta^{\vtheta}$ as a weighting function (see Section~\ref{sec:stateabstraction}), obtaining:
%
\begin{align}
	&\widetilde{R}_{\vtheta}(X,a) = \int_{X}\frac{\delta^{\vtheta}(s)}{Z_{\vtheta}(X)}R(s,a)\de s,\label{eq:abrew2} \\
	&\widetilde{P}_{\vtheta}(X'|X,a) = \int_{X} \frac{\delta^{\vtheta}(s)}{Z_{\vtheta}(X)} \int_{X'} P(s'|s,a)\de s\de s',\label{eq:abtran2}
\end{align}
%
where $Z_{\vtheta}(X)=\int_{X}\delta^{\vtheta}(s)\de s$ is a normalization factor necessary to ensure that the weights sum to one.
This completes the definition of our abstract MDP, called $\delta$-MDP in the following to stress the fundamental role of the future-state distribution:
%
\begin{definition}\label{def:abmdp}
	Given an MDP $\langle\Sspace, \Aspace, P, R, \gamma\rangle$, a policy $\pi_{\vtheta}$, and a partition $\Xspace$ of $\Sspace$, the corresponding $\delta^{\vtheta}$-MDP is $\langle\Xspace, \Aspace, \wt{P}_{\vtheta}, \wt{R}_{\vtheta}, \gamma\rangle$, where $\widetilde{R}_{\vtheta}$ is defined as in Equation~\eqref{eq:abrew2} and $\wt{P}_{\vtheta}$ as in Equation~\eqref{eq:abtran2} for all $X,X'\in\Xspace$ and $a\in\Aspace$.
\end{definition}
%
A theoretical justification for these definitions is provided in Section~\ref{sec:algo}.
A starting-state distribution for the $\delta$-MDP can be similarly defined as $\wt{\mu}(X) = \int_{X}\mu(s)\de s$ for all $X\in\Xspace$, where $\mu$ is a starting-state distribution for the original MDP.
%
A (stationary, Markovian) abstract policy is a (possibly stochastic) mapping from abstract states to actions. In order not to confuse them with concrete policies, we will denote them with the letter $\rho$. Fixed the abstract MPD, we can define value functions $V^{\rho}$ and $Q^{\rho}$, future-state distribution $\delta^{\rho}$ (over $\Xspace$) and optimal policy $\rho^{*}$, as in any MDP. Note that there is no direct correspondence, in general, between concrete and abstract policies, since the behavior of a concrete policy may appear non-Markovian in the abstract MDP~\citep{lihong2006towards}.
However, we introduce a simple "concretization" operator $\Conc: \rho \mapsto \pi$ from abstract policies to concrete policies, defined as
%\begin{align}
$(\Conc\rho)(s) = \rho(\Gamma(s))$
%\end{align}
for any $\rho:\Xspace\to\Aspace$ and $s\in\Sspace$. The resulting concrete policy $\pi = \Conc\rho$ performs the same action $\rho(X)$ for all states $s\in X$, hence it is a piece-wise constant function.

\section{Deterministic Policy Optimization}\label{sec:algo}
In this section, we show how to use the $\delta$-MDPs defined in Section~\ref{sec:deltamdp} to approximately solve the policy optimization problem, under Lipschitz conditions but without access to random actions.
%
The proposed methodology is as follows: given the original MDP and a deterministic parametric policy $\pi_{\vtheta}$, we build the corresponding  $\delta^{\vtheta}$-MDP using only the data collected with $\pi_{\vtheta}$ itself. This requires estimating the abstract reward and transition functions. To do so, we can actively exploit the Lipschitz conditions, as discussed in the next section.
The solution of the $\delta^{\vtheta}$-MDP is a deterministic optimal abstract policy $\rho^{*}$, which is guaranteed to exist and maximizes $V^{\rho}(X)$ for all $X\in\Xspace$. This abstract policy can be emulated in the original MDP by $\Conc\rho^{*}$, the piecewise-constant policy. In general, $\Conc\rho^{*}$ may not belong to the original parametric policy space. Hence, we need to project it back as $\pi_{\vtheta'} = \Proj_{\Pi_{\Theta}}(\Conc\rho^{*})$, where $\Proj$ is a projection operator. The new parameter vector $\vtheta'$ identifies the novel policy that we are going to run in the environment. The procedure can then be repeated. We call this general method Deterministic Policy Optimization (DPO for short), outlined in Algorithm~\ref{alg:dpo}.
We now provide a theoretical justification of this approach, which also gives further insights on how to concretely perform the different phases of the algorithm. 
The overall goal is to maximize the performance improvement $J(\vtheta') - J(\vtheta)$.
%
%
\begin{algorithm}[t]
	\caption{DPO}
	\label{alg:dpo}
	\begin{algorithmic}[1]
		\STATE {\bfseries Input:} policy class $\Pi_{\Theta}$, initial policy parameter $\vtheta$, batch size $N$
		\STATE Partition the state space into $\Xspace$
		\FOR{$t=0,1,\dots$}
		\STATE Collect $N$ samples with $\pi_{\vtheta}$ 
		\STATE Estimate $\wt{R}_{\vtheta}$ and $\wt{P}_{\vtheta}$ over $\Xspace$
		\STATE Solve the $\delta^{\vtheta}$-MDP to find optimal abstract policy $\rho^{*}$
		\STATE Project $\Conc\rho^{*}$ back into $\Pi_{\Theta}$ to find $\vtheta'$
		\STATE $\vtheta\gets\vtheta'$
		\ENDFOR
	\end{algorithmic}
\end{algorithm}
%

\section{Algorithmic Details}\label{sec:detail}
In this section, we provide further details on our implementation of the different phases of DPO.

%\subsection{State Discretization}
\paragraph{State aggregation:}
As mentioned, we discretize the state space $\Sspace$ into a regular grid $\Xspace$, in which each hyperrectangular cell is an abstract state. 
%We assume that the domain of each state variable is a continuous interval. If a variable is unbounded or the bounds are unknown, they are set equal to the minimum and maximum value, relative to the corresponding dimension, observed in the collected samples. 
%For each dimension $i$, we obtain in input a scalar $k$ that indicates the number of subsets in which the $i-th$ dimension of the state space $S$ has to be divided. We consider once per time each dimension $i$ and we divide this mono-dimensional space in a partition of $k$ convex subsets with a constant diameter. By partitioning in this way, the size of the subset's bounds on each dimension can be very different. 
%We allow specifying a different spacing for each dimension.
If the original state space $\Sspace$ contains an absorbing state, we add an absorbing abstract state to $\Xspace$. This can be useful for modeling indefinite-horizon tasks.
%The state space partition does not need to be the same across the different iterations of the algorithm, since a new $\delta$-MDP is built at each iteration. For instance, the partition can change when the grid bounds are computed from samples. If we want to keep a fixed partition for computational reasons, we may obtain some samples whose starting states are outside the grid. Those samples are assigned to the closest cell.
If naively implemented, this aggregation strategy is clearly subject to the \textit{curse of dimensionality}. However, only the abstract states that are actually visited by $\pi_{\vtheta}$ are considered in the next steps of the algorithm, so the size of the $\delta$-MDP is polynomial in the number of collected samples\footnote{Efficiently exploring high-dimensional state spaces is still a difficult problem.}.  
More details on how the abstract state space is constructed and other possible aggregation strategies are given in Appendix~\ref{sec:app2}.

%\subsection{$\delta$-MDP Estimation}
\paragraph{Abstract MDP estimation:}\label{sec:mdpest}
%A fundamental aspect in the creation of the $\delta$-MDP is the estimation of the $\gamma$-discounted future-state distribution $\delta^{\pi}(s)$.
To construct the $\delta^{\vtheta}$-MDP for the current policy $\pi_{\vtheta}$, we need to estimate the abstract reward function $\wt{R}_{\vtheta}$ and the abstract transition function $\wt{P}_{\vtheta}$. Their exact computation would require knowledge of the future state distribution $\delta^{\vtheta}$, which is out of reach. %As we said, $\delta^{\pi}(s)$ is used to define $w(s)$ that weighs the abstract functions $\wt{R}_{\vtheta}(X, a)$ and $\wt{P}_{\vtheta}(X'|X, a)$ of the $\delta$-MDP. 
However, we can write the abstract functions as expected values:
\begin{align}
\wt{R}_{\vtheta}(X, a) &= \EV_{s \sim p(s | X)} R(s, a) \label{eq:mcr}\\
\wt{P}_{\vtheta}(X' | X, a) &= \EV_{s \sim p(s | X)} \int_{X'} P(s' | s, a) \de s'\label{eq:mcp}
\end{align}
where $p(s | X) = Z_{\vtheta}(X)^{-1}\delta^{\vtheta}(s)\Ind_X(s)$ is the probability of visiting state $s$ conditioned by $s\in X$, and $\Ind_X(\cdot)$ is the indicator function for set $X$.
In this way, we can simply estimate the abstract function via Monte Carlo estimation using the samples collected with $\pi_{\vtheta}$, since the visited states are distributed as $\delta^{\vtheta}$.
%\begin{align*}
%\wt{R}_{\vtheta}(X, a) &= \frac{1}{|\widetilde{s} \in X|} \sum_{\widetilde{s} \in X}R(\widetilde{s}, a)\\
%\wt{P}_{\vtheta}(X'|X,a) &= \frac{1}{|\widetilde{s} \in X|} \sum_{\widetilde{s} \in X} \int_{x'} P(s' | \widetilde{s}, a) \,ds
%\end{align*}
%The estimated $\delta^{\pi}(s)$ results to be equal to $\frac {|s|} {|\widetilde{s} \in X|}$, where $s \in X$ and we indicate with $\widetilde{s}$ a generic sampled state.
While this is enough for $\wt{R}_{\vtheta}(X, a)$, since $R(s, a)$ is known, estimating $\wt{P}_{\vtheta}(X'|X, a)$ for all abstract states and actions without knowledge of $P(\cdot|s, a)$ requires more effort. All we get from agent-environment interactions is a finite set of samples $(s,a,s')$. This means that, for almost all the actions in $\Aspace$, we do not have any samples. Fortunately, we can leverage our Lipschitz assumptions to fill in this missing information.
%
Let us first consider deterministic environments, where $P(\cdot|s, a)$ is a Dirac delta function that exactly identifies the next state $s'$ resulting from a state-action pair $(s,a)$. For simplicity, we denote this deterministic mapping as $f:\Sspace\times\Aspace\to\Sspace$ and write $s'=f(s,a)$.
Let $\Delta(s, a) \coloneqq f(s) - s$ be the state difference obtained transitioning from pair $(s,a)$. From Assumption~\ref{ass:lipmdp}, fixed an action $a\in\Aspace$, we obtain:
%
\begin{align}\label{eq:deltas}
\norm{\Delta (s, a) - \Delta (\widetilde{s}, a)} \leq L_{\Delta}\norm{s - \widetilde{s}},
\end{align}
%
for all $s,\wt{s}\in\Sspace$, hence the state difference is also Lipschitz continuous. A valid Lipschitz constant $L_{\Delta}$ can be obtained from Assumption~\ref{ass:lipmdp} as $(1+L_{P})$. Under additional assumptions, such as the linearity of the transition function, we can obtain constants less than one or even zero. The latter happens when the state difference depends only on the action, which can be realistic, or at least a good approximation, for some continuous control problems. We discuss these different settings and provide detailed computations in Appendix~\ref{sec:app2}.
%
When $L_{\Delta}=0$, we can easily generate extra "fictitious" samples. For instance, given sample $(s,a,s')$, we can generate $(\wt{s},a,\wt{s}')$ for any other state $\wt{s}\in\Sspace$ by setting $\wt{s}' = \wt{s} + (s'-s)$. This way, for each action $a$ performed by policy $\pi_{\vtheta}$ in abstract state $X$, we can have several (fictitious) samples (one for each visited state $s\in X$). With these samples, we can simply estimate $\wt{P}_{\vtheta}(X'|X,a)$ as the ratio of the next states that fall into $X'$.
When $L_{\Delta}\neq 0$, we can still use~\eqref{eq:deltas} to get a region containing $\wt{s}'$.
%\begin{align}
%	\norm{\wt{s}' - \left(\wt{s}+(s-s')\right)} \leq L_{\Delta}\norm{s-\wt{s}}.
%\end{align}
From this, we can estimate lower and upper bounds on the transition function. As a result, the $\delta$-MDP will be a \textit{Bounded MDP}~\citep{givan2000bounded} instead of a regular one.
%
In the case of stochastic environments, we follow a different approach for estimating the abstract transition kernel. We define a maximum-likelihood problem using data sampled from $\pi_{\vtheta}$ \textit{and} fictitious samples. We then add special constraints to this problem as to enforce the Lipschitz continuity of $\wt{P}_{\vtheta}(X'|X,a)$ \wrt actions. The resulting problem is still convex and can be solved with standard optimization tools.
%the estimation of $\wt{P}_{\vtheta}(X'|X, a)$ is performed by solving a constrained optimization problem whose goal is to maximize the likelihood of the samples collected. The constraints reflect the Lipschitz properties of $\wt{P}_{\vtheta}(X'|X,a)$: the distance between $\wt{P}_{\vtheta}(X'|X, a)$ and $\wt{P}_{\vtheta}(X'|X, \widetilde{a})$ is bound by a $f(d_A(a, \widetilde{a}))$.
Details on bounded-MDPs, their solution and the constrained optimization approach are provided in Appendix~\ref{sec:app2}. 

%\subsection{Value Iteration on $\delta$-MDP}
\paragraph{Solving the abstract MDP:}
In the previous paragraph, we have proposed a way to estimate the abstract transition function for state-action pairs not experienced by the agent, by exploiting the assumptions about the regularity of the environment. However, this still applies only to the actions actually performed by the agent. As a result, our approximation of the $\delta$-MDP has a finite action set. This introduces a further model bias, but allows us to employ dynamic programming to find the optimal abstract policy. This kind of error can be reduced by increasing the number of collected episodes, or the control frequency. Furthermore, under our Lipschitz assumptions, the finite actions actually performed by the agent are supposedly good representatives for the other ones.
We solve this approximate $\delta$-MDP via value iteration~\citep{bertsekas1996neuro}, with a stopping condition on the max-norm distance between consecutive state-value functions.
%For computational reasons, the value iteration on the $\delta$-MDP is performed with two exit conditions. These conditions involve a maximum number of iterations allowed in a single value iteration and a parameter $\epsilon$, used as a treshold in the following way: said $V_i(X)$ the value function of state $X$ at the $i-th$ iteration, if $\forall X \in %\Xspace% 
%X |V_i(X) - V_{i-1}(X)| < \epsilon$ the value iteration terminates.
%In the case of multiple optimal policies, we store the complete set of optimal actions for each abstract state. This allows for easier projection, as explained in the next paragraph.
We may need to evaluate abstract states for which no samples have been collected by the agent. We propose a risk-averse solution, which consists of considering these "unknown" abstract states as absorbing and set their value to the lowest known value. 
%This strategy follows a risk-minimization principle: to discourage actions that could bring the agent in unexplored macrostates. If the macrostate is worth it, the agent should be able to reach it in more time but in a safe manner. 

%\subsection{Projection on Parametric Space $\theta$}
\paragraph{Projection:}
Given the optimal abstract policy $\rho^*$, its concretization $\Conc\rho^*$ is easily obtained by copying the action selected by $\rho^{*}$ for all the states of the same abstract state. We then need to represent $\Conc\rho^{*}$ in the space $\Pi_{\Theta}$ of the original policy $\pi_{\vtheta}$. As indicated by Theorem~\ref{lem:boundj}, this projection phase can be approached as a regression problem, where the target for $\pi_{\vtheta'}(s)$ is $\Conc\rho^*(s)$. 
%When multiple optimal actions are available for the same abstract state, we consider as a target the error-minimizing one, in order to facilitate the projection. 
The first term from~\eqref{eq:proj} is the RMSE loss, weighted by the future state distribution $\delta^{\vtheta}$. For differentiable policies (including neural networks), this loss can be minimized via gradient descent from the samples collected with $\pi_{\vtheta}$. The second term from~\eqref{eq:proj} penalizes solutions that are too far from the current policy parameters and it can be used as a regularization term. In practice, the regularization coefficient $\lambda$ can be tuned as a meta-parameter. Note that, without this projection step, there would be no room for further policy improvement, as the next $\delta$-MDP would have just one action per abstract state. The projection error actually allows the agent to evaluate new actions and visit new regions of the state space. 

\section{Abstract MDP construction}\label{sec:app2}
In this section, we provide additional details on the construction of the $\delta$-MDP outlined in Section~\ref{sec:mdpest}, in particular \wrt the estimation of the abstract transition function. We propose different approaches depending on the nature of the environment: 

\subsection{Deterministic linear environments}\label{app:b1}
In deterministic environments, the transition function is a Dirac delta function with $P(s'|s,a)=1$ for $s'=f(s,a)$ and $P(\cdot|s,a)=0$ elsewhere, where $f(s,a)$ is a function that outputs the arriving state $s'$ for an input pair $(s,a)$.
Since the Kantorovich distance between two Dirac delta functions is equal to the distance of their locations, we can use Assumption~\ref{ass:lipmdp} to obtain a Lipschitz constant $L_{\Delta}$ for state-difference function $\Delta$. For every $a\in\Aspace$ and $s,\wt{s}\in\Sspace$:
\begin{align}
\norm{\Delta(s,a)-\Delta(\wt{s},a)} &=
\norm{f(s,a) - s - f(\wt{s},a) + \wt{s}} \nonumber\\
&\leq \norm{f(s,a)-f(\wt{s},a)} + \norm{s-\wt{s}} \label{p:23}\\
&= \Kant\left(P(\cdot|s,a), P(\cdot| \wt{s},a)\right) + \norm{s-\wt{s}} \nonumber\\
&\leq L_{P}\norm{s-\wt{s}} + \norm{s-\wt{s}} \label{p:22}\\
&\leq L_{\Delta} \norm{s-\wt{s}} \qquad \text{ with } L_{\Delta} = \left(1 + L_{P}\right), \label{p:18}
\end{align}
where~\eqref{p:23} is from the triangle inequality and~\eqref{p:22} is from Assumption~\ref{ass:lipmdp}.

We call \textit{linear} a deterministic environment in which $f(s, a)$ is linear \wrt the input variables $s$ and $a$. For these environments, we can write $f(s,a) = As + Ba$, where $A$ and $B$ are (possibly unknown) constant matrices. With $f(s,a)$ defined in this way, we can obtain a possibly smaller Lipschitz constant for $\Delta$:
\begin{align}
\norm{\Delta(s,a)-\Delta(\wt{s},a)} &= \norm{As+Ba-s - \left(A\wt{s}+Ba-\wt{s}\right)} \nonumber \\
&= L_{\Delta} \norm{s-\wt{s}} \qquad \text{ with } L_{\Delta}=\norm{A-\mathbb{I}}. \label{p:20}
\end{align}
In general, the constant obtained in~\eqref{p:20} can be smaller than $\left(1 + L_{P}\right)$ and even equal to zero. When $\Delta(s,a)$ depends only on the performed action $a$ and not on the starting state $s$,  $A=\mathbb{I}$ and $L_{\Delta}=0$. From~\eqref{p:18}, if we consider $L_{\Delta} = 0$ instead of $L_{\Delta} = \left(1 + L_{P}\right)$, we obtain the equality $\Delta(s,a) = \Delta(\wt{s}, a)$ that allows to exactly predict the arriving state $\wt{s'}=f(\wt{s}, a)$ as $\wt{s'} = \wt{s} + \Delta(s,a)$.\\
\newline
Unfortunately, when $L_{\Delta} \neq 0$ we cannot generate "fictitious" samples exactly. However, we exploit the inequality in~\eqref{p:18} to obtain an interval (or a higher-dimensional region) including the unknown $\wt{s'}=f(\wt{s},a)$, instead of the exact point $\wt{s'}$. Here we show how to obtain the interval in the simple case of one-dimensional states:
\begin{align}
-L_{\Delta} \left|s - \wt{s}\right| &&\leq&& \Delta s (s,a) - \Delta s (\wt{s},a) &&\leq&& L_{\Delta} \left|s - \wt{s}\right| \nonumber\\
%
\Delta(s,a) - L_{\Delta} \left|s - \wt{s}\right| &&\leq&& \Delta(\wt{s},a) &&\leq&& \Delta(s,a) + L_{\Delta} \left|s - \wt{s}\right| \nonumber\\
%
\Delta(s,a) - L_{\Delta} \left|s - \wt{s}\right| &&\leq&& f(\wt{s},a) - \wt{s} &&\leq&& \Delta(s,a) + L_{\Delta} \left|s - \wt{s}\right|. \label{p:19}
\end{align}
From~\eqref{p:19} we derive $\wt{s'}=f(\wt{s}, a) \in \Big[\wt{s} + \Delta(s,a) - L_{\Delta} \left|s - \wt{s}\right|, \quad \wt{s} + \Delta(s,a) + L_{\Delta} \left|s - \wt{s}\right|\Big]$, where $L_{\Delta} = \norm{A - \mathbb{I}}$.
When the state space is $n-$dimensional with $n\geq 2$, the interval that we estimate is an hyperrectangle. 
%Each side of this hyperrectangle represents a mono-dimensional interval that includes the $i-th$ dimension of the state $\wt{s'}$, where $i \in \left[0, n-1\right]$.\\
\newline
In tasks where it is required a lot of precision in order to estimate $\wt{s'}$, the computed intervals could be not tight enough to represent $\wt{s'}$ with the required accuracy. We can increase the precision by computing a second interval on each $\wt{s'}$ and considering the intersection of the two intervals on $\wt{s'}$ as the most precise estimate. The second interval is calculated by considering two state-action pairs having the \emph{same} state and different \emph{actions}:
\begin{align}
\norm{\Delta(\wt{s},a) - \Delta(\wt{s}, \wt{a})} = \norm{A\wt{s}+Ba-\wt{s} - \left(A\wt{s}+B\wt{a}-\wt{s}\right)} &= \norm{B} \norm{a-\wt{a}}. \label{p:21}
\end{align}
Similarly to~\eqref{p:19}, we obtain the new interval containing $\wt{s'} = f(\wt{s}, a)$:
\begin{align}
\wt{s'} \in \Big[\wt{s} + \Delta(\wt{s},\wt{a}) - L_{\Delta (a)}\norm{a - \wt{a}}, \quad \wt{s} + \Delta(\wt{s},\wt{a}) + L_{\Delta (a)}\norm{a - \wt{a}} \Big] \quad \text{with $L_{\Delta (a)}$ = $\norm{B}$}.\label{p:25}
\end{align}
Hence, given (real) samples $(s_1,a_1,s_1')$ and $(s_2, a_2, s_2')$, we can generate fictitious next states for both $(s_1,a_2)$ and $(s_2,a_1)$, intersecting two intervals for each of them.

\subsection{Deterministic non-linear environments}
%If the environment is non-linear, we can consider $L_{\Delta} = \left(1 + L_{P} \right)$ or we can estimate a smaller $L_{\Delta}$ from the samples and use it to compute the intervals on $\wt{s'}$ in the same way we showed for linear environments. Even if we can no longer consider $f(s,a)$ as a linear function in $s$ and $a$, the environment still satisfies the Lipschitz properties of regularity. If the constants $L_{\Delta}$ estimated from the samples are smaller than the real one, the intersection of two estimated intervals related to some $\wt{s'}$ can be empty. When it happens, we simply discard the intervals and do not generate the fictitious sample.
When the (deterministic) transition function is non-linear, we are not able to exactly predict the next sate of un-sampled state-action pairs. However, we can still exploit the Lipschitz assumptions to obtain upper and lower bounds (or multi-dimensional regions) for the next-states. To do so, we can consider $L_{\Delta} = \left(1 + L_{P} \right)$, as shown above~\eqref{p:18} for general deterministic transitions, or we can estimate a smaller $L_{\Delta}$ from data.
The result is a \emph{bounded-parameter} abstract MDP instead of a regular one.

A bounded-parameter MDP~\citep[BMDP,][]{givan2000bounded} is a five-tuple $\langle \Sspace, \Aspace, P_{\updownarrow}, R_{\updownarrow}, \gamma \rangle$, where $\Sspace$, $\Aspace$ and $\gamma$ are defined as for (finite) MDPs, and $P_{\updownarrow}, R_{\updownarrow}$ are analogous to the MDP transition and reward functions, but yield closed real intervals instead of real values. This can be used to model uncertainty on the true nature of a decision process. To ensure that $P_{\updownarrow}$ admits only well-formed transition functions, we require that for any action $a$ and state $s$, the sum of the lower bounds of $P_{\updownarrow}(s'|s,a)$ over all states $s'$ must be less than or equal to one, while the upper bounds must sum to a value greater than or equal to one. A BMDP $M_{\updownarrow} = \langle \Sspace, \Aspace, P_{\updownarrow}, R_{\updownarrow}, \gamma \rangle$ defines a set of exact MDPs. For any exact MDP $M = \langle \Sspace', \Aspace', P', R', \gamma' \rangle$, we have $M \in M_{\updownarrow}$ if $\Sspace = \Sspace', \Aspace = \Aspace', \gamma = \gamma'$, and for any action $a$ and states $s, s', R'(s,a)$ belongs to the interval $R_{\updownarrow}(s,a)$ and $P'(s'|s,a)$ belongs to the interval $P_{\updownarrow}(s'|s,a)$. An interval value function $V_{\updownarrow}$ is a mapping from states to closed real intervals. We use such functions to indicate that the value of a given state for any exact MDP falls within the selected interval. As in the case of (exact) value functions, interval value functions are specified \wrt a fixed policy.

If we have $L_{\Delta} \neq 0$ and we estimate with~\eqref{p:18} the fictitious arriving state $\wt{s'} = f(\wt{s}, a)$, where $(\wt{s},a)$ is an unseen state-action pair, we obtain a region including $\wt{s'}$ instead of the exact point $\wt{s'}$. The information provided by the estimation of the fictitious arriving states is not sufficient to compute an exact $\wt{P}_{\vtheta}(X'|X,a)$ for each abstract state-action pair $(X,a)$ of the $\delta$-MDP. The value of each $\wt{P}_{\vtheta}(X'|X,a)$ is a range of probabilities, whose lower and upper bounds are computed by evaluating all the estimated intervals related to some $\wt{s'}=f(\wt{s}, a)$, where $\wt{s} \in X$. The intervals estimated for fictitious arriving states can be entirely contained in a single abstract state $X' \in \Xspace$ or they can overlay multiple abstract states of $\Xspace$. We estimate the lower bound of $\wt{P}_{\vtheta}(X'|X,a)$ as the ratio of the intervals entirely contained into $X'$ and the upper bound of $\wt{P}_{\vtheta}(X'|X,a)$ as the ratio of the intervals that overlay $X'$. Once we have $\wt{P}_{\vtheta}(X'|X,a)$, we can build the $\delta$-MDP as a bounded-MDP. According to ~\citep{givan2000bounded}, the bounded MDP can be solved following an optimistic or a pessimistic criterion. Motivated by risk-aversion, we choose the second one, so as to obtain the solution of the exact MDP whose transition function is the $\wt{P}_{\vtheta}(X'|X,a)$ that maximizes the probability of transitioning to the abstract states with the lowest value function.

\subsection{Stochastic environments}\label{sec:app2.3}
%When we estimate the arriving state $\wt{s'}$ resulting from an unsampled pair $(\wt{s},a)$, we exploit the information provided by some $(s,a)$ pairs sampled from a deterministic environment. 
%In stochastic environments, the estimation of fictitious $\wt{s'}$ (and consequently the estimation of the abstract transition function $\wt{P}(X'|X,a)$) cannot be done with the same approach we use in deterministic environments. Indeed, in stochastic environments the arriving state $s'$ of every collected sample contains an additive noise, independent from the noise of the other collected samples. Furthemore, in a stochastic environment the transition function $P(\cdot|s,a)$ is not a Dirac delta function and then we cannot derive $L_{\Delta}$ as in~\eqref{p:18}.
%we cannot easily estimate the arriving states $\wt{s'}$ for unseen pairs $(\wt{s},a)$: with a single sample $(s,a,s')$ we propagate its noise to the estimated $\wt{s'}$, with two samples we compute two different regions (that should contain $\wt{s'}$) whose intersection can be empty in many cases. 
For stochastic environments, we propose a different approach for estimating the abstract transition kernel from the collected data while exploiting regularities. To do so, we require a stronger assumption on the transition function \wrt actions:
%
\begin{assumption}\label{ass:lipmdp2}
	For all $s\in\Sspace$ and $a,\wt{a}\in\Aspace$:
	\begin{align}
	&\mathop{TV}\left(P(\cdot|s,a), P(\cdot| s,\wt{a})\right) \leq L_{TV}\norm{a - \wt{a}}, \\
	\end{align}
	for some positive real constant $L_{TV}$, where $\mathop{TV}(\cdot,\cdot)$ denotes the total variation distance.
\end{assumption}
%
We can use this assumption to derive a Lipschitz constant for the abstract transition function:
\begin{align}
\Big|\wt{P}(X'|X,a) - \wt{P}(X'|X,\wt{a})\Big| &\leq \int_{X} w(s) \int_{X'} \Big|P(s'|s,a) - P(s'|s,\wt{a})\Big| \de s' \de s \nonumber\\
&\leq \int_{X}w(s)\int_{\Sspace} \Big|P(s'|s,a) - P(s'|s,\wt{a})\Big| \de s' \de s \label{p:24} \\
&= 2\int_{X}w(s)\mathop{TV}\left(P(\cdot|s,a), P(\cdot|s,\wt{a})\right)\de s\label{p:33}\\
&\leq L_{\wt{P}}\norm{a-\wt{a}}\qquad\text{with $L_{\wt{P}}=2L_{TV}$}\label{p:27},
\end{align}
where~\eqref{p:24} extends the inner integral to the entire state space $\Sspace$ (the integrands are all non-negative) and~\eqref{p:33} is from the definition of the total variation distance.
We formulate the estimation of $\wt{P}$ as a maximum likelihood problem, with additional constraints obtained from~\eqref{p:27}.
Let $\Dataset$ be the set of available (real or fictitious) samples, in the form of $(s,a,s')$ tuples. Let $\Xspace_{\Dataset}$ denote the set of visited abstract states, \ie for which there exists at least an $s\in\Xspace$ that appears in $\Dataset$. Moreover, let $\Aspace_{X}$ be the set of actions from the dataset that were performed in a state of $X$, for $X\in\Xspace_{\Dataset}$. 
The objective of our optimization problem is to maximize the likelihood of the samples:
%
\begin{align}
&\max_{\wt{P}\in\Reals^{|\Xspace|\times|\Aspace|\times|\Xspace|}} \prod_{X,X'\in\Xspace_{\Dataset}, a\in\Aspace_{X}}\wt{P}(X'|X,a).
\end{align}
%
This can be reformulated as a convex program as follows:
%
\begin{align}
&\min_{\wt{P}\in\Reals^{|\Xspace|\times|\Aspace|\times|\Xspace|}} &&-\sum_{X,X'\in\Xspace_{\Dataset}, a\in\Aspace_{X}}\log\wt{P}(X'|X,a) \nonumber\\
&\text{subject to} &&\wt{P}(X'|X,a) \geq 0 &\forall X,X'\in\Xspace_{\Dataset}, a\in\Aspace_{X} \nonumber\\
& &&\sum_{X'\in\Xspace_{\Dataset}}\wt{P}(X'|X,a) = 1 &\forall X\in\Xspace_{\Dataset}, a\in\Aspace_{X}.
\end{align}
%
We then add additional constraints that enforce in our estimate of $\wt{P}$ the regularity property we know to be true from~\eqref{p:27}:
%
\begin{align}
&\min_{\wt{P}\in\Reals^{|\Xspace|\times|\Aspace|\times|\Xspace|}} &&-\sum_{X,X'\in\Xspace_{\Dataset}, a\in\Aspace_{X}}\log\wt{P}(X'|X,a) \nonumber\\
&\text{subject to} &&\wt{P}(X'|X,a) \geq 0 &\forall X,X'\in\Xspace_{\Dataset}, a\in\Aspace_{X} \nonumber\\
& &&\sum_{X'\in\Xspace_{\Dataset}}\wt{P}(X'|X,a) = 1 &\forall X\in\Xspace_{\Dataset}, a\in\Aspace_{X} \nonumber\\
& &&\left|\wt{P}(X'|X,a) - \wt{P}(X'|X,\wt{a})\right| \leq L_{\wt{P}}\norm{a- \wt{a}} &\forall X,X'\in\Xspace_{\Dataset}, a\in\Aspace_{X}. \label{p:26}
\end{align}
%
This is still a convex program and can be solved with standard optimization tools.
In many cases, we can still generate fictitious samples as in the deterministic setting (see the next Subsection). These are simply added to the dataset $\Dataset$. 

\subsection{Linear-Quadratic Gaussian Regulator}\label{sec:app2.4}
The Double Integrator task presented in Section~\ref{sec:exp} is a special case of Linear-Quadratic Gaussian Regulator~\citep[LQG,][see Section~\ref{sec:di} for further details]{peters2002policy}. This means the transition function is linear plus an additive zero-mean Gaussian noise.
Since this kind of environment is stochastic, we adopt the constrained maximum-likelihood approach presented in~\ref{sec:app2.3}. 
However, we can exploit the underlying linearity of the system to generate fictitious samples.
For each abstract state $X$, we compute a fictitious sample for each unseen pair $(\wt{s}, a)$, where $\wt{s} \in X$ and $a$ is sampled from a state $s \in X$. The next states for fictitious samples are computed with equation~\eqref{p:20} using $L_{\Delta}=0$, as if the noise was not present. 
In Double Integrator, the Gaussian noise is added to each dimension of the state independently from the other one. 
In this case, we can define a separate constraint~\eqref{p:26} for each of the two dimensions of the state space, for increased precision, possibly with a different constant $L_{\wt{P}}$ for each dimension.