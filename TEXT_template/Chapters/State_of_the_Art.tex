\chapter{State of the Art} \label{chap:stateoftheart}

According to (\cite{sutton2018reinforcement}), \acf{RL} is learning what to do so as to maximize a numerical reward signal. The learner must discover which actions yield the most reward by trying them. Actions may affect not only the immediate reward but also the next situation and all subsequent rewards. The problem of \ac{RL} is formalized with ideas coming from dynamical systems theory, specifically with the \acf{MDP} that we detail in \mySec{sec:mdp}. \todo{scaletta dello stato dell'arte}

\section{\NoCaseChange{\acf{MDPs}}}\label{sec:mdp}
\ac{MDPs} are a mathematical framework used for modeling \ac{RL} problems in which an \emph{agent}, by interacting with an \emph{environment}, learns how to achieve a goal. \ac{MDPs} are a formalization of sequential decision making, where the decision taken by the agent influences immediate and future rewards. The agent interacts with the environment by selecting the actions to perform and receiving rewards and information on the new situation. Rewards are provided by the environment in the form of scalar values, the agent aims to maximize the sum of them over time. More specifically, at each time step $t$, the agent receives a representation of the state $s_t \in \Sspace$, on that basis it selects an action $a_t \in \Aspace$. One time step later, the agent receives a reward $r_{t+1}$ and finds itself in a new state $s_{t+1} \in \Sspace$.\\
\newline
% MDP
We define the \ac{MDP} according to~\citep[]{puterman2014markov}. In general, the state space $\Sspace$ and the action space $\Aspace$ can be finite or continuous sets. We focus on the solution of continuous \ac{MDPs} as these are the most suitable for modeling continuous control problems.
\begin{definition}[MDP]\label{def:mdp}
The \ac{MDP} is described by a five-tuple $M=\langle \Sspace, \Aspace, P, R, \gamma \rangle$, where:
\begin{itemize}
	\item $\Sspace$ is the state space, with $\Sspace\subseteq\Reals^N$.
	\item $\Aspace$ is the action space, with $\Aspace\subseteq\Reals^D$.
	\item $P: \Sspace \times \Aspace \to \Delta(\Sspace)$ is the transition function, with $P(s'|s,a)$ denoting the probability of reaching state $s'$ from state $s$ by taking action $a$. For any state-action pair $(s,a)$, it holds the following equality:
	$$ \sum_{s' \in \Sspace} P(s'|s,a) = 1.$$
	\item $R: \Sspace \times \Aspace \to \Reals$ is the reward function, with $R(s,a)$ denoting the expected reward from taking action $a$ in state $s$.
	\item $\gamma \in [0, 1)$ is the discount factor, eventually used to discount the present effect of future rewards.
\end{itemize}
\end{definition}
The transition function $P$ defines the dynamics of the \ac{MDP} and ensures the \emph{Markov property}: the probability of reaching $s_t$ depends only on the immediately previous state and action, $s_{t-1}$ and $a_{t-1}$, and not on earlier states and actions.
\subsection{Policy and Value Functions}
The behaviour of the agent is modeled with a \emph{policy} $\pi: \Sspace \mapsto \Delta(\Aspace)$, \ie a mapping from states to probabilities of selecting each possible action. The agent learns a policy according to its goal of maximizing the sum of rewards collected during the task. Specific details on how to learn the optimal policy are given later, in \mySec{sec:tabmet} and \mySec{sec:ps}.\\
The sum of rewards obtained if we consider time steps from $k=t$ to a certain $k=T$ is called \emph{return} $G_t$. In general, the return is defined as a sum of discounted rewards:
\todo{Uso G come nome?}
$$G_t = \sum_{k=t+1}^{T} \gamma^{k-t-1} r_k, $$where $r_k$ is the reward obtained at step $k$. The last time step $T$ represents the horizon of the task and can be finite or infinite. In the first case, the task is said to be \emph{episodic} and the agent-environment interaction breaks naturally into episodes. In the second case (\ie $T = \infty$), the task is said to be \emph{continuing} and a discount factor $\gamma<1$ is required in order to obtain a finite value for return $G_t$.\\
\newline
In this work, we denote with $\pi(a|s)$ the probability of performing the action $a$ in the state $s$, according to the policy $\pi$. Since $\pi(s)$ is a distribution of probability, the following equality holds:
$$\sum_{a \in \Aspace} \pi(a|s) = 1 \quad \forall s \in \Sspace.$$
If, for each state $s \in \Sspace$, there exists an action $a$ such that $\pi(a|s) = 1$, the policy is deterministic.\\
\newline
Given a policy $\pi$, it is possible to compute the \emph{value function} $V^{\pi}: \Sspace \rightarrow \mathbb{R}$, a widely used function in \ac{RL} that measures how good it is for the agent to be in a given state $s \in \Sspace$ according to the expected return obtainable from that state. Since the rewards the agent can expect to receive in the future depend on the taken actions, the value function is defined with respect to policies: $V^{\pi}(s)$ is the expected sum of discounted rewards that the agent collects by starting at state $s$ and following policy $\pi$. The value function can be defined recursively via the Bellman equation: $$V^{\pi}(s) = \int_{\Aspace} \pi(a|s) \Big(R(s,a) + \gamma \int_{\Sspace}P(s'|s,a)V^{\pi}(s')\de s' \Big) \de a.$$
For control purposes, we can also define an action-value function: $$Q^{\pi}(s,a) = R(s,a) + \gamma \int_{\Sspace} P(s'|s,a) \int_{\Aspace} \pi(a'|s') Q^{\pi}(s',a') \de a' \de s'.$$
It represents the expected return obtained from taking the action $a$ in state $s$ and then following the policy $\pi$. \\
\newline
Finally, we denote with $$A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s)$$ the advantage function of policy $\pi$. It represents the advantage of performing action $a$ in state $s$, instead of the action prescribed by $\pi(s)$.

\subsection{Performance Measure}
In order to evaluate how good a policy $\pi$ is, we consider the expected reward obtained starting from a state $s \in \Sspace$ sampled from the initial-state distribution $\mu$ and following the policy $\pi$.\\
The \emph{performance measure} $J(\pi)$ expressed in the form of an expected value is:
$${J(\pi) = (1 - \gamma)^{-1}\EV_{s\sim\delta^{\pi}, a \sim \pi}R(s,a)} = \EV_{s\sim\mu}V^{\pi}(s),$$
where $\delta^{\pi}$ is the $\gamma$-discounted future-state distribution. This function is defined as:
$$\delta^{\pi}(s) = (1 - \gamma)\EV_{s_{0}\sim\mu}\sum_{t=0}^{\infty}\gamma^t P^{\pi}(S_t = s|S_0=s_0)$$
and represents the probability of being in a certain state $s$ during the execution of the task, provided that the policy is $\pi$ and the initial state distribution is $\mu$.\\
The performance measure can be used to identify the optimal policy that we want to learn in the \ac{RL} problem as:
$$ \pi^{*} \in \arg \max_{\pi} J(\pi).$$

\section{Tabular Solution Methods}\label{sec:tabmet}
Solving a \ac{RL} task means finding a policy that maximizes the return obtained by the agent in the task. First, we consider finite \ac{MDPs}. They are a subset of the \ac{MDPs} defined in \ref{def:mdp}. In finite \ac{MDPs}, $\Sspace$ and $\Aspace$ are finite sets of discrete values and the expressions reported so far can be used for finite \ac{MDPs} if we replace integrals with summations.\\
Policies can be partially ordered according to their value function: 
\begin{align}
\pi' \geq \pi \iff V^{\pi'}(s) \geq V^{\pi}(s) \forall s \in \Sspace. \label{eq:ordpol}
\end{align} 
In finite \ac{MDPs} there always exists a deterministic optimal policy $\pi^{*}$. The value function $V^{*}$ computed according to $\pi^{*}$ has the following property:
$$ V^{*}(s) = \max_{\pi}V^{\pi}(s) \quad \forall s \in \Sspace.$$
Optimal functions can be written with the Bellman optimality equations:
\begin{align*}
V^{*}(s) &= \max_a \Big( R(s,a) + \gamma \sum_{s'}P(s'|s,a)   V^{*}(s') \Big)\\
Q^{*}(s,a) &= R(s,a) + \gamma \sum_{s'}P(s'|s,a) \max_{a'} Q^{*}(s', a').
\end{align*}
If we have a \emph{complete knowledge} of the environment, \ie the agent has a model of transition and reward functions of the \ac{MDP}, we can use \acf{DP} to solve the \ac{RL} problem (\mySubsec{subsec:dp}).\\
Instead, in the case of \emph{incomplete knowledge} the unknown functions can be estimated from experience (\mySubsec{subsec:alt}).\\
These algorithms are called \emph{tabular} because the state and action spaces are small enough for the value functions to be represented in a tabular format.\\
\newline
When the problems involve finite \ac{MDPs} with a larger state space or continuous \ac{MDPs}, we have two possibilities: replace the exact value functions with \emph{function approximators} or search the optimal policy in the policy space, the latter doesn't require to compute any value function (\mySec{sec:ps}).

\subsection{Dynamic Programming}\label{subsec:dp}
\acf{DP} is a collection of algorithms that can be used to compute optimal policies, given a model of the environment in the form of \ac{MDP}. Since \ac{DP} is computationaly expensive and requires finite \ac{MDP}s, its utility in solving \ac{RL} problems is limited. However \ac{DP} provides strong foundation for understanding the following methods.\\
\newline
\ac{DP} offers two possibilities to compute the optimal value functions: \emph{policy iteration} and \emph{value iteration}.

\subsubsection{Policy Iteration}
In policy iteration, two consecutive operations are performed on a policy $\pi$ in order to obtain a policy $\pi'$ that is better according to the ordering rule in~\eqref{eq:ordpol}. These operations are called \emph{policy evaluation} and \emph{policy improvement} and the goal of the algorithm is to obtain the optimal value function $V^{*}$ through the sequence:
$$ \pi_{0} \xrightarrow{\text{E}} V^{\pi_{0}} \xrightarrow{\text{I}} \pi_{1} \xrightarrow{\text{E}} V^{\pi_{1}} \xrightarrow{\text{I}} \pi_{2} \xrightarrow{\text{E}} ... \xrightarrow{\text{I}} \pi_{*} \xrightarrow{\text{E}} V^{*}.
$$
Policy evaluation consists in computing the value function $V^{\pi}$ for every state $s \in \Sspace$, according to the current policy $\pi$, by iteratively applying the following update rule:
$$
V_{k+1}(s) = \sum_{a}\pi(a|s) \Big( R(s,a) + \gamma \sum_{s'}P(s'|s,a) V_{k}(s') \Big),
$$
until $V_{k}$ converges to $V^{\pi}$. The existence of $V^{\pi}$ is guaranteed as long as $\gamma < 1$. \todo{cit smth related to the convergence?} \\
\newline
Policy improvement consists in updating the policy $\pi$ in a new deterministic policy $\pi'$, according to the value function $V^{\pi}$ computed in the previous policy evaluation. For each state $s \in \Sspace$, the policy $\pi'$ is computed according to the following rule:
\begin{align} 
\pi'(s) = \arg \max_{a} \Big( R(s,a) + \gamma \sum_{s'} P(s'|s,a)  V^{\pi}(s') \Big). \label{eq:polimp}
\end{align}
When $\pi'$ is as good as $\pi$ (\ie $V^{\pi} = V^{\pi'}$), the algorithm terminates. The policy $\pi$ is considered to be optimal.
\subsubsection{Value Iteration} \label{subsec:vi}
The main drawback in policy iteration is that, for each policy evaluation performed, multiple iterations are required. If we assume to truncate the policy evaluation after just one application of the update rule, we obtain the value iteration. In value iteration, the value function at each step is updated for all $s \in \Sspace$ as follows:
$$ V_{k+1}(s) = \max_a \Big( R(s,a) + \gamma \sum_{s'}P(s'|s,a)  V_{k}(s') \Big).
$$
Differently from policy iteration, we don't consider any intermediate policy $\pi_i$. However, the sequence \{$V_{k}$\} of the value functions computed converges to $V^{*}$ \todo{cit smth related to the convergence?} and the optimal policy $\pi^{*}$ is obtained applying~\eqref{eq:polimp} with $V^{*}$ instead of $V^{\pi}$.

\subsection{Uncomplete Knowledge of \ac{MDPs}}

\subsubsection{Tabular Alternatives to \ac{DP}}\label{subsec:alt}
If we don't have a complete knowledge of the environment, we can learn the unknown dynamics from experience, \ie from the samples collected by the agent while interacting with the environment. The samples are tuples $\langle s,a,r,s' \rangle_t$, with $s, s' \in \Sspace, a \in \Aspace$ and $r = R(s,a)$. They contain information related to the interaction agent-environment at time step $t$. The two main classes of algorithms are \acf{MC} and \acf{TD}. \ac{MC} methods involve episodic tasks, \ac{TD} learning involves continuing tasks. We don't detail these algorithms since they are not involved in our work.

\subsubsection{Approximate Solution Methods}
The methods present until now are not suitable to solve \ac{RL} problems that involve a larger state space. In these problems we cannot expect to obtain the optimal policy, our goal is to find a good approximate solution using limited computational resources. If we consider a continuous state space, it's very probable that the agents always visits states never seen before. Then, we need to generalize the experience we gather from a subset of states to all the state space. The generalization comes up in the form of \emph{function approximation}. In function approximation, the value function is not representable in a tabular form, instead of a table it is used a function estimated from samples.\\
\newline
There are several methods that rely on value function approximation to provide a solution of \ac{RL} problem. However function approximation arises new issues. First of all, convergence assurances are not easy to be given: if the policy is greedy, an arbitrary small change in the estimated value of an action can cause it to be, or not be, selected [\citet{Sutton1999PolicyGM}]. Approximated value functions $U_t$ can introduce a bias that prevents the convergence to a local optimum, this is not the case when $\EV[U_t|S_t = s] = V^{\pi}(S_t)$. \todo{complex neural networks as approximate value fun?}

\section{Policy Search}\label{sec:ps}
In contrast to value-based methods, \acf{PS} methods use parametrized policies $\pi_{\theta} \in \Pi^{\theta}$. They directly operate in the parameter space $\Theta, \theta \in \Theta$ to find the optimal parametrized policy and typically avoid learning a value function. \ac{PS} copes with high dimensional state and action spaces and offers better convergence assurances than methods that involve function approximation. They have been found to be suitable in robotic applications. Most of the \ac{PS} algorithms are \emph{model-free} (\ie they don't require a representation of the environment) because directly learning a policy is often easier than learning an accurate model. These methods update the policy directly from the sampled trajectories, an exploration strategy is used to generate trajectories. Exploration can be performed in the action space or in the parameter space. The former one is implemented by adding a noise $\epsilon$ directly to the executed actions, the noise is generally sampled from a zero-mean Gaussian distribution. The latter consists in a perturbation of the parameter vector $\theta$ of $\pi_{\theta}$. Also the parameters that define the exploration distributions can be updated: usually the exploration is gradually decreased to fine tune the policy parameters. [\citet{deisenroth2013Survey}].

\subsection{Policy Representations} \label{subsec:polrep}
In this work, we focus on the optimization of deterministic parametric policies of the form $\pi_{\vtheta}:\Sspace\to\Aspace$, with $\vtheta\in\Theta\subseteq\Reals^{m\times D}$. We will often abbreviate $\pi_{\vtheta}$ as $\vtheta$ in subscripts and function arguments, \eg $V^{\vtheta} \equiv V^{\pi_{\vtheta}}$, $J(\vtheta) \equiv J(\pi_{\vtheta})$. The simplest way of parametrizing $\pi_{\vtheta}$ is by means of a linear mapping. The linear policy is defined as $\pi_{\vtheta}(s) = \vtheta^T\vphi(s)$, where $\vtheta\in\Reals^{m\times D}$ and $\vphi:\Sspace\to\Reals^m$ is a feature function. This can be the state itself or, for instance, a set of radial basis functions (RBF). An example of RBF is the Gaussian $\phi_i(s; \mu_i, \sigma_i) = \exp\left\{-{(s -\mu_i)^2}\big/{(2\sigma_i^2)}\right\}$, where $\mu_i$ and $\sigma_i$ are hyperparameters, $i=1,\dots,m$. More complex policy parametrizations include deep neural networks~\citep{duan2016benchmarking}. 
Stochastic policies randomize over actions. In continuous settings, this is typically done by adding a Gaussian noise, \eg for a linear policy $a\sim\mathcal{N}(\vtheta^T\vphi(s),\Sigma)$, where $\Sigma\in\Reals^{D\times D}$ is a covariance matrix.\\
\newline
A common parametrization for countinuous actions, represented by real numbers, is the normal distribuion. The policy can be defined as the normal probability density over a real-valued scalar action, with mean and standard deviation given by parametric function approximators that depend on the state:
$$ \pi_{\theta}(a|s) = \frac{1}{\sigma(s, \theta)\sqrt{2\pi}}exp\Big( -\frac{(a -\mu(s, \theta))^2}{2\sigma(s, \theta)^2}\Big),$$
where $\mu(s, \theta) = \theta_{\mu}^{T} \vphi(s)$ and $\sigma(s, \theta) = exp\Big( \theta_{\sigma}^{T}\vphi(s) \Big)$. The policy's parameter vector is $\theta = [\theta_{\mu}, \theta_{\sigma}]^{T}$.

\subsection{Policy Gradient}
The most important algorithms in \ac{PS} are the ones that learn the parameterized policy $\pi_{\theta}, \theta \in \mathbb{R}^d$ based on the gradient of some scalar performance measure $J(\theta)$ with respect to the policy parameter $\theta$. These methods seek to maximize performance, so their updates approximate gradient ascent in J:
\begin{align}
\theta \leftarrow \theta + \alpha \widehat{\nabla J(\theta)}, \label{eq:grad}
\end{align}
where $\widehat{\nabla J(\theta)} \in \mathbb{R}^d$ is a stochastic estimate whose expectation approximates the gradient of the performance measure with respect to its argument $\theta$ and $\alpha$ is the step size, a scalar that controls the size of each update and can change through time. In policy gradient methods, the policy $\pi(a|s,\theta)$ has to be differentiable with respect to its parameters $\forall s \in \Sspace, a \in \Aspace, \theta \in \mathbb{R}^d$.\\
\newline
With continuous policy parametrization the action probabilities change smoothly as a function of the learned parameter, whereas in $\epsilon$-greedy selction the action probabilities may change dramatically for an arbitrary small change in the estimated action values, if that change results in a different action having the maximal value. Stronger convergence guarantees are available for policy-gradient methods than for action-value methods. In particular, the continuity of the policy dependence on the parameters enables policy-gradient methods to approximate gradient ascent. According to [\cite{Peters2008ReinforcementLO}], if the gradient estimate is unbiased and learning rates fulfill
$$\sum_{t=1}^{\infty}\alpha_t = \infty \quad \sum_{t=1}^{\infty}\alpha_t^2 < \infty$$ \todo{sightly different from the citation}
the learning process is guaranteed to converge to at least a local optimum. Policy-gradient methods can also incorporate domain knowledge in the policy definition and can be made safe by design.\\
\newline
Some methods also learn approximation to value-functions and are called \emph{actor-critic methods}, where "actor" is a reference to the learned policy, and "critic" refers to the learned value function. In these methods, the function approximation for value function introduces bias but reduces variance and accelerates learning.

\subsection{Policy Gradient Theorem}
It may seem challenging to change the policy parameter in a way that ensures improvement. The problem is that the performance, \ie the measure to maximize, depends on both the action selections and the distribution of states in which those selctions are made, and both of these are affected by the policy parameter. We have to estimate the performance gradient with respect to the policy parameter when the gradient depends on the unknown effect of policy changes. Fortunately, there is an excellent theoretical answer to this challenge in the form of the \emph{policy gradient theorem}, which provides an analytic expression for $\nabla_{\theta}J(\theta)$ that does not involve the derivative of the state distribution $\delta^{\theta}(s)$.\\
\newline
The theorem is from [\citet{Sutton1999PolicyGM}] and we enunciate it in the continuous case:
\begin{align}
\nabla_{\theta}J(\theta) = \int_s \delta^{\theta}(s) \int_a \nabla_{\theta} \pi(a|s) Q^{\theta}(s,a) \de a \de s. \label{eq:pgt}
\end{align}
Starting from~\eqref{eq:pgt}, the performance gradient can be estimated from samples in different ways. The estimated value is exactly the value used in~\eqref{eq:grad} to perform an update iteration of policy parameter $\theta$.

\subsection{Policy Gradient Algorithms}
In this section we present some important algorithms and we show their property that are interesting to compare these existing algorithms with our work.
\subsubsection{REINFORCE}
According to [\citet{Williams1992SimpleSG}] it is possible to estimate the gradient from a single trajectory: we replace in~\eqref{eq:pgt} the integral over $a$ with an expectation under $\pi$, and then sampling the expectation:
\begin{align}
\nabla_{\theta}J(\theta) &= \int_s \delta^{\theta}(s) \int_a \pi(a|s) \frac{\nabla_{\theta} \pi(a|s)}{\pi(a|s)} Q^{\theta}(s,a) \de a \de s \nonumber\\
&= \EV_{s_t \sim \delta^{\pi}, a_t \sim \pi} \Big[ \frac{\nabla_{\theta} \pi(a_t|s_t)}{\pi(a_t|s_t)} Q^{\theta}(s_t,a_t) \Big] \nonumber\\
&= \EV_{s_t \sim \delta^{\pi}, a_t \sim \pi} \Big[ \nabla_{\theta} \log\pi(a_t|s_t) r_t(s_t, a_t) \Big] \label{eq:rein}
\end{align}
where~\eqref{eq:rein} is obtained by considering $\nabla_{\theta} \log\pi(a_t|s_t) = \frac{\nabla_{\theta} \pi(a_t|s_t)}{\pi(a_t|s_t)}$ and $r_t(s_t, a_t) = \sum_{j = t}^T \gamma^{j-t} R(s_j, a_j)$.\\
\newline
From~\eqref{eq:pgt} we observe that if $b(s)$ does not depend on $a$ we can write:
$$ \nabla_{\theta}J(\theta) = \int_s \delta^{\theta}(s) \int_a \nabla_{\theta} \pi(a|s) \Big( Q^{\theta}(s,a) -b(s) \Big) \de a \de s. $$
Indeed: $\int_a b(s) \nabla_{\theta} \pi(a|s) \de a = b(s) \nabla_{\theta} \int_a \pi(a|s) \de a = b(s) \nabla_{\theta} 1 = 0.$
$b(s)$ is called baseline and allows to reduce the variance of the estimate, that considerably slow the convergence of the algorithm.
\todo{spiegare come vengono considerate somme/medie degli step della traiettoria}
A common baseline used is:
$$ b_k^h = \frac{\Big\langle\Big( \sum_{k=0}^{K}\nabla_{\theta} \log\pi^{\theta}(a_k|s_k)\Big)^2r_k \Big\rangle}{\Big\langle\Big( \sum_{k=0}^{K}\nabla_{\theta} \log\pi^{\theta}(a_k|s_k)\Big)^2\Big \rangle}
$$
and $Q^{\theta}(s,a)$ is replaced with:
$$ g_h = \Big\langle \sum_{l=0}^{H} \Big( \sum_{k=0}^{l} \nabla_{\theta} \log \pi^{\theta}(a_k|s_k)\Big)(r_l - b_l^h)\Big\rangle.
$$
This is one of the most common policy gradient algorithms and we use it in our work to set a comparison with a state of the art algorithm in policy search.

\subsubsection{PGPE}
\todo{spiegare (anche prima) likelihood gradient}
\acf{PGPE}[\citet{sehnke2008PolicyGradient}] estimates a likelihood gradient by sampling directly in parameter space, which reduces the variance of estimates compared to REINFORCE. This is due to the fact that once parameters are sampled, we can generate an entire action-state history (trajectory) while in REINFORCE we draw the action from the policy at each time step.\\
\newline
Indeed, we replace the stochastic policy with a probability distribution over the parameter themselves:
$$ \pi^{\theta}(a|s) = \int_{\Theta} p(\theta) \delta_{\mu^{\theta}(s)}\de \theta,
$$
where $\delta_{\mu^{\theta}(s)}$ is the Dirac delta function of a deterministic policy $\mu^{\theta}$ depending on parameter $\theta$ weighted for a distribution $p(\theta)$.

\subsubsection{DPG}
\acf{DPG} [\citet{article}] is a class of algorithms that learn a parametric deterministic policy. This is similar to what we want to achieve but the problem is that during the learning phase \ac{DPG} collect samples from a stochastic behavioural policy and then unexpected actions can be performed.\\
\newline
Policy gradient algorithms are perhaps the most popular class of continuous action reinforcement learning algorithms, starting from the results obtained by the \emph{policy gradient theorem}, we consider a deterministic policy $\mu_{\theta}: S \mapsto A$ with parameter vector $\theta \in \mathbb{R}^n$ and performance objective $J(\mu_{\theta}) = \mathbb{E}_{s \sim \delta^{\mu}}[r(s, \mu_{\theta}(s))]$. 
By applying the chain rule we see that the policy improvement may be decomposed:
$$
\nabla_{\theta} Q^{\mu}(s, \mu_{\theta}(s)) = \nabla_{\theta}\mu_{\theta}(s)\nabla_a Q^{\mu}(s, a)|_{a=\mu_{\theta}(s)}
$$
Suppose that $\nabla_{\theta}\mu_{\theta}(s)$ and $\nabla_aQ^{\mu}(s,a)$ exist, then:
$$
\nabla_{\theta}J(\mu_{\theta}) = \mathbb{E}_{s \sim \delta^{\mu}} [\nabla_{\theta} \mu_{\theta} (s) \nabla_a Q^{\mu}(s,a)|_{a = \mu_{\theta} (s)}]
$$
We use the deterministic policy gradient theorem to derive an off-policy actor-critic algorithm. It learns a deterministic target policy $\mu_{\theta}(s)$ from trajectories generated by an arbitrary stochastic behaviour policy $\pi(a|s)$. A critic estimates the action-value function $Q^w(s,a) \approx Q^{\mu}(s,a)$. 

\section{Special \NoCaseChange{\ac{MDPs}}}

\subsection{Bounded \ac{MDPs}}
A \acf{BMDP}~\citep[]{givan2000bounded} is a five-tuple $\langle \Sspace, \Aspace, P_{\updownarrow}, R_{\updownarrow}, \gamma \rangle$, where $\Sspace$, $\Aspace$ and $\gamma$ are defined as for (finite) MDPs, and $P_{\updownarrow}, R_{\updownarrow}$ are analogous to the MDP transition and reward functions, but yield closed real intervals instead of real values: given a lower bound $\underline{P}$ and an upper bound $\overline{P}$, $P_{\updownarrow} = [\underline{P}; \overline{P}]$. Similarly we can specify the interval $R_{\updownarrow}$. This can be used to model uncertainty on the true nature of a decision process. To ensure that $P_{\updownarrow}$ admits only well-formed transition functions, we require that for any action $a$ and state $s$, the sum of the lower bounds of $P_{\updownarrow}(s'|s,a)$ over all states $s'$ must be less than or equal to one, while the upper bounds must sum to a value greater than or equal to one.\\
\newline
A BMDP $M_{\updownarrow} = \langle \Sspace, \Aspace, P_{\updownarrow}, R_{\updownarrow}, \gamma \rangle$ defines a set of exact MDPs. For any exact MDP $M = \langle \Sspace', \Aspace', P', R', \gamma' \rangle$, we have $M \in M_{\updownarrow}$ if $\Sspace = \Sspace', \Aspace = \Aspace', \gamma = \gamma'$, and for any action $a$ and states $s, s', R'(s,a)$ belongs to the interval $R_{\updownarrow}(s,a)$ and $P'(s'|s,a)$ belongs to the interval $P_{\updownarrow}(s'|s,a)$. An interval value function $V_{\updownarrow}$ is a mapping from states to closed real intervals. We use such functions to indicate that the value of a given state for any exact MDP falls within the selected interval. As in the case of (exact) value functions, interval value functions are specified \wrt a fixed policy $\pi$, \ie:
$$V_{\updownarrow, \pi}(s) = \Big[ \underline{V}_{\pi}(s), \overline{V}_{\pi}(s)\Big] = \Big[ \min_{M \in M_{\updownarrow}} V_{M,\pi}(s), \max_{M \in M_{\updownarrow}} V_{M,\pi}(s)\Big].$$
According to~\citep{givan2000bounded}, there exists in $M_{\updownarrow}$ an \ac{MDP} that simultaneously achieves $\underline{V}_{\pi}(s)$ for all $s \in \Sspace$ and another one that achieves $\overline{V}_{\pi}(s)$ for all $s \in \Sspace$.\\
\newline
The notion of optimal value function in \ac{BMDP}s requires an ordering rule for intervals. We can define two different possible orderings:
\begin{align}
[l_1, u_1] \leq_{pes} [l_2, u_2] &\Leftrightarrow 
\begin{cases}
l_1 < l_2 \text{, or }\\
l_1 = l_2 \text{ and } u_1 \leq u_2
\end{cases} \label{eq:pes}\\
[l_1, u_1] \leq_{opt} [l_2, u_2] &\Leftrightarrow 
\begin{cases}
u_1 < u_2 \text{, or }\\
u_1 = u_2 \text{ and } l_1 \leq l_2
\end{cases} \label{eq:opt}
\end{align}
We use these orderings rules to partially order interval value functions in the following way:
$$V_{1 \updownarrow} \leq V_{2 \updownarrow} \iff V_{1 \updownarrow}(s) \leq_{*} V_{2 \updownarrow}(s) \quad \forall s \in \Sspace,$$
with $\leq_{*}$ defined as $\leq_{pes}$ in~\eqref{eq:pes} or $\leq_{opt}$ in~\eqref{eq:opt}.\\
\newline
As stated in~\citep{givan2000bounded}, there exists at least one optimistically and one pessimistically optimal policy:
\begin{align*}
V^{*}_{\updownarrow \text{opt}} = \max_{\pi \in \Pi} V_{\updownarrow, \pi} \text{ using} \leq_{opt} \text{ to order interval value functions,}\\
V^{*}_{\updownarrow \text{pes}} = \max_{\pi \in \Pi} V_{\updownarrow, \pi} \text{ using} \leq_{pes} \text{ to order interval value functions.}
\end{align*}
In order to better understand the meaning of the optimal policies, we consider a game in which we choose a policy $\pi$ and then a second player chooses an \ac{MDP} $M \in M_{\updownarrow}$ to evaluate the policy. $\overline{V}^{*}_{opt}$ is the best value function we can obtain if the second player cooperates in the game, $\underline{V}^{*}_{pes}$ is the best value function obtainable if the second player is an adversary.\\
\newline
We define the \acf{IVI} algorithm to compute optimal policy value intervals in a similar way to the standard value iteration presented in \mySubsec{subsec:vi}. In \ac{IVI} the updating rule is:
\begin{align}
V_{\updownarrow, k+1}(s) = \max_{a \in \Aspace, \leq_{*}} \Big[ \min_{M \in M_{\updownarrow}} VI_{M, a}(\underline{V}_{k})(s), \max_{M \in M_{\updownarrow}} VI_{M, a}(\overline{V}_{k})(s) \Big]. \label{eq:ivi}
\end{align}
$\leq_{*}$ can be $\leq_{pes}$ or $\leq_{opt}$. Given an \ac{MDP} $M$ with transition function $P$ and reward function $R$, an action $a \in \Aspace$ and a value function $v$, $VI_{M, a}(v)(s)$ is a single iteration of policy evaluation, where the action is fixed and 
\begin{align} 
VI_{M, a}(v)(s) = R(s,a) + \gamma \sum_{s' \in \Sspace} P(s'|s,a)v(s'). \label{eq:vi}
\end{align}
In~\eqref{eq:ivi} it is required to perform two iterations VI. The two value functions used to perform the iterations (\ie the functions used instead of $v$ in~\eqref{eq:vi}) are obtained from the interval value function $V_{\updownarrow, k}$. For the first iteration, $v = \underline{V}_{k}$, with $\underline{V}_{k}$ the lower bounds in $V_{\updownarrow, k}$; for the second one $v = \overline{V}_{k}$ with $\overline{V}_{k}$ the upper bounds in $V_{\updownarrow, k}$.\\
\newline
We can select the \ac{MDP} $M$ that minimizes (maximizes) the expressions under $\min (\max)$ operator in~\eqref{eq:ivi} by computing an exact transition function $P$ from the interval transition function $P_{\updownarrow}$ of $M_{\updownarrow}$. In order to do that, the arriving states $s' \in \Sspace$ are sorted in increasing (decreasing) order according to their $\underline{V}(\overline{V})$. Then, for all the state-action pairs $(s,a) \in \Sspace \times \Aspace$ and given an order of arriving states $s'_1, s'_2, ..., s'_k$, we calculate the index $r$, with $1 \leq r \leq k$, that maximizes the following expression without letting it exceed 1:
$$\sum_{i=1}^{r-1}\overline{P}(s'_i|s,a) + \sum_{i=r}^{k}\underline{P}(s'_i|s,a).$$
The exact transition function $P(.|s,a)$ is defined by assigning the upper bound $\overline{P}(s'|s,a)$ to the transition probabilities involving states $s'$ with an index lower than $r$, the lower bound $\underline{P}(s'|s,a)$ to the transition probabilities involving states $s'$ with an index greater than $r$ and the probability that ensures $\sum_{s' \in \Sspace}P(s'|s,a) = 1$ to the state with index $r$.

\subsection{Lipschitz \ac{MDPs}}
We introduce the notion of \emph{Lipschitz continuity} in order to define some properties of regularity in the \ac{MDP}. These properties can be exploited in policy gradient algorithms, in (\citet{pirotta2015policy}) is shown how they can ensure a performance improvement at each iteration of policy-parameter updates. In this section we provide basic concepts related to Lipschitz continuity and useful bounds that will be exploited in our work.
\begin{definition}[Lipschitz continuity]
Given two metric sets $(X, d_X)$ and $(Y, d_Y)$ where $d_X$ and $d_Y$ denote the corresponding metric functions, a function $f: X \rightarrow Y$ is called $L_f$-\acf{LC} if
\begin{align} \forall(x_1, x_2) \in X^2, d_Y(f(x_1), f(x_2)) \leq L_f d_X(x_1, x_2). \label{eq:lip} \end{align}
\end{definition}
The smallest $L_f$ for which~\eqref{eq:lip} holds is called the Lipschitz constant of $f$. For real-valued functions (\eg the reward function $R$), we use the Euclidean distance as metric for the codomain distance. For the transition function $P$ and the policies $\pi$ we need to introduce a distance between probability distributions. We consider the Kantorovich or $L^1$-Wasserstein metric on probability measures, defined as follows.
\begin{definition}[Kantorovich metric]
Given two probability measures $p$ and $q$, the Kantorvich measure $\Kant(p, q)$ is:
$$ \Kant(p, q) = \sup_f\Big\{ \Big| \int_x f(x) d\Big(p(x)-q(x)\Big) \de x \Big|: L_f \leq 1 \Big\}.$$
\end{definition}
In this work, we restrict our attention to Lipschitz-continuous MDPs and policies. We make similar assumptions as in~\citep{pirotta2015policy}. For the MDP, we require both the continuity of the transition model and the reward:
%
\begin{assumption}[Lipschitz MDP]\label{ass:lipmdp}
	For all $s,\wt{s}\in\Sspace$ and $a,\wt{a}\in\Aspace$:
	\begin{align}
	&\Kant\left(P(\cdot|s,a), P(\cdot| \wt{s},\wt{a})\right) \leq L_{P}d_{\Sspace\Aspace}\left((s,a),(\wt{s},\wt{a})\right), \\
	&\left|R(s,a) - R(\wt{s},\wt{a})\right| \leq L_{R}d_{\Sspace\Aspace}\left((s,a),(\wt{s},\wt{a})\right),
	\end{align}
	for some positive real constants $L_{P}$ and $L_{R}$.
\end{assumption}
%
where $d_{\Sspace\Aspace}\left((s,a),(\wt{s},\wt{a})\right) = \norm{s-\wt{s}} + \norm{a-\wt{a}}$ is the taxicab norm on $\Sspace\times\Aspace$.
We also require our deterministic policy to be continuous both \wrt the input state and its parameters:
%
\begin{assumption}[Lipschitz Policies]\label{ass:lippol}
	For all $s,\wt{s}\in\Sspace$ and $\vtheta,\wt{\vtheta}\in\Theta$:
	\begin{align}
	&\norm{\pi_{\vtheta}(s) - \pi_{\vtheta}(\wt{s})} \leq L_{\pi_{\vtheta}}\norm{s-\wt{s}}, \\
	&\norm{\pi_{\vtheta}(s) - \pi_{\wt\vtheta}(s)} \leq L_{\Theta}\norm{\vtheta-\wt{\vtheta}},
	\end{align}
	for some positive real constants $\{L_{\pi_{\vtheta}}\}_{\vtheta\in\Theta}$ and $L_{\Theta}$.
\end{assumption}
%
We use the euclidean norm to measure distances on $\Sspace$, $\Aspace$ and $\Theta$, but everything works for general metrics.
In the following, we will always assume that ${L_{P}(1+L_{\pi_{\vtheta}}) < \gamma^{-1}}$.
These assumptions are enough to guarantee the continuity of the value functions \wrt states and actions:
%
\begin{lemma}[\citet{rachelson2010locality}]\label{lem:lipval}
	Under Assumptions~\ref{ass:lipmdp} and~\ref{ass:lippol}, for all $s,\wt{s}\in\Sspace$, $a,\wt{a}\in\Aspace$ and $\vtheta\in\Theta$:
	\begin{align}
	&\left|V^{\vtheta}(s) - V^{\vtheta}(\wt{s})\right| \leq L_{V^{\vtheta}}\norm{s-\wt{s}}, \\
	&\left|Q^{\vtheta}(s,a) - Q^{\vtheta}(\wt{s},\wt{a})\right| \leq L_{Q^{\vtheta}}d_{\Sspace\Aspace}\left((s,a),(\wt{s},\wt{a})\right),
	\end{align}
	where $L_{Q^{\vtheta}} = \frac{L_{R}}{1-\gamma L_{P}(1+L_{\pi_{\vtheta}})}$ and $L_{V^{\vtheta}} = L_{Q^{\vtheta}}(1+L_{\pi_{\vtheta}})$.
\end{lemma}
%
and also of the future-state distributions \wrt policy parameters:
%
\begin{lemma}[\citet{pirotta2015policy}]\label{lem:lipfut}
	Under Assumptions~\ref{ass:lipmdp} and~\ref{ass:lippol}, for all $\vtheta,\wt\vtheta\in\Theta$:
	\begin{align}
	&\Kant\left(\delta^{\vtheta},\delta^{\wt{\vtheta}}\right) \leq L_{\delta^{\vtheta}}\norm{\vtheta-\wt{\vtheta}},
	\end{align}
	where $L_{\delta^{\vtheta}} = {\gamma L_{P}L_{\pi_{\vtheta}}}\big/{\left(1-\gamma L_{P}(1+L_{\pi_{\vtheta}})\right)}$.
\end{lemma}

\section{Safe Reinforcement Learning}\label{sec:saferl}

\section{State discretization}