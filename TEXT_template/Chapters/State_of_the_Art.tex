\chapter{State of the Art} \label{chap:stateoftheart}

\section{Reinforcement Learning}
\todo{in introduction?}
According to (\cite{sutton2018reinforcement}), \acf{RL} is learning what to do so as to maximize a numerical reward signal. The learner must discover which actions yield the most reward by trying them. Actions may affect not only the immediate reward but also the next situation and all subsequent rewards. The problem of \ac{RL} is formalized with ideas coming from dynamical systems theory, specifically with the \acf{MDP} that we detail in \mySubsec{subsec:mdp}. One of the challenges that arise in \ac{RL} is the trade-off between exploration and exploitation. To obtain an high reward, an agent must perform actions that it has found to be effective in producing reward. But to discover such actions, it has to try actions that it has not selected before.

\subsection{Markov Decision Processes}\label{subsec:mdp}
\acf{MDPs} are a mathematical framework used for modeling \ac{RL} problems in which an \emph{agent}, by interacting with an \emph{environment}, learns how to achieve a goal. \ac{MDPs} are a formalization of sequential decision making, where the decision taken by the agent influences immediate and future rewards. The agent interacts with the environment by selecting the actions to perform and receiving rewards and information on the new situation. Rewards are provided by the environment in the form of scalar values, the agent aims to maximize the sum of them over time. More specifically, at each time step $t$, the agent receives a representation of the state $s_t \in \Sspace$, on that basis it selects an action $a_t \in \Aspace$. One time step later, the agent receives a reward $r_{t+1}$ and finds itself in a new state $s_{t+1} \in \Sspace$.\\
\newline
% MDP
We define the \ac{MDP} according to~\citep[]{puterman2014markov}. In general, the state space $\Sspace$ and the action space $\Aspace$ can be finite or continuous sets. We focus on the solution of continuous \ac{MDPs} as these are the most suitable for modeling continuous control problems.
\begin{definition}\label{def:mdp}
The \ac{MDP} is described by a five-tuple $M=\langle \Sspace, \Aspace, P, R, \gamma \rangle$, where:
\begin{itemize}
	\item $\Sspace$ is the state space, with $\Sspace\subseteq\Reals^N$.
	\item $\Aspace$ is the action space, with $\Aspace\subseteq\Reals^D$.
	\item $P: \Sspace \times \Aspace \to \Delta(\Sspace)$ is the transition function, with $P(s'|s,a)$ denoting the probability of reaching state $s'$ from state $s$ by taking action $a$. For any state-action pair $(s,a)$, it holds the following equality:
	$$ \sum_{s' \in \Sspace} P(s'|s,a) = 1.$$
	\item $R: \Sspace \times \Aspace \to \Reals$ is the reward function, with $R(s,a)$ denoting the expected reward from taking action $a$ in state $s$.
	\item $\gamma \in [0, 1)$ is the discount factor, eventually used to discount the present effect of future rewards.
\end{itemize}
\end{definition}
The transition function $P$ defines the dynamics of the \ac{MDP} and ensures the \emph{Markov property}: the probability of reaching $s_t$ depends only on the immediately previous state and action, $s_{t-1}$ and $a_{t-1}$, and not on earlier states and actions.
\subsubsection{Policy and Value Functions}
The behaviour of the agent is modeled with a \emph{policy} $\pi: \Sspace \mapsto \Delta(\Aspace)$, \ie a mapping from states to probabilities of selecting each possible action. The agent learns a policy according to its goal of maximizing the sum of rewards collected during the task. Specific details on how to learn the optimal policy are given later, in \mySubsec{subsec:solv} and \mySec{sec:ps}.\\
The sum of rewards obtained if we consider time steps from $k=t$ to a certain $k=T$ is called \emph{return} $G_t$. In general, the return is defined as a sum of discounted rewards:
\todo{Uso G come nome?}
$$G_t = \sum_{k=t+1}^{T} \gamma^{k-t-1} r_k, $$where $r_k$ is the reward obtained at step $k$. The last time step $T$ represents the horizon of the task and can be finite or infinite. In the first case, the task is said to be \emph{episodic} and the agent-environment interaction breaks naturally into episodes. In the second case (\ie $T = \infty$), the task is said to be \emph{continuous} and a discount factor $\gamma<1$ is required in order to obtain a finite value for return $G_t$.\\
\newline
In this work, we denote with $\pi(a|s)$ the probability of performing the action $a$ in the state $s$, according to the policy $\pi$. Since $\pi(s)$ is a distribution of probability, the following equality holds:
$$\sum_{a \in \Aspace} \pi(a|s) = 1 \quad \forall s \in \Sspace.$$
If, for each state $s \in \Sspace$, there exists an action $a$ such that $\pi(a|s) = 1$, the policy is deterministic.\\
\newline
Given a policy $\pi$, it is possible to compute the \emph{value function} $V^{\pi}: \Sspace \rightarrow \mathbb{R}$, a widely used function in \ac{RL} that measures how good it is for the agent to be in a given state $s \in \Sspace$ according to the expected return obtainable from that state. Since the rewards the agent can expect to receive in the future depend on the taken actions, the value function is defined with respect to policies: $V^{\pi}(s)$ is the expected sum of discounted rewards that the agent collects by starting at state $s$ and following policy $\pi$. The value function can be defined recursively via the Bellman equation: $$V^{\pi}(s) = \int_{\Aspace} \pi(a|s) \Big(R(s,a) + \gamma \int_{\Sspace}P(s'|s,a)V^{\pi}(s')\de s' \Big) \de a.$$
For control purposes, we can also define an action-value function: $$Q^{\pi}(s,a) = R(s,a) + \gamma \int_{\Sspace} P(s'|s,a) \int_{\Aspace} \pi(a'|s') Q^{\pi}(s',a') \de a' \de s'.$$
It represents the expected return obtained from taking the action $a$ in state $s$ and then following the policy $\pi$. \\

\subsubsection{Performance Measure}
In order to evaluate how good a policy $\pi$ is, we consider the expected reward obtained starting from a state $s \in \Sspace$ sampled from the initial-state distribution $\mu$ and following the policy $\pi$.\\
The \emph{performance measure} $J(\pi)$ expressed in the form of an expected value is:
$${J(\pi) = (1 - \gamma)^{-1}\EV_{s\sim\delta^{\pi}, a \sim \pi}R(s,a)} = \EV_{s\sim\mu}V^{\pi}(s),$$
where $\delta^{\pi}$ is the $\gamma$-discounted future-state distribution. This function is defined as:
$$\delta^{\pi}(s) = (1 - \gamma)\EV_{s_{0}\sim\mu}\sum_{t=0}^{\infty}\gamma^t P^{\pi}(S_t = s|S_0=s_0)$$
and represents the probability of being in a certain state $s$ during the execution of the task, provided that the policy is $\pi$ and the initial state distribution is $\mu$.\\
The performance measure can be used to identify the optimal policy that we want to learn in the \ac{RL} problem as:
$$ \pi^{*} \in \arg \max_{\pi} J(\pi).$$

\subsection{Solving a \ac{RL} task}\label{subsec:solv}
Solving a \ac{RL} task means finding a policy that maximizes the return obtained by the agent in the task. First, we consider finite \ac{MDPs}. They are a subset of the \ac{MDPs} defined in \ref{def:mdp}. In finite \ac{MDPs}, $\Sspace$ and $\Aspace$ are finite sets of discrete values and the expressions reported so far for continuous \ac{MDPs} can be used for finite \ac{MDPs} if we replace integrals with summations.\\
Policies can be partially ordered according to their value function: 
$$\pi \geq \pi' \iff V^{\pi}(s) \geq V^{\pi'}(s) \forall s \in \Sspace.$$ 
In finite \ac{MDPs} there always exists a deterministic optimal policy $\pi^{*}$. The value function $V^{*}$ computed according to $\pi^{*}$ has the following property:
$$ V^{*}(s) = \max_{\pi}V^{\pi}(s) \quad \forall s \in \Sspace.$$
Optimal functions can be written with the Bellman optimality equations:
\begin{align*}
V^{*}(s) &= \max_a \sum_{s'}P(s'|s,a) \Big( R(s,a) + \gamma V^{*}(s') \Big)\\
Q^{*}(s,a) &= \sum_{s'}P(s'|s,a) \Big( R(s,a) + \gamma \max_{a'} Q^{*}(s', a') \Big).
\end{align*}
If we have a \emph{complete knowledge} of the environment, \ie the agent has a model of transition and reward functions of the \ac{MDP}, we can use dynamic programming to solve the \ac{RL} problem (\mySubsec{subsec:dp}). Instead, in the case of \emph{incomplete knowledge} the unknown functions can be estimated an the problem is similarly solved (\mySubsec{subsec:alt}). All of these algorithms are called \emph{tabular} because the state and action spaces are small enough for the value functions to be represented in a tabular format. In larger problems, we can use function approximators instead of value functions or directly search the optimal policy in the policy space, without using any value function (\mySec{sec:ps}).

\subsection{Dynamic Programming}\label{subsec:dp}
\acf{DP} is a collection of algorithms used to compute optimal policies given a knowledge on the environment. It is generally applied to discrete \ac{MDP}s because \ac{DP} is computational expensive and requires that the $P$ and $R$ functions of the \ac{MDP} are known.\\
\newline
There are two possibilities to obtain the optimal value functions: \emph{policy iteration} and \emph{value iteration}.

\subsubsection{Policy Iteration}
Policy iteration starts from an initial policy $\pi_0$ and then alternates two phases of the algorithm: \emph{policy evaluation} and \emph{policy iprovement}. The idea is to obtain $V_{*}$ through the sequence:
$$ \pi_{0} \xrightarrow{\text{E}} V_{\pi_{0}} \xrightarrow{\text{I}} \pi_{1} \xrightarrow{\text{E}} V_{\pi_{1}} \xrightarrow{\text{I}} \pi_{2} \xrightarrow{\text{E}} ... \xrightarrow{\text{I}} \pi_{*} \xrightarrow{\text{E}} V_{*}.
$$
The first phase consists in compute the value function for every state, given the current policy $\pi$. In the second phase the policy is updated according to the new values of the value function.
In policy evaluation the value function is computed for every state as:
$$
V(s) = \sum_{s'}P(s'|s,\pi(s)) \Big( R(s,\pi(s)) + \gamma V(s') \Big),
$$
until each $V(s)$ converges. Then, for each state the policy is computed according to the following rule:
$$ \pi(s) = \arg \max_{a} \sum_{s'} P(s'|s,a) \Big( R(s,a) + \gamma V(s') \Big).$$
\subsubsection{Value Iteration}
The main drawback in policy iteration is that, for each policy evaluation, multiple iterations are required. If we assume to truncate the policy evaluation after just one update for each state we obtain a special case of policy iteration, called value iteration. In value iteration, the value function at each step is updated as follows:
$$ V_{k+1}(s) = \max_a \sum_{s'}P(s'|s,a) \Big( R(s,a) + \gamma V_{k}(s') \Big),
$$
for all $s \in \Sspace$. The sequence ${v_{k}}$ can be shown to converge to $V_{*}$ under the same conditions that guarantee the existence of $V_{*}$.
\subsubsection{Limit of $\ac{DP}$}
If the state space is very large,  than even a single update of $V(s) \forall s \in \Sspace$ can be prohibitively expensive. Furthemore, \ac{DP} requires a complete knowledge on the \ac{MDP} that is not always possible. In continuous \ac{MDP}s the methods presented above are not feasible. A solution is \emph{policy serach}, presented in \mySec{sec:polser}.

\subsection{Alternatives to \ac{DP}}\label{subsec:alt}

\subsection{Bounded MDP}
A \acf{BMDP}~\citep[]{givan2000bounded} is a five-tuple $\langle \Sspace, \Aspace, P_{\updownarrow}, R_{\updownarrow}, \gamma \rangle$, where $\Sspace$, $\Aspace$ and $\gamma$ are defined as for (finite) MDPs, and $P_{\updownarrow}, R_{\updownarrow}$ are analogous to the MDP transition and reward functions, but yield closed real intervals instead of real values. This can be used to model uncertainty on the true nature of a decision process. To ensure that $P_{\updownarrow}$ admits only well-formed transition functions, we require that for any action $a$ and state $s$, the sum of the lower bounds of $P_{\updownarrow}(s'|s,a)$ over all states $s'$ must be less than or equal to one, while the upper bounds must sum to a value greater than or equal to one. A BMDP $M_{\updownarrow} = \langle \Sspace, \Aspace, P_{\updownarrow}, R_{\updownarrow}, \gamma \rangle$ defines a set of exact MDPs. For any exact MDP $M = \langle \Sspace', \Aspace', P', R', \gamma' \rangle$, we have $M \in M_{\updownarrow}$ if $\Sspace = \Sspace', \Aspace = \Aspace', \gamma = \gamma'$, and for any action $a$ and states $s, s', R'(s,a)$ belongs to the interval $R_{\updownarrow}(s,a)$ and $P'(s'|s,a)$ belongs to the interval $P_{\updownarrow}(s'|s,a)$. An interval value function $V_{\updownarrow}$ is a mapping from states to closed real intervals. We use such functions to indicate that the value of a given state for any exact MDP falls within the selected interval. As in the case of (exact) value functions, interval value functions are specified \wrt a fixed policy $\pi$, \ie:
$$V_{\updownarrow, \pi}(s) = \Big[ \underline{V_{\pi}}(s), \overline{V_{\pi}}(s)\Big] = \Big[ \min_{M \in M_{\updownarrow}} V_{M,\pi}(s), \max_{M \in M_{\updownarrow}} V_{M,\pi}(s)\Big].$$
According to ~\citep{givan2000bounded}, there exists in $M_{\updownarrow}$ an \ac{MDP} that simultaneously achieves $\underline{V_{\pi}}(s) \forall s$ and another one that achieves $\overline{V_{\pi}}(s) \forall s$.\\
\newline
In order to define an optimal value function for a \ac{BMDP} we have to clarify the ordering between intervals. There are two possibilities:
\begin{align*}
[l_1, u_1] \leq_{pes} [l_2, u_2] &\Leftrightarrow 
\begin{cases}
l_1 < l_2 \text{, or }\\
l_1 = l_2 \text{ and } u_1 \leq u_2
\end{cases}\\
[l_1, u_1] \leq_{opt} [l_2, u_2] &\Leftrightarrow 
\begin{cases}
u_1 < u_2 \text{, or }\\
u_1 = u_2 \text{ and } l_1 \leq l_2
\end{cases}
\end{align*}
We extend these orderings to interval value functions by relating two value functions $V_{1 \updownarrow} \leq V_{2 \updownarrow}$ only when $V_{1 \updownarrow}(s) \leq V_{2 \updownarrow}(s)$ for every state $s$.
There exists at least one optimistically and one pessimistically optimal policy:
\begin{align*}
V_{\updownarrow \text{opt}} = \max_{\pi \in \Pi} V_{\updownarrow, \pi} \text{ using} \leq_{opt} \text{ to order interval value functions}\\
V_{\updownarrow \text{pes}} = \max_{\pi \in \Pi} V_{\updownarrow, \pi} \text{ using} \leq_{pes} \text{ to order interval value functions}
\end{align*}
Now we can modify the value iteraion so that it computes optimal policy value intervals:
\begin{align*}
IVI_{\updownarrow \text{opt}}(V_{\updownarrow})(s) = \max_{a \in \Aspace, \leq_{opt}} \Big[ \min_{M \in M_{\updownarrow}} VI_{M, a}(\underline{V})(s), \max_{M \in M_{\updownarrow}} VI_{M, a}(\overline{V})(s) \Big]\\
IVI_{\updownarrow \text{pes}}(V_{\updownarrow})(s) = \max_{a \in \Aspace, \leq_{pes}} \Big[ \min_{M \in M_{\updownarrow}} VI_{M, a}(\underline{V})(s), \max_{M \in M_{\updownarrow}} VI_{M, a}(\overline{V})(s) \Big]
\end{align*}
where $VI_{a}(v)(s) = R(s,a) + \gamma \sum_{s' \in \Sspace} P(s'|s,a)v(s')$ and $P(.|s,a)$ depends on the $M$ selected to perform $VI$.\\
\newline
In order to select the $M$ that minimizes (maximizes) the expression, the arriving states $s'$ are sorted into increasing (decreasing) order according to their $\underline{V}(\overline{V})$, and then the transition probability is chosen so as to send as much probability mass to the states early in the ordering without exceeding 1. Given the order of arriving states $s'_1, s'_2, ..., s'_k$, the index $r$ with $1 \leq r \leq k$ maximizes the following expression without letting it exceed 1:
$$\sum_{i=1}^{r-1}\overline{P}(s'_i|s,a) + \sum_{i=r}^{k}\underline{P}(s'_i|s,a).$$
The transition function is defined by assigning the upper bound to the states below $r$, the lower bound to the states above $r$ and the rest of probability to the state in index $r$.

\subsection{Lipschitz MDP}
We introduce the notion of \emph{Lipschitz continuity} to express properties of regularity in the \ac{MDP} related to some metric distances. Given two metric sets $(X, d_X)$ and $(Y, d_Y)$ where $d_X$ and $d_Y$ denote the corresponding metric functions, a function $f: X \rightarrow Y$ is called $L_f$-Lipschitz continuous if
\begin{align} \forall(x_1, x_2) \in X^2, d_Y(f(x_1), f(x_2)) \leq L_f d_X(x_1, x_2). \label{eq:lip} \end{align}
The smalles $L_f$ for which~\eqref{eq:lip} holds is called the Lipschitz constant of $f$. For real-valued functions (\eg the reward function), we will use the Euclidean distance as metric for the codomain. For the state-transition model and the policies we need to introduce a distance between probability distributions. We consider the Kantorovich or $L^1$-Wasserstein metric on probability measures $p$ and $q$:
$$ \Kant(p, q) = \sup_f\Big\{ \Big| \int_x f d(p-q) \Big|: L_f \leq 1 \Big\}.$$
In this work, we restrict our attention to Lipschitz-continuous MDPs and policies. We make similar assumptions as in~\citep{pirotta2015policy}. For the MDP, we require both the continuity of the transition model and the reward:
%
\begin{assumption}[Lipschitz MDP]\label{ass:lipmdp}
	For all $s,\wt{s}\in\Sspace$ and $a,\wt{a}\in\Aspace$:
	\begin{align}
	&\Kant\left(P(\cdot|s,a), P(\cdot| \wt{s},\wt{a})\right) \leq L_{P}d_{\Sspace\Aspace}\left((s,a),(\wt{s},\wt{a})\right), \\
	&\left|R(s,a) - R(\wt{s},\wt{a})\right| \leq L_{R}d_{\Sspace\Aspace}\left((s,a),(\wt{s},\wt{a})\right),
	\end{align}
	for some positive real constants $L_{P}$ and $L_{R}$.
\end{assumption}
%
where $\Kant(\cdot,\cdot)$ denotes the Kantorovich (or $1$-Wasserstein) distance between probability distributions and $d_{\Sspace\Aspace}\left((s,a),(\wt{s},\wt{a})\right) = \norm{s-\wt{s}} + \norm{a-\wt{a}}$ is the taxicab norm on $\Sspace\times\Aspace$.
We also require our deterministic policy to be continuous both \wrt the input state and its parameters:
%
\begin{assumption}[Lipschitz Policies]\label{ass:lippol}
	For all $s,\wt{s}\in\Sspace$ and $\vtheta,\wt{\vtheta}\in\Theta$:
	\begin{align}
	&\norm{\pi_{\vtheta}(s) - \pi_{\vtheta}(\wt{s})} \leq L_{\pi_{\vtheta}}\norm{s-\wt{s}}, \\
	&\norm{\pi_{\vtheta}(s) - \pi_{\wt\vtheta}(s)} \leq L_{\Theta}\norm{\vtheta-\wt{\vtheta}},
	\end{align}
	for some positive real constants $\{L_{\pi_{\vtheta}}\}_{\vtheta\in\Theta}$ and $L_{\Theta}$.
\end{assumption}
%
We use the euclidean norm to measure distances on $\Sspace$, $\Aspace$ and $\Theta$, but everything works for general metrics.
In the following, we will always assume that ${L_{P}(1+L_{\pi_{\vtheta}}) < \gamma^{-1}}$.
These assumptions are enough to guarantee the continuity of the value functions \wrt states and actions:
%
\begin{lemma}[\citet{rachelson2010locality}]\label{lem:lipval}
	Under Assumptions~\ref{ass:lipmdp} and~\ref{ass:lippol}, for all $s,\wt{s}\in\Sspace$, $a,\wt{a}\in\Aspace$ and $\vtheta\in\Theta$:
	\begin{align}
	&\left|V^{\vtheta}(s) - V^{\vtheta}(\wt{s})\right| \leq L_{V^{\vtheta}}\norm{s-\wt{s}}, \\
	&\left|Q^{\vtheta}(s,a) - Q^{\vtheta}(\wt{s},\wt{a})\right| \leq L_{Q^{\vtheta}}d_{\Sspace\Aspace}\left((s,a),(\wt{s},\wt{a})\right),
	\end{align}
	where $L_{Q^{\vtheta}} = \frac{L_{R}}{1-\gamma L_{P}(1+L_{\pi_{\vtheta}})}$ and $L_{V^{\vtheta}} = L_{Q^{\vtheta}}(1+L_{\pi_{\vtheta}})$.
\end{lemma}
%
and also of the future-state distributions \wrt policy parameters:
%
\begin{lemma}[\citet{pirotta2015policy}]\label{lem:lipfut}
	Under Assumptions~\ref{ass:lipmdp} and~\ref{ass:lippol}, for all $\vtheta,\wt\vtheta\in\Theta$:
	\begin{align}
	&\Kant\left(\delta^{\vtheta},\delta^{\wt{\vtheta}}\right) \leq L_{\delta^{\vtheta}}\norm{\vtheta-\wt{\vtheta}},
	\end{align}
	where $L_{\delta^{\vtheta}} = {\gamma L_{P}L_{\pi_{\vtheta}}}\big/{\left(1-\gamma L_{P}(1+L_{\pi_{\vtheta}})\right)}$.
\end{lemma}

\section{Policy Search}\label{sec:ps}
In policy search the problem of computing a value function for each state is avoided . The optimal policy is directly searched in the set of possible policies, that depends on the parametrization.\\
\newline
In this work, we focus on the optimization of deterministic parametric policies of the form $\pi_{\vtheta}:\Sspace\to\Aspace$, with $\vtheta\in\Theta\subseteq\Reals^m$. We will often abbreviate $\pi_{\vtheta}$ as $\vtheta$ in subscripts and function arguments, \eg $V^{\vtheta} \equiv V^{\pi_{\vtheta}}$, $J(\vtheta) \equiv J(\pi_{\vtheta})$. The goal of policy optimization is to find $\vtheta^{*}\in\arg\max_{\vtheta\in\Theta}J({\vtheta})$.
The simplest way of parametrizing $\pi_{\vtheta}$ is by means of a linear mapping. The linear policy is defined as $\pi_{\vtheta}(s) = \vtheta^T\vphi(s)$, where $\vtheta\in\Reals^{m\times D}$ and $\vphi:\Sspace\to\Reals^m$ is a feature function. This can be the state itself or, for instance, a set of radial basis functions (RBF). An example of RBF is the Gaussian $\phi_i(s; \mu_i, \sigma_i) = \exp\left\{-{(s -\mu_i)^2}\big/{(2\sigma_i^2)}\right\}$, where $\mu_i$ and $\sigma_i$ are hyperparameters, $i=1,\dots,m$. More complex policy parametrizations include deep neural networks~\citep{duan2016benchmarking}. 
Stochastic policies randomize over actions. In continuous settings, this is typically done by adding a Gaussian noise, \eg for a linear policy $a\sim\mathcal{N}(\vtheta^T\vphi(s),\Sigma)$, where $\Sigma\in\Reals^{D\times D}$ is a covariance matrix.

\subsection{Policy Gradient}
The most important algorithms in policy search are the ones that learn the policy parameters based on the gradient of some scalar performance measure $J(\theta)$, with $\theta \in \mathbb{R}^d$. These methods seek to maximize performance, so their updates approximate gradient ascent in J:
\begin{align}
\theta \leftarrow \theta + \alpha \widehat{\nabla J(\theta)}, \label{eq:grad}
\end{align}
where $\widehat{\nabla J(\theta)} \in \mathbb{R}^d$ is a stochastic estimate whose expectation approximates the gradient of the performance measure with respect to its argument $\theta$ and $\alpha$ is the step size. Some methods also learn approximation to value-functions and are called \emph{actor-critic methods}, where "actor" is a reference to the learned policy, and "critic" refers to the learned value function.\\
\newline
In policy gradient methods, the policy $\pi(a|s,\theta)$ has to be differentiable with respect to its parameters $\forall s \in \Sspace, a \in \Aspace, \theta \in \mathbb{R}^d$.

\subsection{Policy Gradient Theorem}
With continuous policy parametrization the action probabilities change smoothly as a function of the learned parameter, whereas in $\epsilon$-greedy selction the action probabilities may change dramatically for an arbitrary small change in the estimated action values, if that change results in a different action having the maximal value.
\todo{parlare (prima) di algoritmi con value function}
Stronger convergence guarantees are available for policy-gradient methods than for action-value methods. In particular, the continuity of the policy dependence on the parameters enables policy-gradient methods to approximate gradient ascent.\\
\newline
The problem is that the performance depends on both the action selections and the distribution of states in which those selctions are made, and both of these are affected by the policy parameter. The effect of the policy on the state distribution is a function of the environment, typically unknown. Fortunately, there is an excellent theoretical answer to this challenge in the form of the \emph{policy gradient theorem}, which provides an analytic expression for $\nabla_{\theta}J(\theta)$ that does not involve the derivative of the state distribution.\\
\newline
The theorem is from [\citet{Sutton1999PolicyGM}] and we enunciate it in the continuous case:
\begin{align}
\nabla_{\theta}J(\theta) = \int_s \delta^{\theta}(s) \int_a \nabla_{\theta} \pi(a|s) Q^{\theta}(s,a) \de a \de s. \label{eq:pgt}
\end{align}
We can estimate the gradient from samples collected following the policy $\pi_{\theta}$ and using the estimated value in the formula~\eqref{eq:grad}.

\subsection{Algorithms}
In this section we present some important algorithms and we show their property that are interesting to compare these existing algorithms with our work.
\subsubsection{REINFORCE}
According to [\citet{Williams1992SimpleSG}] it is possible to estimate the gradient from a single trajectory: we replace in~\eqref{eq:pgt} the integral over $a$ with an expectation under $\pi$, and then sampling the expectation:
\begin{align}
	\nabla_{\theta}J(\theta) &= \int_s \delta^{\theta}(s) \int_a \pi(a|s) \frac{\nabla_{\theta} \pi(a|s)}{\pi(a|s)} Q^{\theta}(s,a) \de a \de s \nonumber\\
	&= \EV_{s_t \sim \delta^{\pi}, a_t \sim \pi} \Big[ \frac{\nabla_{\theta} \pi(a_t|s_t)}{\pi(a_t|s_t)} Q^{\theta}(s_t,a_t) \Big] \nonumber\\
	&= \EV_{s_t \sim \delta^{\pi}, a_t \sim \pi} \Big[ \nabla_{\theta} \log\pi(a_t|s_t) r_t(s_t, a_t) \Big] \label{eq:rein}
\end{align}
where~\eqref{eq:rein} is obtained by considering $\nabla_{\theta} \log\pi(a_t|s_t) = \frac{\nabla_{\theta} \pi(a_t|s_t)}{\pi(a_t|s_t)}$ and $r_t(s_t, a_t) = \sum_{j = t}^T \gamma^{j-t} R(s_j, a_j)$.\\
\newline
From~\eqref{eq:pgt} we observe that if $b(s)$ does not depend on $a$ we can write:
$$ \nabla_{\theta}J(\theta) = \int_s \delta^{\theta}(s) \int_a \nabla_{\theta} \pi(a|s) \Big( Q^{\theta}(s,a) -b(s) \Big) \de a \de s. $$
Indeed: $\int_a b(s) \nabla_{\theta} \pi(a|s) \de a = b(s) \nabla_{\theta} \int_a \pi(a|s) \de a = b(s) \nabla_{\theta} 1 = 0.$
$b(s)$ is called baseline and allows to reduce the variance of the estimate, that considerably slow the convergence of the algorithm.
\todo{spiegare come vengono considerate somme/medie degli step della traiettoria}
A common baseline used is:
$$ b_k^h = \frac{\Big\langle\Big( \sum_{k=0}^{K}\nabla_{\theta} \log\pi^{\theta}(a_k|s_k)\Big)^2r_k \Big\rangle}{\Big\langle\Big( \sum_{k=0}^{K}\nabla_{\theta} \log\pi^{\theta}(a_k|s_k)\Big)^2\Big \rangle}
$$
and $Q^{\theta}(s,a)$ is replaced with:
$$ g_h = \Big\langle \sum_{l=0}^{H} \Big( \sum_{k=0}^{l} \nabla_{\theta} \log \pi^{\theta}(a_k|s_k)\Big)(r_l - b_l^h)\Big\rangle.
$$
This is one of the most common policy gradient algorithms and we use it in our work to set a comparison with a state of the art algorithm in policy search.

\subsubsection{PGPE}
\todo{spiegare (anche prima) likelihood gradient}
\acf{PGPE}[\citet{inproceedings}] estimates a likelihood gradient by sampling directly in parameter space, which reduces the variance of estimates compared to REINFORCE. This is due to the fact that once parameters are sampled, we can generate an entire action-state history (trajectory) while in REINFORCE we draw the action from the policy at each time step.\\
\newline
Indeed, we replace the stochastic policy with a probability distribution over the parameter themselves:
$$ \pi^{\theta}(a|s) = \int_{\Theta} p(\theta) \delta_{\mu^{\theta}(s)}\de \theta,
$$
where $\delta_{\mu^{\theta}(s)}$ is the Dirac delta function of a deterministic policy $\mu^{\theta}$ depending on parameter $\theta$ weighted for a distribution $p(\theta)$.


\subsubsection{DPG}
\acf{DPG} [\citet{article}] is a class of algorithms that learn a parametric deterministic policy. This is similar to what we want to achieve but the problem is that during the learning phase \ac{DPG} collect samples from a stochastic behavioural policy and then unexpected actions can be performed.\\
\newline
Policy gradient algorithms are perhaps the most popular class of continuous action reinforcement learning algorithms, starting from the results obtained by the \emph{policy gradient theorem}, we consider a deterministic policy $\mu_{\theta}: S \mapsto A$ with parameter vector $\theta \in \mathbb{R}^n$ and performance objective $J(\mu_{\theta}) = \mathbb{E}_{s \sim \delta^{\mu}}[r(s, \mu_{\theta}(s))]$. 
By applying the chain rule we see that the policy improvement may be decomposed:
$$
\nabla_{\theta} Q^{\mu}(s, \mu_{\theta}(s)) = \nabla_{\theta}\mu_{\theta}(s)\nabla_a Q^{\mu}(s, a)|_{a=\mu_{\theta}(s)}
$$
Suppose that $\nabla_{\theta}\mu_{\theta}(s)$ and $\nabla_aQ^{\mu}(s,a)$ exist, then:
$$
\nabla_{\theta}J(\mu_{\theta}) = \mathbb{E}_{s \sim \delta^{\mu}} [\nabla_{\theta} \mu_{\theta} (s) \nabla_a Q^{\mu}(s,a)|_{a = \mu_{\theta} (s)}]
$$
We use the deterministic policy gradient theorem to derive an off-policy actor-critic algorithm. It learns a deterministic target policy $\mu_{\theta}(s)$ from trajectories generated by an arbitrary stochastic behaviour policy $\pi(a|s)$. A critic estimates the action-value function $Q^w(s,a) \approx Q^{\mu}(s,a)$. 

\section{Safe Reinforcement Learning}

\section{State discretization}