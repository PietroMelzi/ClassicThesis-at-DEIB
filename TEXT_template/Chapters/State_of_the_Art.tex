\chapter{State of the Art} \label{chap:state}

\section{Policy Gradient}
The most important class of algorithms in \ac{PS} is the one in which the algorithms learn the parameterized policy $\pi_{\theta}, \theta \in \mathbb{R}^d$ based on the gradient of some scalar performance measure $J(\theta)$ with respect to the policy parameter $\theta$. These methods seek to maximize performance, so their updates approximate gradient ascent in J:
\begin{align}
\theta \leftarrow \theta + \alpha \widehat{\nabla}J(\theta), \label{eq:grad}
\end{align}
where $\widehat{\nabla}J(\theta) \in \mathbb{R}^d$ is a stochastic estimate whose expectation approximates the gradient of the performance measure with respect to its argument $\theta$ and $\alpha$ is the step size, a scalar that controls the size of each update and can change through time. In policy gradient methods, the policy $\pi(a|s,\theta)$ has to be differentiable with respect to its parameters $\theta \in \mathbb{R}^d \quad \forall s \in \Sspace, \forall a \in \Aspace$.\\
\newline
With appropriate policy parametrization the action probabilities change smoothly as a function of the learned parameter, whereas for greedy policies the action probabilities may change enormously for a small change in the estimated action values. Stronger convergence guarantees are available for policy-gradient methods and approximate gradient ascent can be performed [\ref{eq:grad}]. According to [\cite{Peters2008ReinforcementLO}], if the gradient estimate is unbiased and learning rates fulfill:
\begin{align}\sum_{t=1}^{\infty}\alpha_t = \infty \quad \sum_{t=1}^{\infty}\alpha_t^2 < \infty\end{align} \todo{sightly different from the citation}
the learning process is guaranteed to converge to at least a local optimum. Policy-gradient methods can also incorporate domain knowledge in the policy definition and can be made safe by design.\\
\newline
Some methods also learn approximations to value-functions and are called \emph{actor-critic methods}, where "actor" is a reference to the learned policy, and "critic" refers to the learned value function. In these methods, the function approximation for value function introduces bias but reduces variance and accelerates learning.

\subsection{Policy Gradient Theorem} \label{subsec:pgt}
It may seem challenging to change the policy parameter in a way that ensures improvement. The problem is that the performance $J(\theta)$, \ie the measure to maximize, depends on both the action selections and the distribution of states in which those selctions are made, and both of these are affected by the policy parameter $\theta$. We have to estimate the performance gradient with respect to $\theta$ but the gradient is affected by the unknown effect of policy changes. Fortunately, there is a theoretical answer to this challenge in the form of the \emph{policy gradient theorem}, which provides an analytic expression for $\nabla_{\theta}J(\theta)$ that does not involve the derivative of the state distribution $\delta^{\theta}(s)$.\\
\newline
The theorem is from [\citet{Sutton1999PolicyGM}] and we enunciate it in the case of continuous state space $\Sspace$ and action space $\Aspace$:\todo{scrivere come teorema}
\begin{align} 
\nabla_{\theta}J(\theta) = \int_{\Sspace} \delta^{\theta}(s) \int_{\Aspace} \nabla_{\theta} \pi(a|s) Q^{\theta}(s,a) \de a \de s. \label{eq:pgt}
\end{align}
Starting from~\eqref{eq:pgt}, the performance gradient $\nabla_{\theta}J(\theta)$ can be estimated from samples in different ways. In \mySubsec{subsec:alg} we present some policy gradient algorithms. Generally, a batch of $N$ trajectories is sampled and $N$ single estimates of the gradient are averaged to obtain the final estimate. This estimate is the $\widehat{\nabla }J(\theta)$ used in~\eqref{eq:grad} to perform a gradient ascent iteration that updates the policy parameter $\theta$.

\subsection{Policy Gradient Algorithms} \label{subsec:alg}
In this section we present some policy gradient algorithms that have some interesting properties for our work. We compare these existing algorithms and their properties with the algorithm proposed in our work. The first algorithm we present is REINFORCE [\ref{subsec:rein}], a method directly derived from the Policy Gradient Theorem that shows how the gradient of performance can be easily estimated from samples. Then, we present two methods that learn deterministic policies. The difference between these two methods is the way in which exploration is performed: in \acf{PGPE} [\ref{subsec:pgpe}] exploration is performed in the policy space while in \acf{DPG} [\ref{subsec:dpg}] exploration is performed in action space by a behavioural policy (\ie a policy that collects samples, different from the policy that will be learnt).

\subsubsection{REINFORCE} \label{subsec:rein}
In REINFORCE the gradient of performance $\nabla_{\theta}J(\theta)$ can be estimated from a single trajectory $\langle s_0, a_0, r_1, s_1, a_1, ..., s_{T-1}, a_{T-1}, r_{T} \rangle$ without performing any sort of perturbation on parameters $\theta$, according to [\citet{Williams1992SimpleSG}]. Starting from~\eqref{eq:pgt}, we can write the gradient as an expected value depending on the state distribution $\delta^{\theta}$ and the parameterized policy $\pi_{\theta}$. Then, the samples collected following $\pi_{\theta}$ can be used for the estimation of $\nabla_{\theta}J(\theta)$: 
\begin{align}
\nabla_{\theta}J(\theta) &= \int_s \delta^{\theta}(s) \int_a \pi(a|s) \frac{\nabla_{\theta} \pi(a|s)}{\pi(a|s)} Q^{\theta}(s,a) \de a \de s \nonumber\\
&= \EV_{s_t \sim \delta^{\pi}, a_t \sim \pi} \Big[ \frac{\nabla_{\theta} \pi(a_t|s_t)}{\pi(a_t|s_t)} Q^{\theta}(s_t,a_t) \Big] \nonumber\\
&= \EV_{s_t \sim \delta^{\pi}, a_t \sim \pi} \Big[ \nabla_{\theta} \log\pi(a_t|s_t) Q^{\theta}(s_t,a_t) \Big] \label{eq:rein}
\end{align}
where~\eqref{eq:rein} is obtained by considering $\nabla_{\theta} \log\pi(a_t|s_t) = \frac{\nabla_{\theta} \pi(a_t|s_t)}{\pi(a_t|s_t)}$ and $Q^{\theta}(s_t, a_t)$ can be approximated by a function.\\
\newline
REINFORCE has good theoretical convergence properties: by construction, the expected update computed at each iteration increases the performance. However, it presents ah high variance that slows the convergence process. In order to mitigate this problem, we consider \emph{baseline functions} $b: \Sspace \to \mathbb{R}$, \ie functions that don't depend on the action and can reduce variance in the estimate without introducing bias.
Indeed, we observe that if $b(s)$ does not depend on $a$ we can modify~\eqref{eq:pgt} in the following way:
\begin{align} \nabla_{\theta}J(\theta) = \int_s \delta^{\theta}(s) \int_a \nabla_{\theta} \pi(a|s) \Big( Q^{\theta}(s,a) -b(s) \Big) \de a \de s. \label{eq:pgtbase}\end{align}
The new term appearing in~\eqref{eq:pgtbase} doesn't affect the value of gradient:
\begin{align} \int_a b(s) \nabla_{\theta} \pi(a|s) \de a = b(s) \nabla_{\theta} \int_a \pi(a|s) \de a = b(s) \nabla_{\theta} 1 = 0. \end{align}
An intuitive baseline $b(s)$ that can be used is the value function $V^{\theta}(s)$. In some tasks, $Q^{\theta}(s,a)$ can assume high values and the use of $V^{\theta}(s)$ as baseline provides an effect of normalization. The difference in expression~\eqref{eq:pgtbase} becomes the advantage function $A^{\theta}(s,a)$ when $V$ is used as a baseline:
\begin{align} \nabla_{\theta}J(\theta) = \int_s \delta^{\theta}(s) \int_a \nabla_{\theta} \pi(a|s) \Big( Q^{\theta}(s,a) - V^{\theta}(s) \Big) \de a \de s.\end{align}
In practice, the value functions $Q^{\theta}$ and $V^{\theta}$ are not available and have to be estimated. In episodic tasks, the return $G_t$ can be computed from every time step $t$, then a value $Q(s_t, a_t)$ can be assigned to every state-action pair $(s_t, a_t)$ sampled in the trajectory. As a result, starting from a trajectory $\langle s_0, a_0, r_1, s_1, a_1, ..., s_{T-1}, a_{T-1}, r_{T} \rangle$ terminating in time step $T$, the gradient of performance is estimated as:
\begin{align}
\widehat{\nabla_{\theta}}J(\theta) = \sum_{k=0}^{T-1} \nabla_{\theta}\log\pi_{\theta}(a_k|s_k)\Big(\sum_{l=k+1}^{T}\gamma^{l-1}r_{l} - b_{l} \Big),
\end{align}
where the variance-minimizing baseline [\cite{Peters2008ReinforcementLO}] is: \todo{controllare indici delle formule}
\begin{align} b_l = \frac{\Big( \sum_{k=0}^{l}\nabla_{\theta} \log\pi^{\theta}(a_k|s_k)\Big)^2 \gamma^{l-1}r_l} {\Big( \sum_{k=0}^{l}\nabla_{\theta} \log\pi^{\theta}(a_k|s_k)\Big)^2}
\end{align} 
This is one of the most common policy gradient algorithms and we use it in our work to compare our algorithm with a standard algorithm in policy search.

\subsubsection{\ac{PGPE}} \label{subsec:pgpe}
\acf{PGPE}[\citet{sehnke2008PolicyGradient}] is a method that estimates a gradient by directly sampling in parameter space. In \ac{PGPE} the policy is defined by a distribution over the parameters of deterministic controllers that we indicate with the function $\mu_{\theta}: \Sspace \to \Aspace$. At the beginning of each step, the parameter $\theta$ of the controller is sampled and then, the deterministic policy $\mu_{\theta}$ generated from $\theta$ is followed for all the episode length. Even if samples are collected with a detrministic policy, the choice of the policy is stochastic and then unexpected behaviour could arise in the task.\\
\newline
The variance of the estimates obtained with \ac{PGPE} is lower than the variance of the estimates obtained with REINFORCE. This is due to the fact that, in REINFORCE, a repetitive sampling from a stochastic policy injects noise in the gradient estimate at every time-step. Furthemore, the variance increases linearly with the length of the history since each state depends on the entire sequence of previous samples.\\
\newline
As we said, in \ac{PGPE} the stochasticity that in REINFORCE was given by a stochastic policy $\pi_{\theta}$ is replaced by a probability distribution over the parameters $\theta$ themselves. In turn, this probability distribution is parameterized with parameter $\rho$, independent from $\theta$. Given a parameter space $\Theta$ for (deterministic) controllers $\mu_{\theta}, \theta \in \Theta$, the policy considered to perform exploration in the task is:
\begin{align}
\pi_{\rho}(a|s) = \int_{\Theta} p_{\rho}(\theta) \delta_{\mu_{\theta}(s)}\de \theta,
\end{align}
where $\delta_{\mu_{\theta}(s)}$ is the Dirac delta function corresponding to the deterministic controller $\mu_{\theta}$ depending on parameter $\theta$ sampled from a distribution with probability $p_{\rho}(\theta)$.\\
\newline
Given a trajectory $h \in \mathbb{H}$, where $\mathbb{H}$ is the set of possible trajectories, a probability $p_{\rho}(h, \theta)$ of sampling parameter $\theta$ and trajectory $h$ and defined $G(h)$ the return of trajectory $h$, we can define a suitable performance measure $J(\rho)$ as:
\begin{align}
J(\rho) = \int_{\Theta} \int_{\mathbb{H}} p_{\rho}(h, \theta) G(h) \de h \de \theta. \label{eq:pgpej}
\end{align}
From \ref{eq:pgpej}, we can write the expected value of the gradient of $J$ \wrt the distribution parameter $\rho$ as:
\begin{align}
\widehat{\nabla_{\rho}}J(\rho) \approx \nabla_{\rho} \log p_{\rho} (\theta) G(h),
\end{align}
where $\theta$ is sampled at the beginning of the episode and $h$ is resulting from the deterministic controller $\mu_{\theta}$. Usually, we can consider the parameter $\rho$ learnt by the algorithm as $\rho = \Big( \{\mu_i\}, \{\sigma_i\} \Big)$, where $\mu_i$ and $\sigma_i$ are the parameters that determine an indipendent normal distribution $p_{\rho_i}(\theta)$ for each parameter $\theta_i \in \Theta$.  

\subsubsection{\ac{DPG}} \label{subsec:dpg}
\acf{DPG} [\citet{article}] is a class of algorithms that learn a parametric deterministic policy $\mu_{\theta}$ defined as $\mu_{\theta}: S \mapsto A$. The approach in \ac{DPG} is similar to the one used in policy gradient methods that learn stochastic policies $\pi_{\theta}$, where the gradient of performance is estimated according to the Policy Gradient Theorem (\ref{subsec:pgt}). However, for deterministic policies the gradient of performance is slightly different than the one computed by the theorem (\ref{eq:pgt}).\\
\newline
In order to ensure exploration while learning a deterministic policy $\mu_{\theta}$, an \emph{off-policy} algorithm is introduced (\ie an algorithm in which the policy used for collecting samples, the \emph{behavioural policy} $\beta(a|s)$, is different from the learnt policy). Usually, in off-policy algorithms, every expected value contains the \emph{importance sampling ratio} $\frac{\pi(a|s)}{\beta(a|s)}$ to adjust the fact that actions were selected with policy $\beta$ rather than policy $\pi$.\\
\newline
Both in our work and in \ac{DPG} we want to learn a deteministic policy, however during the learning phase \ac{DPG} collects samples from a stochastic behavioural policy, differently from what we are looking for.\\
\newline
Similarly to (\mySubsec{subsec:pgt}) but considering a deterministic policy $\mu_{\theta}: S \mapsto A$ with parameter vector $\theta \in \mathbb{R}^n$ and performance objective $J(\mu_{\theta}) = \mathbb{E}_{s \sim \delta^{\mu}}[R(s, \mu_{\theta}(s))]$, the expression for \emph{Deterministic Policy Gradient} is derived:
\begin{theorem}[Deterministic Policy Gradient Theorem] \label{th:dpgt}
	Suppose that $P(s'|s,a), \nabla_aP(s'|s,a), \mu_{\theta}(s), \nabla_{\theta} \mu_{\theta}(s), R(s,a), \nabla_a R(s,a), \mu(s)$ \todo{mu usato per distribuzione iniziale degli stati e per policy deterministiche: correggere!!!!} are continuous in all parameters and variables $s, a, s'$.\todo{x variable in Sutton appendix?} Then, 
	\begin{align}
	\nabla_{\theta}J(\mu_{\theta}) = \mathbb{E}_{s \sim \delta^{\mu}} \Big[\nabla_{\theta} \mu_{\theta} (s) \nabla_a Q^{\mu}(s,a)|_{a = \mu_{\theta} (s)}\Big].
	\end{align}
\end{theorem}
\noindent We use the Theorem \ref{th:dpgt} to derive an off-policy actor-critic algorithm that learns a deterministic target policy $\mu_{\theta}(s)$ from trajectories generated by an arbitrary stochastic behaviour policy $\pi(a|s)$. A critic estimates the action-value function $Q^w(s,a) \approx Q_{\mu}(s,a)$, where $w$ is the parameter of the function approximator $Q^w(s,a)$. In [\citet{article}] the conditions required for a function approximator to be compatible (\ie avoid to introduce any bias in gradient update) are specified.

%\section{Safe \NoCaseChange{\acf{RL}}}\label{sec:saferl}
\section{Safe Reinforcement Learning}\label{sec:saferl}
In \ac{RL} several definitions of safety have been proposed, and also methods that ensure a certain degree of safety according to some measure of risk. In this section, first we provide an overview of the different methods that face the problem of safe learning, using the same classification proposed by [\citeauthor{JMLR:v16:garcia15a}], then we focus on the more specific issue related to \emph{safe exploration}.\\
\newline
According to [\cite{JMLR:v16:garcia15a}], there exist two main trends for Safe \ac{RL}. The first one is based on the modification of the optimality criterion to introduce the concept of risk. The second is based on the modification of the exploration process, so as to avoid the exploration of actions that may lead the system to undesirable or catastrophic situations. Regarding the first trend, there are several alternatives to quantify risk:
\begin{itemize}
	\item \emph{Worst Case Criterion}: a policy is considered to be optimal if it has the maximum worst-case return:
	\begin{align}
	\max_{\pi \in \Pi}\min_{w \in \Omega^{\pi}} \EV_{\pi, w}[G_{0}],
	\end{align}
	where $\Omega^{\pi}$ is a set of trajectories that occurs under the policy $\pi$ and the quantity to maximize-minimize is an expected value \wrt the policy $\pi$ and the trajectory $w$.	This criterion is used to mitigate the effects of variability induced by a given policy. It is also possible to use this kind of criterion when the transition function $P$ is uncertain:
	\begin{align}
	\max_{\pi \in \Pi}\min_{P} \EV_{\pi, P}[G_{0}],
	\end{align}
	\item \emph{Risk-Sensitive Criterion}: the optimization criterion balances return and risk by means of a scalar parameter that is included in the objective function and allows the sensitivity to the risk to be controlled. Risk can be defined, for instance, as the variance of return or as the probability of entering into an error state.
	\item \emph{Constrained Criterion}: the method maximizes the return subject to one or more constraints.
\end{itemize}
Regarding the modification of the exploration process, it can be modified through the incorporation of external knowledge in three different ways:
\begin{itemize}
	\item \emph{Providing Initial Knowledge}: examples gathered from a teacher or previous information on the task can be used to provide an initial knowledge for the learning algorithm. From the ealier steps of the algorithm, the most relevant regions of the state and action spaces are visited. It considerably reduces random exploration.
	\item \emph{Deriving a policy from a finite set of demonstrations}: a set of examples provided by a teacher, that replace examples provided by exploration, can be used to learn a model from which to derive a policy in an off-line and, hence, safe manner.
	\item \emph{Providing Teach Advice}: a human or a simple controller assists the exploration during the learning process and provides advice. In some approaches, the teacher can provide advice only when the agent explicitly asks for, in others whenever it feels it necessary. 
\end{itemize}

\subsection{Safe Exploration}
In our work, it is fundamental to ensure that the agent learns in complete safety (\ie the agent doesn't perform undesired actions throughout the learning phase). The second class of safe methods is suitable for this. However, it requires external knowledge, in the form of experience samples or teacher advice, that is not always available. Instead, the methods of the first class can learn an optimal policy (according to a definition of return that includes risk) but they trade-off short term loss in performance for a long term gain. This is not accettable in the case of safety-critical applications because the risk is encoded in the reward function. This problem is known as policy oscillation, it can cause failure or harm the environment. Here we describe two state of the art approaches to safe exploration, the first one for finite \ac{MDPs} (\mySubsec{subsec:fmdp}) and the second one for policy gradient methods (\mySubsec{subsec:safepg}). Both of the methods rely on a statistical confidence $\epsilon$ in order to define safety constraints, then the complete avoidance of unwanted actions is only ensured with high probability. 

\subsubsection{Safe Exploration in Finite \ac{MDPs}} \label{subsec:fmdp}
[\cite{turchetta2016}] address the problem of safely exploring finite \ac{MDPs}, where safety is defined in terms of a constraint that satisfies regularity conditions. The algorithm, called SafeMDP, cautiously explores safe states and actions, obtains noisy observations and gains knowledge on the safety of unvisited state-action pairs.
Starting from a set of states and actions that are known to be safe, the regularity assumptions are exploited in order to evaluate only state-action pairs known to fulfill the safety constraint.
The reward (encoding a measure of safety) is unknown and drawn from a gaussian distribution. At each iteration of the algorithm a posterior distribution is computed from the sampled rewards, affected by an additive noise drawn from a zero-mean gaussian distribution. The reward function $R(s)$ is Lipschitz continuous, with Lipschitz constant $L_{R}$. Since only noisy measurements are observed, $R(s)$ is known up to some statistical confidence $\epsilon$. By considering the Lipschitz continuity of $R(s)$ and the confidence $\epsilon$ and starting from some safe set $\Sspace_s$, the resulting set of safe states is:
\begin{align}
R_{\epsilon}^{safe}(\Sspace_s) = \Sspace_s \cup \{s \in \Sspace | \exists s' \in \Sspace_s: R(s') - \epsilon - L_{R} d(s, s') \geq h\}, \label{eq:safe}
\end{align}
where $h$ represents a safety threshold. The obtained set is then restricted by considering only the safe states that are reachable and from which we can move to other safe states.
By iteratively applying the operator $R_{\epsilon}^{safe}$ in (\ref{eq:safe}) to the restricted set of safe states $\Sspace_s$ and updating the gaussian distribution of $R(s)$ from samples, we obtain the largest set of states that can be safely reached by the exploring agent.

\subsubsection{Safe Policy Gradients} \label{subsec:safepg}
\acf{SPG} is an algorithm from [\cite{papini2019}] in which the learning agent is constrained to never worsen its performance during learning. According to the classification of safe \ac{RL} methods, the algorithm follows the \emph{constrained criteria}, as oscillating performances may violate the constraint, called \emph{Monotonic Improvement}. Actor-only policy gradient from a stochastic optimization perspective algorithms and \emph{smoothing policies}, \ie policies for which the Lipschitz assumptions hold, are considered. From upper bounds on the variance of policy gradient estimators, a lower bound on the performance improvement (with a certain confidence $\epsilon$) provided by gradient-based updates is calculated and expressed as a function of some meta-parameters. These meta-parameters are the step size $\alpha$ of the parameter updates and the batch size $N$ of the gradient estimators. Then, the adaptive meta-parameters that guarantee monotonic improvement with high probability are identified.\\
\newline 
Policy gradient methods may suffer from the explosion of gradients when the current policy is close to deterministic, leading to unstable training process. \acf{TDL} [\cite{DBLP:journals/corr/abs-1905-11041}] addresses this problem, alternating between proposing a target distribution and training the policy to approach the target distribution. The target policy is the solution of a constrained optimization problem where the constraints involve the difference between updated policies, so that the algorithm leads to more stable improvements.