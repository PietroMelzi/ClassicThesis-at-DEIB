\chapter{State of the Art} \label{chap:state}
\hl{In this chapter we present two topics from} \ac{RL} \hl{that are crucial in our work, namely} \acf{PG} (\mySec{sec:pg}) and Safe Reinforcement Learning (\mySec{sec:saferl}). \hl{We also provide the description of some existing algorithm or implementation strategies, some of them representing the state of the art, that we consider in this work in order to make a comparison with our algorithm.}

\section{Policy Gradient Methods} \label{sec:pg}
\hl{Most of the algorithms in} \ac{PS} \hl{learn the parameterized policy $\pi_{\theta}$, with $\theta \in \Theta \subseteq \mathbb{R}^d$, according to the gradient of some scalar performance measure $J(\theta)$ with respect to the policy parameter $\theta$. These methods seek to maximize performance, so they perform gradient ascent updating the parameter $\theta$ in the direction of $\widehat{\nabla}J(\theta)$:}
\begin{align}
\theta \leftarrow \theta + \alpha \widehat{\nabla}J(\theta), \label{eq:grad}
\end{align}
where $\widehat{\nabla}J(\theta) \in \mathbb{R}^d$ is a stochastic estimate whose expectation approximates the gradient of the performance measure with respect to its argument $\theta$ and $\alpha>0$ is the step size, a scalar that controls the size of each update and can change through time. In \ac{PG} methods, the policy $\pi(a|s,\theta)$ has to be differentiable with respect to its parameters $\theta \in \mathbb{R}^d \quad \forall s \in \Sspace, \forall a \in \Aspace$.\\
\newline
With appropriate policy parametrization the action probabilities change smoothly as a function of the learned parameter, whereas for greedy policies the action probabilities may change enormously for a small change in the estimated action values. Stronger convergence guarantees are available for policy-gradient methods and approximate gradient ascent can be performed as in \myEq{eq:grad}. According to [\cite{peters2010pg}], if the gradient estimate is unbiased\footnote{In [\cite{williams1992simple}] it is proved that an unbiased estimate of the gradient can be obtained from samples without the assistance of approximate functions.} and learning rates fulfill the following conditions:
\begin{align}\sum_{t=1}^{\infty}\alpha_t = \infty \quad \sum_{t=1}^{\infty}\alpha_t^2 < \infty\end{align} \todo{ho cambiato paper, qui le condizioni coincidono}
the learning process is guaranteed to converge to at least a local optimum.\\
\newline
Some methods also learn approximations to value-functions and are called \emph{actor-critic methods}, where "actor" is a reference to the learned policy, and "critic" refers to the learned value function. In these methods, the function approximation for value function introduces bias but reduces variance and accelerates learning.

\subsection{Policy Gradient Theorem} \label{subsec:pgt}
It may seem challenging to change the policy parameter in a way that ensures improvement. The problem is that the performance $J(\theta)$, \ie the measure to maximize, depends on both the action selections and the distribution of states in which those selctions are made, and both of these are affected by the policy parameter $\theta$. We have to estimate the performance gradient with respect to $\theta$ but the gradient is affected by the unknown effect of policy changes. Fortunately, there is a theoretical answer to this challenge in the form of the \emph{policy gradient theorem}, which provides an analytic expression for $\nabla_{\theta}J(\theta)$ that does not involve the derivative of the state distribution $\delta^{\theta}(s)$.\\
\newline
The theorem is from [\cite{Sutton1999PolicyGM}] and we enunciate it in the case of continuous state space $\Sspace$ and action space $\Aspace$:
\begin{theorem}[Policy Gradient Theorem]
	\hl{Given a stochastic policy $\pi_{\theta}$ differentiable} \wrt \hl{its parameter, the gradient of performance measure $J(\theta)$ with respect to the policy parameter $\theta$ can be written as:}
	\begin{align} 
	\nabla_{\theta}J(\theta) = \int_{\Sspace} \delta^{\theta}(s) \int_{\Aspace} \nabla_{\theta} \pi(a|s) Q^{\theta}(s,a) \de a \de s. \label{eq:pgt}
	\end{align}
\end{theorem}
\noindent Starting from~\eqref{eq:pgt}, the performance gradient $\nabla_{\theta}J(\theta)$ can be estimated from samples in different ways. In \mySubsec{subsec:alg} we present some \ac{PG} algorithms. Generally, a batch of $N$ trajectories is sampled and $N$ single estimates of the gradient are averaged to obtain the final estimate. This estimate is the $\widehat{\nabla }J(\theta)$ used in~\eqref{eq:grad} to perform a gradient ascent iteration that updates the policy parameter $\theta$.

\subsection{Policy Gradient Algorithms} \label{subsec:alg}
In this section we present some \ac{PG} algorithms that have some interesting properties for our work. We compare these existing algorithms and their properties with the algorithm proposed in our work. The first algorithm we present is REINFORCE, a method directly derived from the Policy Gradient Theorem that shows how the gradient of performance can be easily estimated from samples. Then, we present two methods that learn deterministic policies. The difference between these two methods is the way in which exploration is performed: in \acf{PGPE} exploration is performed in the policy space while in \acf{DPG} exploration is performed in action space by means of a behavioural policy (\ie a policy that collects samples, different from the policy that is being learnt).

\paragraph{REINFORCE} \label{subsec:rein}
In REINFORCE the gradient of performance $\nabla_{\theta}J(\theta)$ can be estimated from a single trajectory $\langle s_0, a_0, r_1, s_1, a_1, ..., s_{T-1}, a_{T-1}, r_{T} \rangle$ without performing any sort of perturbation on parameters $\theta$, according to [\cite{Williams1992SimpleSG}]. Starting from~\eqref{eq:pgt}, we can write the gradient as an expected value depending on the state distribution $\delta^{\theta}$ and the parameterized policy $\pi_{\theta}$. Then, the samples collected following $\pi_{\theta}$ can be used for the estimation of $\nabla_{\theta}J(\theta)$: 
\begin{align}
\nabla_{\theta}J(\theta) &= \int_s \delta^{\theta}(s) \int_a \pi_{\theta}(a|s) \frac{\nabla_{\theta} \pi_{\theta}(a|s)}{\pi_{\theta}(a|s)} Q^{\theta}(s,a) \de a \de s \nonumber\\
&= \EV_{s \sim \delta^{\theta}, a \sim \pi_{\theta}} \Big[ \frac{\nabla_{\theta} \pi_{\theta}(a|s)}{\pi_{\theta}(a|s)} Q^{\theta}(s,a) \Big] \nonumber\\
&= \EV_{s \sim \delta^{\theta}, a \sim \pi_{\theta}} \Big[ \nabla_{\theta} \log\pi_{\theta}(a|s) Q^{\theta}(s,a) \Big] \label{eq:rein}
\end{align}
where~\eqref{eq:rein} is obtained by considering $\nabla_{\theta} \log\pi_{\theta}(a|s) = \frac{\nabla_{\theta} \pi_{\theta}(a|s)}{\pi_{\theta}(a|s)}$ and $Q^{\theta}(s, a)$\hl{, usually unknown, can be replaced by an action-value function approximation.}\\
\newline
\hl{In episodic tasks, the return $G_t$ can be computed from every time step $t$ once the episode is terminated. Then, $Q^{\theta}(s, a)$ can be approximated with $G_{t}$ for every state-action pair $(s_t, a_t)$ sampled in the trajectory. This approximation is unbiased, because $\EV_{\pi_{\theta}}[G_t | s_t, a_t] = Q^{\theta}(s_t, a_t)$.
Given a batch of $N$ trajectories $\langle s_0, a_0, r_1, s_1, a_1, ..., s_{T-1}, a_{T-1}, r_{T} \rangle$ from an episodic task, the gradient of performance can be estimated as:}
\begin{align}
	\widehat{\nabla}_{\theta}J(\theta) = \Big\langle \sum_{t=0}^{T-1} \nabla_{\theta} \log\pi_{\theta}(a_t|s_t) G_{t} \Big\rangle_{N}, \label{eq:estrein}
\end{align}
\hl{where $\langle \cdot \rangle_{N}$ denotes the average of the inner expression over the $N$ trajectories.}\\
\newline
REINFORCE has good theoretical convergence properties: by construction, \hl{an improvement in expected performance and convergence to a local optimum under standard stochastic approximation conditions are ensured} [\cite{sutton2018reinforcement}]. However, it presents an high variance that slows the convergence process. In order to mitigate this problem, we consider \emph{baseline functions} $b: \Sspace \to \mathbb{R}$, \ie functions that don't depend on the action and can reduce variance in the estimate without introducing bias.
Indeed, we observe that if $b(s)$ does not depend on $a$ we can modify~\eqref{eq:pgt} in the following way:
\begin{align} \nabla_{\theta}J(\theta) = \int_s \delta^{\theta}(s) \int_a \nabla_{\theta} \pi(a|s) \Big( Q^{\theta}(s,a) -b(s) \Big) \de a \de s. \label{eq:pgtbase}\end{align}
The new term appearing in~\eqref{eq:pgtbase} doesn't affect the value of gradient:
\begin{align} \int_a b(s) \nabla_{\theta} \pi(a|s) \de a = b(s) \nabla_{\theta} \int_a \pi(a|s) \de a = b(s) \nabla_{\theta} 1 = 0. \end{align}
An intuitive baseline $b(s)$ that can be used is the value function $V^{\theta}(s)$. In some tasks, $Q^{\theta}(s,a)$ can assume high values and the use of $V^{\theta}(s)$ as baseline provides an effect of normalization. The difference in expression~\eqref{eq:pgtbase} becomes the advantage function $A^{\theta}(s,a)$ when $V$ is used as a baseline:
\begin{align} \nabla_{\theta}J(\theta) = \int_s \delta^{\theta}(s) \int_a \nabla_{\theta} \pi(a|s) \Big( Q^{\theta}(s,a) - V^{\theta}(s) \Big) \de a \de s.\end{align}
In [\cite{Peters2008ReinforcementLO}] \hl{a different formulation for the estimation of gradient is provided. It is used in the \emph{GPOMDP} algorithm and reduces variance in the estimation of gradient because it prevents to sum, for each time-step, the rewards obtained from the current time-step until the end of the episode:}
\begin{align}
\widehat{\nabla}_{\theta}^{GPOMDP}J(\theta) &= \Big\langle \sum_{l=0}^{T-1} \Big( \sum_{k=0}^{l} \nabla_{\theta}\log\pi_{\theta}(a_k|s_k)\Big)\Big(\gamma^{l}r_{l+1} - b_{l} \Big)  \Big\rangle_{N}.
\end{align}
\hl{Apart from the baseline $b_l$, this expression is equivalent to} (\ref{eq:estrein}). 
\hl{In this work, we consider the GPOMDP estimation of gradient with the variance-minimizing baseline provided by} [\cite{Peters2008ReinforcementLO}]:
\begin{align} b_l = \frac{\Big\langle\Big( \sum_{k=0}^{l}\nabla_{\theta} \log\pi^{\theta}(a_k|s_k)\Big)^2 \gamma^{l}r_{l+1}\Big\rangle_{N}} {\Big\langle\Big( \sum_{k=0}^{l}\nabla_{\theta} \log\pi^{\theta}(a_k|s_k)\Big)^2\Big\rangle_{N}}
\end{align} \todo{Come fare i quadrati? Li ho trovati scritti cosÃ¬ in tutti i paper}
This is one of the most common policy gradient algorithms and we use it in this work to compare our algorithm with a standard algorithm in policy search.

\paragraph{Policy Gradient with Parameter-Based Exploration(PGPE)} \label{subsec:pgpe}
\ac{PGPE} [\cite{sehnke2008PolicyGradient}] is a method that estimates a gradient by directly sampling in parameter space. In \ac{PGPE} the policy is defined by a distribution over the parameters of deterministic controllers that we indicate with the function $\mu_{\theta}: \Sspace \to \Aspace$. At the beginning of each step, the parameter $\theta$ of the controller is sampled and then, the deterministic policy $\mu_{\theta}$ generated from $\theta$ is followed for all the episode length. Even if samples are collected with a detrministic policy, the choice of the policy is stochastic and then unexpected behaviour could arise in the task.\\
\newline
The variance of the estimates obtained with \ac{PGPE} is lower than the variance of the estimates obtained with REINFORCE [\cite{zhao2013efficient}]. This is due to the fact that, in REINFORCE, a repetitive sampling from a stochastic policy injects noise in the gradient estimate at every time-step. Furthemore, the variance increases linearly with the length of the history since each state depends on the entire sequence of previous samples.\\
\newline
As we said, in \ac{PGPE} the stochasticity that in REINFORCE was given by a stochastic policy $\pi_{\theta}$ is replaced by a probability distribution over the parameters $\theta$ themselves. In turn, this probability distribution is parameterized with parameter $\rho$, independent from $\theta$. Given a parameter space $\Theta$ for (deterministic) controllers $\mu_{\theta}, \theta \in \Theta$, the policy considered to perform exploration in the task is:
\begin{align}
\pi_{\rho}(a|s) = \int_{\Theta} p_{\rho}(\theta) \delta_{\mu_{\theta}(s)}\de \theta,
\end{align}
where $\delta_{\mu_{\theta}(s)}$ is the Dirac delta function corresponding to the deterministic controller $\mu_{\theta}$ depending on parameter $\theta$ sampled from a distribution with probability $p_{\rho}(\theta)$.\\
\newline
Given a trajectory $h \in \mathbb{H}$, where $\mathbb{H}$ is the set of possible trajectories, a probability $p_{\rho}(h, \theta)$ of sampling parameter $\theta$ and trajectory $h$ and defined $G(h)$ the return of trajectory $h$, we can define a suitable performance measure $J(\rho)$ as:
\begin{align}
J(\rho) = \int_{\Theta} \int_{\mathbb{H}} p_{\rho}(h, \theta) G(h) \de h \de \theta. \label{eq:pgpej}
\end{align}
From \ref{eq:pgpej}, we can write the expected value of the gradient of $J$ \wrt the distribution parameter $\rho$ as:
\begin{align}
\widehat{\nabla}_{\rho}J(\rho) \approx \nabla_{\rho} \log p_{\rho} (\theta) G(h),
\end{align}
where $\theta$ is sampled at the beginning of the episode and $h$ is resulting from the deterministic controller $\mu_{\theta}$. \hl{This is a} \acf{BBO} \hl{approach because it uses a constant policy perturbation during policy execution and it stores only scalar return in the trajectory} [\cite{stulp2012pol}]. \hl{Differently from REINFORCE, for instance,} \ac{PGPE} \hl{does not use information fron single time steps and, then, it does not exploit the temporal structure of} \ac{RL} problems.\\
\newline
Usually, we can consider the parameter $\rho$ learnt by the algorithm as $\rho = \Big( \{\mu_i\}, \{\sigma_i\} \Big)$, where $\mu_i$ and $\sigma_i$ are the parameters that determine an indipendent normal distribution
\paragraph{Deterministic Policy Gradient(DPG)} \label{subsec:dpg}
\ac{DPG} [\cite{article}] is a class of algorithms that learn a parametric deterministic policy $\mu_{\theta}$ defined as $\mu_{\theta}: S \mapsto A$. The approach in \ac{DPG} is similar to the one used in \ac{PG} methods that learn stochastic policies $\pi_{\theta}$, where the gradient of performance is estimated according to the Policy Gradient Theorem (\ref{subsec:pgt}). However, for deterministic policies the gradient of performance is slightly different than the one computed by the theorem (\ref{eq:pgt}).\\
\newline
Indeed, by considering a deterministic policy $\mu_{\theta}$ with parameter vector $\theta \in \mathbb{R}^n$ and performance objective $J(\mu_{\theta}) = \mathbb{E}_{s \sim \delta^{\mu}}[R(s, \mu_{\theta}(s))]$, the expression for \emph{Deterministic Policy Gradient} \hl{derived by the theorem is the following:}
\begin{theorem}[Deterministic Policy Gradient Theorem] \label{th:dpgt}
	Suppose that $P(s'|s,a), \nabla_aP(s'|s,a), \mu_{\theta}(s), \nabla_{\theta} \mu_{\theta}(s), R(s,a), \nabla_a R(s,a), \mu(s)$ \todo{mu usato per distribuzione iniziale degli stati e per policy deterministiche: correggere!!!!} are continuous in all parameters and variables $s, a, s'$.\todo{x variable in Sutton appendix?} Then, 
	\begin{align}
	\nabla_{\theta}J(\mu_{\theta}) = \mathbb{E}_{s \sim \delta^{\mu}} \Big[\nabla_{\theta} \mu_{\theta} (s) \nabla_a Q^{\mu}(s,a)|_{a = \mu_{\theta} (s)}\Big].
	\end{align}
\end{theorem}
\noindent In order to ensure exploration while learning a deterministic policy $\mu_{\theta}$, an \emph{off-policy} algorithm is introduced. %Usually, in off-policy algorithms, every expected value contains the \emph{importance sampling ratio} $\frac{\pi(a|s)}{\beta(a|s)}$ to adjust the fact that actions were selected with policy $\beta$ rather than policy $\pi$.\\
Both in our work and in \ac{DPG} \hl{algorithms a deteministic policy is learnt.} However, during the learning phase \ac{DPG} \hl{algorithms collect} samples from a stochastic behavioral policy, differently from \hl{what our goal aims: to avoid the execution of random actions.}\\
\newline
\hl{More precisely, relying on Theorem} \ref{th:dpgt} \hl{an off-policy actor-critic algorithm can be derived. It learns a deterministic target policy $\mu_{\theta}(s)$ from trajectories generated by an arbitrary stochastic behavioral policy $\pi(a|s)$}. A critic estimates the action-value function $Q^w(s,a) \approx Q_{\mu}(s,a)$, where $w$ is the parameter of the function approximator $Q^w(s,a)$. In [\cite{article}] the conditions required for a function approximator to be compatible (\ie avoid to introduce any bias in gradient update) are specified.\\
\newline
\acf{DDPG} \hl{is an algorithm of the} \ac{DPG} \hl{class that follows an off-policy actor-critic scheme. It uses four neural networks: an actor $\mu_{\theta}$, a critic $Q_{\phi}$, a target actor $\mu_{\theta targ}$ and a target critic $Q_{\phi targ}$, where $\theta, \phi, \theta_{targ}$ and $\phi_{targ}$ are the parameters of the neural networks. The behavioral policy is obtained from $\mu_{\theta}$ by adding a gaussian noise to the action prescribed. From the collected samples, a batch $B$ of samples $i = (s,a,r,s')$ is generated in order to perform a parameter update.}\\
\newline
\hl{The critic parameter $\phi$ is updated in a way such that the following \emph{loss measure} $L$ is minimized:}
\begin{align}
	L = \frac{1}{|B|} \sum_{i \in B}\Big(y_i - Q_{\phi}(s_i, a_i) \Big)^2,
\end{align}
\hl{where $y_i = r_i + \gamma Q_{\phi targ}\Big(s', \mu_{\theta targ}(s')\Big)$. The actor parameter $\mu$ is updated by one step of gradient ascent using an expression derived from Theorem} \ref{th:dpgt} \hl{and applying the inverse of \emph{chain rule}:}
\begin{align}
	\frac{1}{|B|}\sum_{i \in B}\nabla_{\theta}Q_{\phi}\Big(s, \mu_{\theta}(s)\Big)
\end{align}
\hl{Finally, the target policy are updated as follows, in order to ensure stability in learning according to hyperparameter $\rho$:}
\begin{align}
	\phi_{targ} &= \rho \phi_{targ} + (1 - \rho) \phi\\
	\theta_{targ} &= \rho \theta_{targ} + (1 - \rho) \theta
\end{align}

%\section{Safe \NoCaseChange{\acf{RL}}}\label{sec:saferl}
\section{Safe Reinforcement Learning}\label{sec:saferl}
In \ac{RL} several definitions of safety have been proposed and also methods that ensure a certain degree of safety according to some measures of risk. \hl{These definitions refer to different facets of safety, hence they are not conflicting to each other.} In this section, firstly we provide an overview of the different methods that face the problem of safe learning, using the same classification proposed by \citeauthor{JMLR:v16:garcia15a}, then we focus on the more specific issue related to \emph{safe exploration}.\\
\newline
According to [\cite{JMLR:v16:garcia15a}], there exist two main trends for Safe \ac{RL}. The first one is based on the modification of the optimality criterion to introduce the concept of risk. The second is based on the modification of the exploration process, so as to avoid the exploration of actions that may lead the system to undesirable or catastrophic situations. Regarding the first trend, there are several alternatives to quantify risk:
\begin{itemize}
	\item \emph{Worst Case Criterion}: a policy is considered to be optimal if it has the maximum return \wrt the worst-case \hl{scenario, namely the worst possible trajectory in terms of return}:
	\begin{align}
	\max_{\pi \in \Pi}\min_{h \in H^{\pi}} \EV_{\pi, w}[G_{0}],
	\end{align}
	where $H^{\pi}$ is a set of trajectories that occurs under the policy $\pi$ and the quantity to maximize-minimize is an expected value \wrt the policy $\pi$ and the trajectory $h$ [\cite{Heger1994ConsiderationOR}]. This criterion is used to mitigate the effects of variability induced by a given policy. It is also possible to use this kind of criterion when the transition function $P$ is uncertain:
	\begin{align}
	\max_{\pi \in \Pi}\min_{P} \EV_{\pi, P}[G_{0}],
	\end{align}
	\item \emph{Risk-Sensitive Criterion}: the optimization criterion balances return and risk by means of a scalar parameter that is included in the objective function and allows the sensitivity to the risk to be controlled. Risk can be defined, for instance, as the variance of return or as the probability of entering into an error state [\cite{Geibel2005RiskSensitiveRL}].
	\item \emph{Constrained Criterion}: the method maximizes the \hl{expectation of} return subject to one or more constraints. \hl{It is defined as:}
	\begin{align}
		\max_{\pi \in \Pi} \EV_{\pi}[G] \text{ subject to }c_i \in C,
	\end{align}
	\hl{where $G$ is the return and $C$ is the set of constraints $c_i$, with $c_i = f_i \leq t_i$. $f_i$ is a function related with the return and $t_i$ is its threshold} [\cite{moldovan2012safe}]. \hl{The set of constraints $C$ limits the space of allowable policies and makes this optimization criterion suitable for risky domains. The constraints, for instance, can ensure that the expectation of return exceeds a minimum threshold or that the variance of the return does not exceed a maximum threshold.}
	
\end{itemize}
Regarding the modification of the exploration process, it can be modified through the incorporation of external knowledge in three different ways:
\begin{itemize}
	\item \emph{Providing Initial Knowledge}: examples gathered from a teacher or previous information on the task can be used to provide an initial knowledge for the learning algorithm. From the ealier steps of the algorithm, the most relevant regions of the state and action spaces are visited. It considerably reduces random exploration \hl{providing guidance in the form of reasonable policies} [\cite{driessen2004integrating}].
	\item \emph{Deriving a policy from a finite set of demonstrations}: a set of examples provided by a teacher, that replace examples provided by exploration, can be used to learn a model from which to derive a policy in an off-line and, hence, safe manner. \hl{An example of this approach, related to autonomous helicopter flight, is provided in} [\cite{abbeel2010autonomous}].
	\item \emph{Providing Teach Advice}: a human or a simple controller assists the exploration during the learning process and provides advice. In some approaches, the teacher can provide advice only when the agent explicitly asks for, \hl{as in} [\cite{clouse1997on}], in others whenever it feels it is necessary. 
\end{itemize}
\hl{Alternatively, the exploration can be managed through the use of a risk measure that favors the execution of low-risk actions, this approach is called} \emph{Risk-directed Exploration}. \hl{An example of it is provided in} [\cite{gehring2013smart}], \hl{where the notion of} \emph{state controllability} \hl{is defined. This notion tells the agent in which states the effects of actions are easier to predict.}\\
\newline 
\hl{Accidents in learning systems may emerge from a poor design of real-world systems. According to} [\cite{amodei2016concrete}], \hl{the system could behave undesirably if the designers wrongly carry out one of the following:}
\begin{itemize}
	\item assign a wrong objective function: the function may not consider the negative side effects of some actions. Moreover, the function may allow the agent to hack the reward. Indeed, the agent may find a way to obtain an high reward with an unintended behavior;
	\item define an objective function that is too expensive to evaluate frequently;
	\item enable undesirable behavior during the learning phase: the agent may perform dangerous exploratory moves (\mySubsec{subsec:safeexp}) or it may have difficulties to recognize environments that are different from the training one. 
\end{itemize}
\hl{Another approach to safe} \ac{RL} \hl{is presented in} [\cite{thomas2019preventing}]. \hl{Here it is proposed a framework that simplifies the problem of specifying and regulating undesirable behavior. The framework allows to easily constrain the behavior of the algorithm, without requiring extensive domain knowledge or additional data analysis, by setting the maximum admissible probability of undesirable behavior.}

\subsection{Safe Exploration} \label{subsec:safeexp}
In our work, it is fundamental to ensure that the agent learns in complete safety (\ie the agent doesn't perform undesired actions throughout the learning phase). The second class of safe methods is suitable for this. However, it requires external knowledge, in the form of experience samples or teacher advice, that is not always available. Instead, the methods of the first class can learn an optimal policy (according to a definition of return that includes risk) but they trade-off short term loss in performance for a long term gain. This is not accettable in the case of safety-critical applications, \hl{regardless of whether the return contains a measure of risk. Moreover, when the risk is encoded in the reward function, any sort of performance oscillation points out a dangerous situation that} can cause failure or harm the environment. \hl{In order to avoid this source of risk, a monotonic improvement in performance can be required while learning the policy.} Here we describe two state of the art approaches to safe exploration, the first one for finite \ac{MDPs} and the second one for policy gradient methods. Both of the methods rely on a statistical confidence $\epsilon$ in order to define safety constraints, then the complete avoidance of unwanted actions is only ensured with high probability. 

\paragraph{Safe Exploration in Finite MDPs} \label{subsec:fmdp}
[\cite{turchetta2016}] address the problem of safely exploring finite \ac{MDPs}, where safety is defined in terms of a constraint that satisfies regularity conditions. The algorithm, called SafeMDP, cautiously explores safe states and actions, obtains noisy observations and gains knowledge on the safety of unvisited state-action pairs.
Starting from a set of states and actions that are known to be safe, the regularity assumptions are exploited in order to evaluate only state-action pairs known to fulfill the safety constraint.
The reward (encoding a measure of safety) is unknown and drawn from a gaussian distribution. At each iteration of the algorithm a posterior distribution is computed from the sampled rewards, affected by an additive noise drawn from a zero-mean gaussian distribution. The reward function $R(s)$ is Lipschitz continuous, with Lipschitz constant $L_{R}$. Since only noisy measurements are observed, $R(s)$ is known up to some statistical confidence $\epsilon$. By considering the Lipschitz continuity of $R(s)$ and the confidence $\epsilon$ and starting from some safe set $\Sspace_s$, the resulting set of safe states is:
\begin{align}
R_{\epsilon}^{safe}(\Sspace_s) = \Sspace_s \cup \{s \in \Sspace | \exists s' \in \Sspace_s: R(s') - \epsilon - L_{R} d(s, s') \geq h\}, \label{eq:safe}
\end{align}
where $h$ represents a safety threshold. The obtained set is then restricted by considering only the safe states that are reachable and from which we can move to other safe states.
By iteratively applying the operator $R_{\epsilon}^{safe}$ in (\ref{eq:safe}) to the restricted set of safe states $\Sspace_s$ and updating the gaussian distribution of $R(s)$ from samples, we obtain the largest set of states that can be safely reached by the exploring agent.

\paragraph{Safe Policy Gradients} \label{subsec:safepg}
\acf{SPG} is an algorithm from [\cite{papini2019}] in which the learning agent is constrained to never worsen its performance during learning. According to the classification of safe \ac{RL} methods, the algorithm \hl{is included in the \emph{constrained criterion} class} as oscillating performances may violate the constraint, called \emph{Monotonic Improvement}. \hl{It can be considered a safety constraint only when the risk is perfectly encoded in the reward.} \hl{This work involves} actor-only \ac{PG} from a stochastic optimization perspective and \emph{smoothing policies}, \ie \hl{twice-differentiable parametric} policies $\pi_{\theta}$, with $\theta \in \Theta$, for which \hl{properties\footnote{The parameter space $\Theta$ is convex. For every state $s$ and in expectation over actions $a \sim \pi_{\theta}(\cdot|s)$, the euclidean norm $\norm{\nabla \log \pi_{\theta}(a|s)}$, the squared euclidean norm $\norm{\nabla \log \pi_{\theta}(a|s)}^2$ and the spectral norm $\norm{\nabla\nabla^T \log \pi_{\theta}(a|s)}$ are upper-bounded by non-negative constants.} that induce the smoothness of performance are valid. In} [\cite{papini2019}] \hl{it is shown that gaussian and softmax policies are smoothing.} From upper bounds on the variance of policy gradient estimators, a lower bound on the performance improvement (with a certain confidence $\epsilon$) provided by gradient-based updates is calculated and expressed as a function of some meta-parameters. These meta-parameters are the step size $\alpha$ of the parameter updates and the batch size $N$ of the gradient estimators. Then, the adaptive meta-parameters that guarantee monotonic improvement with high probability are identified.\\
\newline 
\ac{PG} methods may suffer from the explosion of gradients when the current policy is close to be deterministic, leading to unstable training process. \acf{TDL} [\cite{DBLP:journals/corr/abs-1905-11041}] addresses this problem, alternating between proposing a target distribution and training the policy to approach the target distribution. The target policy is the solution of a constrained optimization problem where the constraints involve the difference between updated policies, so that the algorithm leads to more stable improvements.