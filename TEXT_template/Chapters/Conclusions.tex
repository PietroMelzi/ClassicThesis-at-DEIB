\chapter{Conclusions} \label{chap:conc}
We proposed a policy optimization algorithm (\ac{DPO}) that is completely deterministic for the whole learning process and allows to learn a satisfactory deterministic policy in tasks where the environment meets some Lipschitz properties of regularity. We tested the validity of the proposed approach on simulated control tasks and we provided the results of the performed experiments in \MySec{chap:exp}. In \MySec{chap:proofs}, we also provided theoretical support to this approach under these regularity assumptions on the environment, which we deem realistic for continuous control problems of practical interest [\cite{kober2013reinforcement}]. Indeed, in many continuous problems from industrial robotics, for instance, the effects of actions vary smoothly with the observed states and with the actions themselves. Driven by the goal of finding a learning approach able to ensure safe exploration, we proposed the construction of an abstract \ac{MDP} ($\delta$-\ac{MDP}) where the exploration usually obtained with random actions is replaced with a passive exploration, ensured by the regularity assumptions on the environment. The support provided by the theory on $\delta$-\ac{MDP} combined with the advantages provided by \ac{PS} methods, such as the robustness to noise or the possibility of encoding domain knowledge in the policy definition, motivated us to follow the strategy presented in this work.\\
\newline
We have shown, empirically, that \ac{DPO} is competitive with policy gradient methods, at least in the fine-tuning of parametric controllers. We think that the latter experiment, that we discussed in \mySubsec{sec:safe}, is an important example for the deploying of lifelong-learning agents in the real world, where changes in the environment and in the goal can occur. This kind of setting may represent a natural application for \ac{RL} in the near future. In this setting, \ac{DPO} can remove the unnecessary source of risk deriving from random actions. As a consequence, several problems related to the safety of the agent's hardware and of the environment are avoided. Random exploration may still play an indispensable role in learning challenging tasks \textit{from scratch}, which should be done in a controlled environment. Indeed, the limitation of the feasible actions for each abstract state of the $\delta$-\ac{MDP} prevents, in many cases, to evaluate the optimal actions and an excessive number of iterations may be necessary before that these actions can be evaluated.\\
\newline
Being the first algorithm of his kind, \ac{DPO} leaves room for several improvements that we present here as open questions and future research directions of this work.
First of all, state aggregation could be performed in a more informed way, in order to guarantee a more \textit{efficient exploration}. In this work, we analyzed some theoretical aspects of state abstraction (\MySec{sec:stdisc}) and we decided to consider only partitions of the state space in which all the subsets have the same size. However, we captured an important trade-off between precision and exploration in the algorithm that can be addressed by selecting a finer or coarser aggregation of states (Figure~\ref{fig:mass}). Starting from this trade-off, new solutions for the state aggregation can be evaluated: partitions of the state space with finer subsets covering the most visited regions and coarser subsets covering the less visited ones, adaptive discretization through algorithm's iterations or soft aggregation [\cite{NIPS1994_981}]. Moreover, we could make a more efficient use of the collected data, by re-using samples collected from previous policies once they are put in relationship with the new policy. Establishing a relationship between consecutive $\delta$-\ac{MDPs} may also prove necessary to provide convergence guarantees for \ac{DPO}. Since \ac{DPO} only removes the risk deriving from action stochasticity, a further extension of this work that would be of great practical relevance is the combination of \ac{DPO} with other safe \ac{RL} approaches. Removing the algorithm's hyperparameters is also important, since a deployed agent cannot perform grid-search. Finally, when available, prior knowledge on the controlled system should be integrated with collected samples in order to better estimate the abstract model of the $\delta$-\ac{MDP}.