\chapter{Introduction} \label{ch:intro}

%rl
One of the challenges that arise in \acf{RL} is the trade-off between exploration and exploitation. To obtain an high reward, an agent must perform actions that it has found to be effective in producing reward. But to discover such actions, it has to try actions that it has not selected before [\cite{sutton2018reinforcement}]. An intuitive strategy to address the issue is to perform random actions in the beginning of the task, so as to test as many actions as possible in a first phase and then exploit the most rewarding actions in the long term. By executing random actions in any state, the agent potentially executes and evaluates the entire range of feasible behaviours. An approach of this kind strongly supports exploration and provides future advantages to the agent.\\
\newline
%problem
If the agent evolves into a simulator, no concerns are related to the strategy, however when the agent interacts with a real-life environment serious drawbacks come up. The lack of command on the agent's behaviour can lead to dangerous actions that damage the agent's hardware or, even worse, harm the humans that operate in the environment. Because of this safety issue, \ac{RL} techniques are scarcely used in fields where they could provide outstanding benefits as industrial robotics, surgery [\cite{baek2018PathPlanning}] or autonomous driving. Again, another important scenario in which the randomness of actions is unwanted is finance: certainly nobody wants to perform stochastic operations involving their money. In \ac{RL} several definitions of safety have been proposed (we discuss them in \mySec{sec:saferl}), however the safety problem due to randomness of actions is poorly addressed. The reason is that in \ac{RL} the stochasticity is essential for exploration. Indeed, if the agent always does the same action in every state, it cannot find any better action that improves its behaviour.\todo{stress the concpet: no rdm actions, "even single unsafe actions may cause failure or harm the environment"}\\
\newline
%solution
Driven by these motivations, we explore an alternative strategy, that ensures safeness in every instant of the agent-environment interaction by monitoring the explorative behaviour of the agent. The solution we propose is suitable for regular environments, \ie environments in which performing the same action in similar states produces similar effects. In these environments, once an action is executed in a certain state, we can evaluate its effect in other (similar) states, without necessarily redoing the action. This assumption provides a sort of exploration that the agent can exploit instead of relying on the stochasticity of its actions. As a result, in any state the agent performs only a deterministic action but evaluates all the actions that have been executed in similar states. Only a subset of the possible action is evaluated, then the learning ability of the agent is significantly reduced in this approach: learning an optimal behaviour becomes infeasible or requires an excessive amount of time. On the other hand, the agent is able to learn and improve his behaviour without the need to perform random actions.\\
\newline
%conclusion
We present our work... \todo{descrizione delle sezioni}