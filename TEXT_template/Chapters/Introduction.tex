\chapter{Introduction} \label{ch:intro}

%rl
\acf{RL} is a machine learning field that includes techniques that allow an agent to learn the best possible behavior from its interaction with the environment. In \ac{RL}, the agent explores different situations and evaluates their effectiveness according to the rewards it receives from the environment. \ac{RL} techniques are applied in numerous real-world domains, such as recommender systems, computer systems, energy, finance, healthcare, robotics, transportation. Details on specific applications are discussed in [\cite{yuxi2019}].
One of the challenges that arise in \ac{RL} is the trade-off between exploration and exploitation. To obtain a high reward, an agent must perform actions that it has found to be effective in producing reward. However, to discover such actions, it has to try actions it has not selected before [\cite{sutton2018reinforcement}]. An intuitive strategy to address this issue is to perform random actions at the beginning of the task, so as to test as many actions as possible in a first phase and then exploit the most rewarding actions in the long term. By executing random actions in any state, the agent potentially executes and evaluates the entire range of feasible behaviors it can undertake. Such an approach strongly supports exploration and provides future advantages to the agent. If the agent learns within a simulator there are no concerns related to this strategy. Unfortunately, a simulator is not always available: when knowledge of the environment is lacking or too much complexity is required to faithfully reproduce the real system, the agent has to learn directly in the real world. When the agent interacts with a real-life environment, serious drawbacks may come up. For instance, the randomness of the action selections allows the execution of unwanted actions in some specific situations.
%problem
\paragraph{1.1 Motivations} \mbox{} \\
\newline
A lack of control over the agent's behavior in the real world can lead to dangerous actions that may damage the agent's hardware or, even worse, harm people that operate in the environment. Partly because of this issue raised by safety concerns, \ac{RL} techniques are scarcely used in fields where they could otherwise provide outstanding benefits as industrial robotics, surgery [\cite{baek2018PathPlanning}] or autonomous driving. Again, another important scenario in which the randomness of actions is unwanted and deemed dangerous is finance: certainly nobody wants to perform stochastic operations involving their own money, which is different than making bets in an informed way. In all of these fields, the effectiveness of \ac{RL} techniques is concealed by the willingness to ensure a predictable agent's behavior. Indeed, even a single execution of a random and unsafe action may cause a failure in the task or harm the environment, which is often less acceptable than the consequences of epistemic uncertainty or an aleatory environment. According to [\cite{garcia2015}], the latter can be referred to as \emph{inherent uncertainty} and it is unavoidable, the former is called \emph{parameter uncertainty} and it is due to the lack of knowledge of the environment. Parameter uncertainty can be reduced by performing random actions that explore the environment but introduce a new source of uncertainty: \emph{agent uncertainty}. Several definitions of safety have been proposed in \ac{RL} (we discuss them in \MySec{sec:saferl}). These definitions involve different aspects of safety, such as the variance in the reward signal received by the agent or the ability to avoid the exploration of dangerous regions of the environment, however, the safety problem due to the randomness of the actions is poorly addressed. Moreover, many techniques in safe \ac{RL} ensure convergence to a safe behavior but do not set any constraint on intermediate solutions. In \ac{RL} the stochasticity on actions, widely used for intermediate solutions, appears to be essential for successful learning. Indeed, if the agent always performs the same action in every state, it may not find any better action to improve its behavior. In this work, we want to contradict the idea that stochastic behavior is fundamental to the agent's learning process.
%deterministic
\paragraph{1.2 Goal} \mbox{} \\
\newline
Driven by these motivations, we explore an alternative strategy, which ensures the safeness of the agent-environment interaction in every instant by forcing the explorative behavior of the agent to be deterministic. In particular, exploration in the environment is performed according to the same deterministic policy that the agent is learning, in order to perfectly monitor the agent's behavior. In addition, we opt for risk-averse approaches to implement specific parts of our strategy in order to strengthen the concept of safeness. Several algorithms that learn a deterministic policy have been proposed in the literature, most of which are suitable for environments with a finite number of feasible actions. In this work, we consider environments with continuous (possibly infinite) actions, also for them there are algorithms that learn deterministic policies. Two of these algorithms, \acf{PGPE} [\citet{sehnke2008PolicyGradient}, \mySubsec{subsec:pgpe}] and \acf{DPG} [\citet{article}, \mySubsec{subsec:dpg}], have been considered in this work for a comparison with our algorithm, called \acf{DPO}. Anyhow, to the best of our knowledge, all existing algorithms in continuous \ac{RL} involve the execution of random actions.
%solution
\paragraph{1.3 Contributions} \mbox{} \\
\newline
The solution we propose is free from this issue and it is suitable for regular environments, \ie environments where performing the same action in similar states produces similar effects. In these environments, once an action is executed in a certain state, we can evaluate its effect in other (similar) states, without necessarily redoing the action. This assumption provides a sort of passive exploration that the agent can exploit, instead of relying on the stochasticity of its actions. As a result, in any state, the agent performs only a deterministic action and evaluates, within a certain precision, all the actions that have been executed in similar states. Because of this, only a subset of all feasible actions is evaluated in every state, therefore the learning ability of the agent may be significantly reduced in this approach: learning an optimal behavior may no longer be feasible or require an unreasonable amount of time. On the other hand, this approach allows the agent to learn and improve its behavior without the necessity of performing any random action. If the environment is partially observable or we consider multi-agent contexts, the optimal behavior may require stochasticity on actions. However, we do not consider these settings in the work. We tested our algorithm on simulated continuous control tasks obtaining promising results.
%conclusion
\paragraph{1.4 Outline} \mbox{} \\
\newline
In this document we present our work according to the following structure: \MyChap{chap:rl} provides the theory (and notation) related to \acf{RL} and \acf{MDP}, \MyChap{chap:state} illustrates the state of the art in \acf{PG} and safe \ac{RL} from which we started to develop our algorithm, \MyChap{chap:dpo} describes the \ac{DPO} algorithm, providing details on the implementation of its different building blocks, \MyChap{chap:proofs} contains a theoretical analysis of our approach, in \MyChap{chap:exp} the performed experiments are reported and discussed, at last \MyChap{chap:conc} concludes the thesis by providing some hints for the future development of this work.