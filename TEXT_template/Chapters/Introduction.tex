\chapter{Introduction} \label{ch:intro}

%rl
One of the challenges that arise in \acf{RL} is the trade-off between exploration and exploitation. To obtain an high reward, an agent must perform actions that it has found to be effective in producing reward. But to discover such actions, it has to try actions that it has not selected before [\cite{sutton2018reinforcement}]. An intuitive strategy to address this issue is to perform random actions at the beginning of the task, so as to test as many actions as possible in a first phase and then exploit the most rewarding actions in the long term. By executing random actions in any state, the agent potentially executes and evaluates the entire range of feasible behaviors that it can undertake. An approach of this kind strongly supports exploration and provides future advantages to the agent. If the agent learns within a simulator there are no concerns related to this strategy. Unfortunately, a simulator is not always available: when the knowledge of the environment is missing or too much complexity is required to faithfully reproduce the real system, the agent has to learn directly in the real world. When the agent interacts with a real-life environment serious drawbacks may come up.\\
\newline
%problem
A lack of control on the agent's behavior in the real world can lead to dangerous actions that may damage the agent's hardware or, even worse, harm the humans that operate in the environment. Partly because of this issue raised by safety concerns, \ac{RL} techniques are scarcely used in fields where they could otherwise provide outstanding benefits as industrial robotics, surgery [\cite{baek2018PathPlanning}] or autonomous driving. Again, another important scenario in which the randomness of actions is unwanted and deemed dangerous is finance: certainly nobody wants to perform stochastic operations involving their money\hl{, which is different than making bets in an informed way.} In all of these fields the effectiveness of \ac{RL} techniques is concealed by the willingness to ensure a predictable agent's behavior. Indeed, even the single execution of a random and unsafe action may cause a failure in the task or harm the environment, which is often less acceptable than the consequences of epistemic uncertainty or an aleatory environment. In \ac{RL} several definitions of safety have been proposed (we discuss them in \mySec{sec:saferl}). These definitions involve different aspects of safety, such as the variance in the reward signal received by the agent or the ability to avoid the exploration of \hl{dangerous regions of the environment}, however the safety problem due to randomness of actions is poorly addressed. Moreover, many techniques in safe \ac{RL} ensure the convergence to a safe behavior but don't set any constraint on intermediate solutions. In \ac{RL} the stochasticity \hl{on actions, widely used for intermediate solutions, appears to be} essential for \hl{successful learning}. Indeed, if the agent always does the same action in every state, it \hl{may not} find any better action that allows to improve its behavior. \hl{In this work we want to contradict the idea that stochastic behavior is fundamental to the agent's learning process.}\\
\newline
%deterministic
Driven by these motivations, we explore an alternative strategy, that ensures the safeness of the agent-environment interaction in every instant by forcing the explorative behavior of the agent to be deterministic. In particular, the exploration in the environment is performed according to the same deterministic policy the agent is learning, in order to perfectly monitor the agent's behavior. In the literature, several algorithms that learn a deterministic policy have been proposed. Two of them, \acf{PGPE} [\citet{sehnke2008PolicyGradient}, \mySubsec{subsec:pgpe}] and \acf{DPG} [\citet{article}, \mySubsec{subsec:dpg}], have been considered in this work for a comparison with our algorithm, called \acf{DPO}. Anyhow, to the best of our knowledge, all the existing algorithms in continuous \ac{RL} involve the execution of random actions.\\
\newline
%solution
The solution that we propose is free from this issue and it is suitable for regular environments, \ie environments in which performing the same action in similar states produces similar effects. In these environments, once an action is executed in a certain state, we can evaluate its effect in other (similar) states, without necessarily redoing the action. This assumption provides a sort of passive exploration that the agent can exploit, instead of relying on the stochasticity of its actions. As a result, in any state the agent performs only a deterministic action and evaluates, within a certain precision, all the actions that have been executed in the similar states. Because of this, only a subset of all the feasible actions is evaluated in every state, therefore the learning ability of the agent may be significantly reduced in this approach: learning an optimal behavior may be no more feasible or require an unreasonable amount of time. On the other hand, this approach allows the agent to learn and improve its behavior without the necessity of performing any random action. \hl{If the environment is partially observable or we consider multi-agent contexts, the optimal behavior may require stochasticity on actions. However we do not consider these settings in the work.} We tested our algorithm on simulated continuous control tasks obtaining promising results.\\
\newline
%conclusion
In this document we present our work according to the following structure: \MyChap{chap:rl} provides the theory (and the notation) relatd to \acf{RL} and \acf{MDP}, \MyChap{chap:state} illustrates the state of the art in \acf{PG} and safe \ac{RL} from which we started to develop our algorithm, \MyChap{chap:dpo} describes the \ac{DPO} algorithm, providing details on the implementation of its different building blocks, \MyChap{chap:proofs} contains theoretical analysis of our approach, in \MyChap{chap:exp} the performed experiments are reported and discussed. \todo{aggiungere le sezioni mancanti + appendix}