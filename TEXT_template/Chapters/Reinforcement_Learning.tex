\chapter{Preliminaries on Reinforcement Learning} \label{chap:rl}

According to [\cite{sutton2018reinforcement}], \acf{RL} consists in learning what to do so as to maximize a numerical reward signal. The learner must discover which actions yield the most reward by trying them. Actions may affect not only the immediate reward but also the next situation and all subsequent rewards. The problem of \ac{RL} is formalized with mathematical tools coming from dynamical systems theory, specifically with the \acf{MDPs} that we detail in \mySec{sec:mdp}.\\
\newline
\MyChap{chap:rl} provides a strong theoretical background for our work. After the introduction on \ac{MDPs} in \mySec{sec:mdp}, \mySec{sec:tabmet} addresses the \emph{Tabular Methods} used to solve \ac{MDPs}. Even if the application of these methods is limited by computational costs, they provide basic concepts in \ac{RL}. \MySec{sec:ps} explores \emph{\acf{PS}}, a class of methods that solve \ac{RL} problems when \emph{Tabular Methods} are infeasible. \ac{PS} offers several advantages in the robotic field (described in \mySec{sec:ps}) and for this reason it is widely used there. Then, we present two special types of \ac{MDPs} in \mySec{sec:specmdp}: \emph{Bounded \ac{MDPs}}, suitable to model uncertainty in the environment, and \emph{Lipschitz \ac{MDPs}}, suitable to model regularity in the environment. Finally, \mySec{sec:stdisc} presents some topics related to state discretization that will be useful for the description of our approach.

%\section{\NoCaseChange{\acf{MDPs}}}\label{sec:mdp}
\section{Markov Decision Processes}\label{sec:mdp}
\ac{MDPs} are a mathematical framework used for modeling \ac{RL} problems in which an \emph{agent}, by interacting with an \emph{environment}, learns how to achieve a goal. \ac{MDPs} are a formalization of sequential decision making, where the decision taken by the agent influences immediate and future rewards. The agent interacts with the environment by selecting the actions to perform and receiving rewards and information on the new situation. Rewards are provided by the environment in the form of scalar values and the agent aims to maximize the sum of rewards over time.
\begin{definition}[MDP]\label{def:mdp}
\sloppy A \acf{MDP} is described by a six-tuple $M=\langle \Sspace, \Aspace, P, R, \gamma, p_0 \rangle$, where:
\begin{itemize}
	\item $\Sspace$ is the state space, with $\Sspace\subseteq\Reals^N$.
	\item $\Aspace$ is the action space, with $\Aspace\subseteq\Reals^D$.
	\item $P: \Sspace \times \Aspace \to \Delta(\Sspace)$ is the transition function, with $P(s'|s,a)$ denoting the probability of reaching state $s'$ from state $s$ by taking action $a$. $P(\cdot|s,a)$ is a distribution of probability on the arriving state, then for any state-action pair $(s,a)$, the following equality holds:
	\begin{align} \sum_{s' \in \Sspace} P(s'|s,a) = 1 \end{align}
	and $P(s'|s,a) \geq 0 \quad \forall s, s' \in \Sspace, a \in \Aspace$.
	\item $R: \Sspace \times \Aspace \to \Reals$ is the reward function, with $R(s,a)$ denoting the expected reward from taking action $a$ in state $s$. Usually the reward function is bounded with a finite value $\overline{R}$ such that $|R(s,a)| \leq \overline{R} \quad \forall s \in \Sspace, a \in \Aspace$.
	\item $\gamma \in [0, 1)$ is the discount factor, used to discount the present effect of future rewards.
	\item $p_0 \in \Delta(\Sspace)$ is the initial-state distribution.
\end{itemize}
\end{definition}
\noindent The transition function $P$ defines the dynamics of the \ac{MDP} and satisfies the \emph{Markov property}: the probability of reaching $s_t$ depends only on the immediately previous state and action, $s_{t-1}$ and $a_{t-1}$, and not on earlier states and actions.\\
\newline
The interaction between agent and environment can be better explained with symbols: at each time step $t$ the agent receives a representation of the state $s_t \in \Sspace$ and on that basis it selects an action $a_t \in \Aspace$. One time step later, the agent receives a reward $r_{t+1}$ and finds itself in a new state $s_{t+1} \in \Sspace$. We have defined the \ac{MDP} following [\cite{puterman2014markov}]. In general, the state space $\Sspace$ and the action space $\Aspace$ can be finite or continuous sets. We focus on continuous-space \ac{MDPs} as these are the most suitable for modeling continuous control problems.
\subsection{Policy and Value Functions}
The behaviour of the agent is modeled with a \emph{policy} $\pi: \Sspace \to \Delta(\Aspace)$, \ie a mapping from states to probabilities of selecting each possible action. The agent should learn a policy according to its goal of maximizing the sum of rewards collected during the task. Specific details on how to learn policies that achieve this goal are given later, in \mySec{sec:tabmet} and \mySec{sec:ps}. The sum of rewards obtained by the agent, if we consider the time steps $k \in (t, T]$, where the time step $T$ represents the horizon of the task, is called \emph{return} $G_t$. In general, the return is defined as a sum of discounted rewards:
\begin{align}G_t = \sum_{k=t+1}^{T} \gamma^{k-t-1} r_k, \end{align} where $r_k$ is the reward obtained at step $k$. The horizon $T$ can be finite or infinite. In the first case, the task is said to be \emph{episodic} and the agent-environment interaction breaks naturally into episodes. In the second case (\ie $T = \infty$), the task is said to be \emph{continuing} and a discount factor $\gamma<1$ is required in order to obtain a return $G_t < \infty$.\\
\newline
In this work, we denote with $\pi(a|s)$ the probability of performing the action $a$ in the state $s$, according to the policy $\pi$. Since $\pi(s)$ is a distribution of probability, the following equality holds:
\begin{align} \sum_{a \in \Aspace} \pi(a|s) = 1 \quad \forall s \in \Sspace \end{align}
and $\pi(a|s) \geq 0 \quad \forall s \in \Sspace, a \in \Aspace$.
If the set of actions $\Aspace$ is finite and for each state $s \in \Sspace$ there exists an action $a$ such that $\pi(a|s) = 1$, the policy is deterministic. If the set of actions $\Aspace$ is continuous and for each state $s \in \Sspace$ the probability distribution $\pi(s)$ is a Dirac delta function, the policy is deterministic. In the case of deterministic policies, $\pi(s)$ identifies the action prescribed by $\pi$ when the agent is in state $s$.\\
\newline
Given a policy $\pi$, it is possible to compute, according to the policy, the \emph{value function} $V^{\pi}: \Sspace \rightarrow \mathbb{R}$. This is a widely used function in \ac{RL} that measures how good it is for the agent to be in a given state $s \in \Sspace$, according to the expected return obtainable from that state. Since the rewards that the agent can expect to receive in the future depend on the actions taken in any state, the value function is defined with respect to policies: $V^{\pi}(s)$ is the expected sum of discounted rewards that the agent collects by starting at state $s$ and following policy $\pi$. The value function can be defined recursively via the Bellman equation: 
\begin{align} V^{\pi}(s) = \int_{\Aspace} \pi(a|s) \Big(R(s,a) + \gamma \int_{\Sspace}P(s'|s,a)V^{\pi}(s')\de s' \Big) \de a. \label{eq:v}\end{align}
For control purposes, we can also define an action-value function $Q^{\pi}: \Sspace \times \Aspace \rightarrow \mathbb{R}$: \begin{align} Q^{\pi}(s,a) = R(s,a) + \gamma \int_{\Sspace} P(s'|s,a) \int_{\Aspace} \pi(a'|s') Q^{\pi}(s',a') \de a' \de s'.\end{align}
$Q^{\pi}(s,a)$ represents the expected return obtained from taking the action $a$ in state $s$ and then following the policy $\pi$. \\
\newline
Finally, we denote with
\begin{align} A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s) \end{align}
the advantage function of policy $\pi$, that represents the advantage in terms of value functions given by performing action $a$ in state $s$, instead of the action prescribed by $\pi(s)$.

\subsection{Performance Measure}
In order to evaluate how good a policy $\pi$ is, we consider the expected return obtained starting from a state $s \in \Sspace$ drawn from the initial-state distribution $p_0$ and following the policy $\pi$. The \emph{performance measure} $J(\pi)$ expressed in the form of an expected value is:
\begin{align} J(\pi) = \EV_{s_0\sim p_0}[G_0] \quad = (1 - \gamma)^{-1}\EV_{s\sim\delta^{\pi}, a \sim \pi}[R(s,a)] \quad = \EV_{s\sim p_0}[V^{\pi}(s)], \end{align}
where $\delta^{\pi}$ is the $\gamma$-discounted future-state distribution. This function is defined as:
\begin{align} \delta^{\pi}(s) = (1 - \gamma)\EV_{s_{0}\sim p_0}\sum_{t=0}^{\infty}\gamma^t P^{\pi}(S_t = s|S_0=s_0) \end{align}
and represents the probability of being in a certain state $s$ during the execution of the task, provided that the policy is $\pi$ and the initial state distribution is $p_0$.\\
\newline
The performance measure is used to identify the optimal policy $\pi^{*}$ that we want to learn in the \ac{RL} problem as:
\begin{align} \pi^{*} \in \arg \max_{\pi} J(\pi). \label{eq:optj}\end{align}
To be precise, in \ac{RL} the optimal policy $\pi^{*}$ is required to maximize the value function (defined as in \myEq{eq:v}) for each state $s \in \Sspace$. The methods described in \mySubsec{subsec:dp} allow to obtain optimal policies that satisfy this property. Instead, the set of optimal policies obtained from (\ref{eq:optj}) includes policies whose value function is not maximum in every state. The criterion used to identify optimal policies in \myEq{eq:optj} is weaker\footnote{The performance measure $J$ depends on the initial state distribution $p_0$. We consider, for instance, an \ac{MDP} whose state space $\Sspace$ is composed of two regions such that it is not possible to reach one region from the other. If the probability of being in one of these two regions is equal to zero (according to $p_0$), the policies maximizing $J$ are the ones that maximize $J$ in the visited states, regardless of their performance in the unvisited region.} than the one used in \mySubsec{subsec:dp}, however it is appropriate in our work.

\subsection{Properties of \ac{RL} techniques}
\ac{RL} algorithms require exploration to learn the optimal policy $\pi^{*}$. Exploration is provided by the agent that evolves in the \ac{MDP} according to the current policy $\pi$ and collects information about states, actions and rewards at each time step $t$, until the (possibly infinite) horizon $T$ of the episode. All the information gathered by the agent is represented by a tuple called \emph{trajectory}: $\langle s_0, a_0, r_1, s_1, a_1, ..., s_{T-1}, a_{T-1}, r_{T} \rangle$, where $s_{0} \sim p_0$, $a_{t} \sim \pi(\cdot|s_{t})$, $r_{t} = R(s_{t}, a_{t})$, $s_{t+1} \sim P(\cdot|s_{t}, a_{t})$. If the task is episodic, usually the agent collects a \emph{batch} of $N$ different trajectories with the same policy $\pi$ before updating it. The number $N$ of trajectories is called \emph{batch size}.\\
\newline
The policy used to generate trajectories can be different from the policy that the \ac{RL} technique learns. We can distinguish between:
\begin{itemize}
	\item \emph{On-policy algorithms:} the policy used to interact with the environment is the policy that is being learnt;
	\item \emph{Off-policy algorithms:} the policy used to interact with the environment is different from the policy that is being learnt. It is called \emph{behavioral policy}.
\end{itemize}
Another differentiation, important for our work, is between:
\begin{itemize}
	\item \emph{Model-based algorithms:} the agent knows or estimates the model of the environment, as in the algorithm we present;
	\item \emph{Model-free algorithms:} the agent has no knowledge on the model of the environment.
\end{itemize}

\section{Tabular Solution Methods}\label{sec:tabmet}
Solving a \ac{RL} task means finding a policy that maximizes the return obtained by the agent in the task. First, we consider finite \ac{MDPs}, a subset of the \ac{MDPs} defined in \ref{def:mdp}. In order to reason with finite \ac{MDPs}, we consider the state space $\Sspace$ and the action space $\Aspace$ as finite sets of discrete values and we replace all the integrals appearing in the expressions reported so far with summations. The methods used to solve problems involving finite \ac{MDPs} are called \emph{tabular} because the state space $\Sspace$ and the action space $\Aspace$ are small enough for the value functions to be represented in a tabular format.\\
\newline
Policies can be partially ordered according to their value function: 
\begin{align}
\pi' \succcurlyeq \pi \iff V^{\pi'}(s) \geq V^{\pi}(s) \quad \forall s \in \Sspace. \label{eq:ordpol}
\end{align} 
In finite \ac{MDPs}, there always exists a deterministic optimal policy $\pi^{*}$ such that $\pi^{*} \geq \pi \quad \forall \pi \in \Pi$, said $\Pi$ the set of policies. The value function $V^{*}$ computed according to $\pi^{*}$ has the following property:
\begin{align} V^{*}(s) = \max_{\pi}V^{\pi}(s) \quad \forall s \in \Sspace. \end{align}
Optimal value function $V^{*}$ and optimal action-value function $Q^{*}$ can be written with the Bellman optimality equations:
\begin{align}
V^{*}(s) &= \max_a \Big( R(s,a) + \gamma \sum_{s'}P(s'|s,a)   V^{*}(s') \Big)\\
Q^{*}(s,a) &= R(s,a) + \gamma \sum_{s'}P(s'|s,a) \max_{a'} Q^{*}(s', a').
\end{align}
If the agent has a \emph{complete knowledge} of the environment, \ie the agent knows the transition function $P$ and the reward function $R$ of the \ac{MDP} $M$ representing the environment, \acf{DP} can be used to solve the \ac{RL} problem. Details are provided in \mySubsec{subsec:dp}. Instead, if the agent has an \emph{incomplete knowledge} of the environment, the unknown functions $P$ and $R$ of \ac{MDP} $M$ can be estimated from experience. Details are provided in \mySubsec{subsec:alt}.\\
\newline
When the problems involve finite \ac{MDPs} with a larger state space or continuous \ac{MDPs}, tabular methods are no more suitable because value functions cannot be represented as tables. Two main approaches are possible:
\begin{itemize}
	\item the value functions can be estimated with \emph{function approximators} and used to solve the task;
	\item the optimal policies can be directly searched in the space of policies, without the necessity of computing any value function. Details are provided in \mySec{sec:ps}.
\end{itemize}

\subsection{Dynamic Programming}\label{subsec:dp}
\acf{DP} is a collection of algorithms that can be used to compute optimal policies, given a model of the environment in the form of an \ac{MDP}. Since \ac{DP} is expensive and requires finite \ac{MDPs}, its utility in solving \ac{RL} problems is limited. However \ac{DP} provides strong foundations for understanding the more advanced methods. \ac{DP} offers two algorithms that compute the optimal value functions: \emph{policy iteration} and \emph{value iteration}.

\paragraph{Policy Iteration:}
In policy iteration, two consecutive operations are performed on a policy $\pi$ in order to obtain a policy $\pi'$ that is better according to the ordering rule in~\eqref{eq:ordpol}. These operations are called \emph{policy evaluation} and \emph{policy improvement} and the goal of the algorithm is to obtain the optimal value function $V^{*}$ through the sequence:
\begin{align}
\pi_{0} \xrightarrow{\text{E}} V^{\pi_{0}} \xrightarrow{\text{I}} \pi_{1} \xrightarrow{\text{E}} V^{\pi_{1}} \xrightarrow{\text{I}} \pi_{2} \xrightarrow{\text{E}} ... \xrightarrow{\text{I}} \pi_{*} \xrightarrow{\text{E}} V^{*},
\end{align}
where $\pi_{i} \xrightarrow{\text{E}} V^{\pi_{i}}$ indicates that policy evaluation is performed on policy $\pi_{i}$ to calculate the value function $V^{\pi_{i}}$ and $V^{\pi_{i}} \xrightarrow{\text{I}} \pi_{i+1}$ indicates that policy improvement is performed from policy $\pi_{i}$ and its value function $V^{\pi_{i}}$ to obtain the policy $\pi_{i+1}$.\\
\newline
Policy evaluation consists in computing the value function $V^{\pi}$ for every state $s \in \Sspace$, according to the current policy $\pi$, by iteratively applying the following updating rule:
\begin{align} V_{k+1}(s) = \sum_{a}\pi(a|s) \Big( R(s,a) + \gamma \sum_{s'}P(s'|s,a) V_{k}(s') \Big), \label{eq:updrule} \end{align}
until $V_{k+1}(s) = V_{k}(s) \quad \forall s \in \Sspace$. When it happens, the convergence to $V^{\pi}$ is obtained. The existence and uniqueness of $V^{\pi}$ are guaranteed as long as either $\gamma<1$ or eventual termination is guaranteed from all states under the policy $\pi$ [\cite{sutton2018reinforcement}]. Formally policy evaluation converges in the limit of infinite iterations, in practice the algorithm stops when:
\begin{align}
	\max_{s \in \Sspace}|V_{k+1}(s) - V_{k}(s)| \leq \epsilon, \label{eq:stopcond}
\end{align}
where $\epsilon$ is a fixed threshold. The value function $V^{\pi}$ is approximated with the computed value function $V_{k+1}$.\\
\newline
Policy improvement consists in an update of policy $\pi$ in a new deterministic policy $\pi'$, according to the value function $V^{\pi}$ computed in the previous policy evaluation. For each state $s \in \Sspace$, indeed, the policy $\pi'$ is computed according to the following rule:
\begin{align} 
\pi'(s) = \arg \max_{a} \Big( R(s,a) + \gamma \sum_{s'} P(s'|s,a)  V^{\pi}(s') \Big). \label{eq:polimp}
\end{align}
By contruction $V^{\pi'}(s) \geq V^{\pi}(s) \quad \forall s \in \Sspace$, then the policy $\pi'$ must be as good as or better than $\pi$ [\cite{sutton2018reinforcement}].
When $\pi'(s) = \pi(s) \quad \forall s \in \Sspace$ or $\pi'$ is as good as $\pi$ (\ie $V^{\pi} = V^{\pi'}$), the algorithm terminates. The policy $\pi'$ is optimal, hence $\pi^{*} = \pi'$.\\
\newline
Because a finite \ac{MDP} has a finite number of policies, the entire process is ensured to converge to an optimal policy $\pi^{*}$ and optimal value function $V^{*}$ in a finite number of iterations.
\paragraph{Value Iteration:} \label{subsec:vi}
The main drawback in policy iteration is that, for each policy evaluation, multiple iterations of the updating rule (\ref{eq:updrule}) are required. If we truncate the policy evaluation after just one application of the updating rule, we obtain value iteration. In value iteration, the value function at each step is updated for all $s \in \Sspace$ as follows:
\begin{align} V_{k+1}(s) = \max_a \Big( R(s,a) + \gamma \sum_{s'}P(s'|s,a)  V_{k}(s') \Big). \end{align}
Differently from policy iteration, we don't compute any intermediate policy $\pi_i$. However, the sequence \{$V_{k}$\} of the value functions converges to $V^{*}$ since value iteration is a special case of policy iteration. The optimal policy $\pi^{*}$ is obtained applying~\eqref{eq:polimp}, with $V^{*}$ instead of $V^{\pi}$.\\
\newline
For computational reasons, value iteration is implemented with the \emph{stopping condition} showed in \myEq{eq:stopcond}. $V^{*}$ is approximated with the last value function computed in the truncated sequence \{$V_{k}$\}.

\subsection{Uncomplete Knowledge of \ac{MDPs}}
In many cases, however, it is not possible to apply \ac{DP}. Some alternatives are introduced below for sake of completeness. We don't detail these methods because they are not related to our work.  

\paragraph{Tabular Alternatives to DP:}\label{subsec:alt}
If we do not have a complete knowledge of the environment, we can learn the unknown dynamics from experience, \ie from the samples collected by the agent while interacting with the environment. The samples are tuples $\langle s,a,r,s' \rangle_t$, with $s, s' \in \Sspace, a \in \Aspace$ and $r = R(s,a)$. They contain information related to the agent-environment interaction at time step $t$. The two main classes of algorithms are \acf{MC} and \acf{TD}. \ac{MC} methods involve episodic tasks, \ac{TD} learning involves continuing tasks. 

\paragraph{Approximate Solution Methods:}
The methods presented so far are not suitable to solve \ac{RL} problems that involve a very large or infinite state space, such as the tasks in which a robot is free to move in a wide area and its state is represented by several (possibly continuous) dimensions. In these problems we cannot expect to obtain the optimal policy, and our goal is to find a good approximate solution using limited computational resources. In an environment with a continuous state space $\Sspace$, the agent will always visit states never seen before. Then, the experience we gather gives information on a subset of states but we need to generalize it to all the state space $\Sspace$. The generalization comes up in the form of \emph{function approximations}. These functions are estimated from samples and approximate value functions over continuous state space $\Sspace$ in place of using tables. Usually a function approximation is a parameterized function $f^{w}: \Sspace \to \mathbb{R}$, with parameter $w \in \mathbb{R}^{d}$.\\
\newline
Several methods rely on value function approximation in order to provide the solution of the \ac{RL} problem. However, function approximation brings new issues. First of all, convergence guarantees are difficult to obtain for \emph{greedy policies}, \ie policies for which the action to be performed in any state is the action having the highest value according to some value functions\footnote{For instance, $\pi(s) = \arg \max_{a \in \Aspace}Q(s,a)$, with $Q(s,a)$ a function approximation, is a greedy policy.}. If the policy is greedy, an arbitrary small change in the estimated value of an action can cause it to be, or not be, selected [\cite{Sutton1999PolicyGM}]. Furthermore, approximated value functions can introduce a bias that prevents the method to converge even to a local optimum [\cite{deisenroth2013Survey}] and increase the number of parameters to learn.

\section{Policy Search}\label{sec:ps}
In contrast with value-based methods, \acf{PS} methods use parametrized policies $\pi_{\vtheta}$, where $\vtheta \in \Theta$ and $\Theta$ is the parameter space. The set of parameter $\vtheta$ fixes in advance the candidate policies, \ie the policy class $\Pi_{\Theta} = \{ \pi_{\vtheta} | \vtheta \in \Theta \}$. \ac{PS} methods directly operate in the parameter space $\Theta$ (\ie in the set of candidate policies) to find the optimal parametrized policy and typically do not need to learn a value function. \ac{PS} copes with high dimensional state space $\Sspace$ and action space $\Aspace$ and offers better convergence guarantees compared to the methods that involve value-function approximation. \ac{PS} methods are more robust to noise because they are able to prevent a small variation in the state to produce a completely different action (this can happen in greedy policies, for instance). \ac{PS} methods can also incorporate domain knowledge in the policy definition (for instance, in the motor primitive policies defined in [\cite{Peters2008ReinforcementLO}]) and can be made safe by design. Because of these reasons, \ac{PS} methods have been found to be suitable in robotic applications.\\
\newline
Most of the algorithms in \ac{PS} are \emph{model-free} (\ie they don't require a model of the environment) because directly learning a policy is often easier than learning an accurate model. These methods update the policy directly exploiting the sampled trajectories, hence it is important to define an exploration strategy that provides variety in trajectories. Exploration can be performed both in the action space and in the parameter space. The former one can be implemented by adding a noise $\epsilon$ directly to the executed actions, the noise is generally sampled from a zero-mean Gaussian distribution. The latter consists in a perturbation of the parameter vector $\vtheta$ of $\pi_{\vtheta}$. The magnitude of noise present in any kind of exploration depends on some parameters. These parameters can also be updated by the algorithms: usually the size of exploration is gradually decreased to fine tune the policy parameters. [\cite{deisenroth2013Survey}].\\
\newline
The principal class of algorithms in \ac{PS} is composed by \acf{PG} methods. We detail this class in \mySec{sec:pg}.

\subsection{Policy Representations} \label{subsec:polrep}
In our work, we focus on the optimization of deterministic parametric policies of the form $\pi_{\vtheta}:\Sspace\to\Aspace$, with $\vtheta\in\Theta\subseteq\Reals^{m\times D}$. We will often abbreviate $\pi_{\vtheta}$ as $\vtheta$ in subscripts and function arguments, \eg $V^{\vtheta} \equiv V^{\pi_{\vtheta}}$, $J(\vtheta) \equiv J(\pi_{\vtheta})$. The simplest way of parametrizing $\pi_{\vtheta}$ is by means of a linear mapping. The linear policy is defined as:
\begin{align} \pi_{\vtheta}(s) = \vtheta^T\vphi(s), \end{align} where $\vtheta\in\Reals^{m\times D}$ and $\vphi:\Sspace\to\Reals^m$ is a feature function. This can be the state itself or, for instance, a set of \acf{RBF}. An example of \ac{RBF} is the Gaussian
\begin{align} \phi_i(s; \mu_i, \sigma_i) = \exp\left\{-{(s -\mu_i)^2}\big/{(2\sigma_i^2)}\right\}, \end{align}
where $\mu_i$ and $\sigma_i$ are hyperparameters of the feature function $\phi_i$, $i=1,\dots,m$. According to the parameters learnt by the algorithm, we can distinguish between:
\begin{itemize}
	\item \emph{Shallow policies: }the hyperparameters included in the feature functions are fixed throughout the algorithm, only the parameter $\vtheta$ from $\pi_{\vtheta}$ is learnt by the algorithm;
	\item \emph{Deep policies: }both the hyperparameters included in the feature functions and the parameter $\vtheta$ from $\pi_{\vtheta}$ are learnt by the algorithm. 
\end{itemize}
More complex policy parametrizations include deep neural networks [\cite{duan2016benchmarking}]. 
Stochastic policies randomize over actions. %In continuous settings, this is typically done by adding a Gaussian noise, \eg for a linear policy $a\sim\mathcal{N}(\vtheta^T\vphi(s),\Sigma)$, where $\Sigma\in\Reals^{D\times D}$ is a covariance matrix.

\paragraph{Softmax policies:}
A possible policy parameterization for \ac{PS} methods is the one that uses the \emph{softmax function}. The policies defined according to the softmax function are also called \emph{Gibbs policies}. This kind of policies is mostly used when the action set $\Aspace$ is discrete because, in order to define the probability $\pi(a|s)$, it is required to consider all the feasible actions in the state $s$:
\begin{align}
\pi_{\vtheta}(a|s) = \frac{\exp\Big(\vtheta^{T}\phi(s,a)\Big)}{\sum_{a_{k} \in \Aspace(s)}\exp\Big(\vtheta^{T}\phi(s,a_{k})\Big)},
\end{align}
where $\Aspace(s)$ is the set of actions that can be performed in state $s$ and $\phi(s,a)$ is a feature function depending both on state and action.

\paragraph{Normal Policies:}
A common parametrization for countinuous actions represented by real numbers, is the normal distribution. The policy can be defined as the normal probability density over a scalar action, with mean $\mu$ and standard deviation $\sigma$ given by parametric function approximators that depend on the state:
\begin{align} \pi_{\vtheta}(a|s) = \frac{1}{\sigma(s, \vtheta)\sqrt{2\pi}}\exp\Big( -\frac{(a -\mu(s, \vtheta))^2}{2\sigma(s, \vtheta)^2}\Big), \end{align}
where $\mu(s, \vtheta) = \vtheta_{\mu}^{T} \vphi(s)$ and $\sigma(s, \vtheta) = \exp\Big( \vtheta_{\sigma}^{T}\vphi(s) \Big)$. The policy's parameter vector is $\vtheta = [\vtheta_{\mu}, \vtheta_{\sigma}]^{T}$. This is only a possible parameterization for the standard deviation $\sigma$ of the normal distribution, in this configuration the exploration is said to be \emph{heteroscedastic} because the standard deviation changes according to the state. Usually the standard deviation $\sigma$ of the normal distribution is represented by a single parameter that does not depend on the state.

%\section{Special \NoCaseChange{\ac{MDPs}}} \label{sec:specmdp}
\section{Special Markov Decision Processes} \label{sec:specmdp}
In this section we present two special classes of \ac{MDPs} that allow us to represent some properties of interest for the \ac{MDPs} considered in this work.

\subsection{Bounded \ac{MDPs}} \label{subsec:bmdp}
A \acf{BMDP} [\cite{givan2000bounded}] is a five-tuple $\langle \Sspace, \Aspace, P_{\updownarrow}, R_{\updownarrow}, \gamma \rangle$, where $\Sspace$, $\Aspace$ and $\gamma$ are defined as for (finite) \ac{MDPs}, and $P_{\updownarrow}, R_{\updownarrow}$ are analogous to the \ac{MDP} transition and reward functions, but yield closed real intervals instead of real values: given a lower bound $\underline{P}$ and an upper bound $\overline{P}$, $P_{\updownarrow} = [\underline{P}; \overline{P}]$. Similarly we can specify the interval $R_{\updownarrow}$. This can be used to model uncertainty on the true nature of a decision process. To ensure that $P_{\updownarrow}$ admits only well-formed transition functions, we require that for any action $a$ and state $s$, the sum of the lower bounds of $P_{\updownarrow}(s'|s,a)$ over all states $s'$ must be less than or equal to one, while the upper bounds must sum to a value greater than or equal to one.\\
\newline
A \ac{BMDP} $M_{\updownarrow} = \langle \Sspace, \Aspace, P_{\updownarrow}, R_{\updownarrow}, \gamma \rangle$ defines a set of exact \ac{MDPs}. For any exact \ac{MDP} $M = \langle \Sspace', \Aspace', P', R', \gamma' \rangle$, we have $M \in M_{\updownarrow}$ if $\Sspace = \Sspace', \Aspace = \Aspace', \gamma = \gamma'$, and for any action $a$ and states $s, s', R'(s,a)$ belongs to the interval $R_{\updownarrow}(s,a)$ and $P'(s'|s,a)$ belongs to the interval $P_{\updownarrow}(s'|s,a)$. An interval value function $V_{\updownarrow}$ is a mapping from states to closed real intervals. We use such functions to indicate that the value of a given state for any exact \ac{MDP} falls within the selected interval. As in the case of (exact) value functions, interval value functions are specified \wrt a fixed policy $\pi$, \ie:
\begin{align} V_{\updownarrow}^{\pi}(s) = \Big[ \underline{V}^{\pi}(s), \overline{V}^{\pi}(s)\Big] = \Big[ \min_{M \in M_{\updownarrow}} V_{M}^{\pi}(s), \max_{M \in M_{\updownarrow}} V_{M}^{\pi}(s)\Big]. \end{align}
As shown in [\cite{givan2000bounded}], $M_{\updownarrow}$ includes both an \ac{MDP} that simultaneously achieves $\underline{V}^{\pi}(s)$ for all $s \in \Sspace$ and another one that achieves $\overline{V}^{\pi}(s)$ for all $s \in \Sspace$.\\
\newline
The notion of optimal value function in \ac{BMDP}s requires an ordering rule for intervals. We can define two different possible orderings:
\begin{align}
[l_1, u_1] \leq_{pes} [l_2, u_2] &\Leftrightarrow 
\begin{cases}
l_1 < l_2 \text{, or }\\
l_1 = l_2 \text{ and } u_1 \leq u_2
\end{cases} \label{eq:pes}\\
[l_1, u_1] \leq_{opt} [l_2, u_2] &\Leftrightarrow 
\begin{cases}
u_1 < u_2 \text{, or }\\
u_1 = u_2 \text{ and } l_1 \leq l_2
\end{cases} \label{eq:opt}
\end{align}
We use these orderings rules to partially order interval value functions in the following way:
\begin{align} V_{1 \updownarrow} \leq V_{2 \updownarrow} \iff V_{1 \updownarrow}(s) \leq_{*} V_{2 \updownarrow}(s) \quad \forall s \in \Sspace, \end{align}
with $\leq_{*}$ defined either as $\leq_{pes}$ in~\eqref{eq:pes} or $\leq_{opt}$ in~\eqref{eq:opt}.\\
\newline
As stated in [\cite{givan2000bounded}], there exists at least one optimistically and one pessimistically optimal policy:
\begin{align*}
V^{*}_{\updownarrow \text{opt}} = \max_{\pi \in \Pi} V_{\updownarrow}^{\pi} \text{ using} \leq_{opt} \text{ to order interval value functions,}\\
V^{*}_{\updownarrow \text{pes}} = \max_{\pi \in \Pi} V_{\updownarrow}^{\pi} \text{ using} \leq_{pes} \text{ to order interval value functions.}
\end{align*}
In order to better understand the meaning of the optimal policies, we consider a game in which we choose a policy $\pi$ and then a second player chooses an \ac{MDP} $M \in M_{\updownarrow}$ to evaluate the policy. $\overline{V}^{*}_{opt}$ is the best value function we can obtain if the second player cooperates in the game, $\underline{V}^{*}_{pes}$ is the best value function obtainable if the second player is an adversary.
\paragraph{Interval Value Iteration:}
In [\cite{givan2000bounded}] it is defined the \acf{IVI} algorithm that computes optimal value intervals, which is similar to the standard value iteration presented in \mySubsec{subsec:vi}. In \ac{IVI} the updating rule is:
\begin{align}
V_{\updownarrow, k+1}(s) = \max_{a \in \Aspace, \leq_{*}} \Big[ \min_{M \in M_{\updownarrow}} VI_{M, a}(\underline{V}_{k})(s), \max_{M \in M_{\updownarrow}} VI_{M, a}(\overline{V}_{k})(s) \Big], \label{eq:ivi}
\end{align}
where $\leq_{*}$ is $\leq_{pes}$ or $\leq_{opt}$. Given an \ac{MDP} $M$ with transition function $P$ and reward function $R$, an action $a \in \Aspace$ and a value function $v$, $VI_{M, a}(v)(s)$ is a single iteration of policy evaluation, where the action is fixed and 
\begin{align} 
VI_{M, a}(v)(s) = R(s,a) + \gamma \sum_{s' \in \Sspace} P(s'|s,a)v(s'). \label{eq:vi}
\end{align}
In~\eqref{eq:ivi} it is required to perform two iterations of VI. The two value functions used to perform the iterations (\ie the functions used instead of $v$ in~\eqref{eq:vi}) are obtained from the interval value function $V_{\updownarrow, k}$. For the first iteration, $v = \underline{V}_{k}$, with $\underline{V}_{k}$ being the lower bounds in $V_{\updownarrow, k}$; for the second one $v = \overline{V}_{k}$ with $\overline{V}_{k}$ being the upper bounds in $V_{\updownarrow, k}$.\\
\newline
Instead of searching in the set $M_{\updownarrow}$, the \ac{MDP} $M$ that minimizes (maximizes) the expression contained in $\min (\max)$ operator in~\eqref{eq:ivi} can be directly obtained by computing an exact transition function $P$ from the interval transition function $P_{\updownarrow}$ of $M_{\updownarrow}$. In order to do that, the arriving states $s' \in \Sspace$ are sorted in increasing (decreasing) order according to their value $\underline{V}(\overline{V})$. Then, for all the state-action pairs $(s,a) \in \Sspace \times \Aspace$ and given an ordering of arriving states $s'_1, s'_2, ..., s'_k$, we calculate the index $r$, with $1 \leq r \leq k$, that maximizes the following expression without letting it exceed 1:
\begin{align} \sum_{i=1}^{r-1}\overline{P}(s'_i|s,a) + \sum_{i=r}^{k}\underline{P}(s'_i|s,a). \end{align}
The exact transition function $P(\cdot|s,a)$ is defined by assigning the upper bound $\overline{P}(s'|s,a)$ to the transition probabilities involving states $s'$ with an index lower than $r$, the lower bound $\underline{P}(s'|s,a)$ to the transition probabilities involving states $s'$ with an index greater than $r$ and the probability that ensures $\sum_{s' \in \Sspace}P(s'|s,a) = 1$ to the state with index $r$.\\
\newline
According to [\cite{givan2000bounded}], the \ac{IVI} algorithm is able to converge to $V^{*}_{\updownarrow \text{opt}}$ or $V^{*}_{\updownarrow \text{pes}}$ in a polynomial number of iterations, where polynomial is relative to the problem size.

\subsection{Lipschitz \ac{MDPs}} \label{sub:lipmdp}
We introduce the notion of \emph{Lipschitz continuity} in order to define some properties of regularity in the \ac{MDP}. These properties can be exploited in policy gradient algorithms, for instance in [\cite{pirotta2015policy}] it is shown how they can ensure a performance improvement at each iteration of policy-parameter updates. In this section we provide basic concepts related to Lipschitz continuity and useful bounds that will be exploited in our work.
\begin{definition}[Lipschitz continuity]
Given two metric spaces $(X, d_X)$ and $(Y, d_Y)$ where $d_X$ and $d_Y$ denote the corresponding metric functions, a function $f: X \rightarrow Y$ is called $L_f$-\acf{LC} if
\begin{align} \forall(x_1, x_2) \in X^2, d_Y(f(x_1), f(x_2)) \leq L_f d_X(x_1, x_2). \label{eq:lip} \end{align}
\end{definition}
\noindent The smallest $L_f$ for which~\eqref{eq:lip} holds is called \emph{Lipschitz constant} of $f$. For real-valued functions (\eg the reward function $R$), we use the Euclidean distance as metric for the codomain distance. The same metric can be used to measure the distance between states and actions, as long as they can be represented in a numeric (possibly multidimensional) format. For the transition function $P$ and the stochastic policies $\pi$ we need to introduce a distance between probability distributions. We consider the Kantorovich or $L^1$-Wasserstein metric on probability measures, defined as follows:
\begin{definition}[Kantorovich metric]
Given two probability measures $p$ and $q$, the Kantorvich measure $\Kant(p, q)$ is:
\begin{align} \Kant(p, q) = \sup_f\Big\{ \Big| \int_x f(x) d\Big(p(x)-q(x)\Big) \de x \Big|: L_f \leq 1 \Big\}. \label{eq:kant}\end{align}
\end{definition}
\noindent In the case of deterministic policies, the probability distributions $p$ and $q$ are Dirac delta functions. The Kantorovich distance between two Dirac delta functions is equal to the distance of their locations. As an alternative to the Kantorovich metric, we can consider the \acf{TV} distance, defined as:
\begin{definition}[Total Variation distance]
	Given two probability measures $p$ and $q$, the \ac{TV} distance $\mathop{TV}(p, q)$ is:
	\begin{align} \mathop{TV}(p, q) = \frac{1}{2} \int_x \big|p(x)-q(x)\big| \de x. \label{eq:tv}\end{align}
\end{definition}
\noindent This metric is more demanding than Kantorovich metric: \ac{MDPs} that are Lipchitz according to \ac{TV} are also Lipschitz according to the Kantorovich metric but not vice versa.\\
\newline
In this work, it is also important to consider the \emph{Hausdorff metric} to measure the distance of two subsets of a metric space. We use it in the abstract state space since each abstract state $X$ is a subset of the state space $\Sspace$.
\begin{definition}[Hausdorff metric]
	Let $X$ and $Y$ be two non-empy subsets of a metric space $(M,d)$. The Hausdorff distance $d_{H}(X,Y)$ is defined as:
	\begin{align}
		d_{H}(X,Y) = \max \Big\{\sup_{x \in X}\inf_{y \in Y}d(x, y), \sup_{y \in Y}\inf_{x \in X}d(x,y)\Big\}.
	\end{align}
\end{definition}
\noindent In this work, we restrict our attention to Lipschitz-continuous \ac{MDPs} and policies. We make similar assumptions as in [\cite{pirotta2015policy}]. For the \ac{MDP}, we require both the continuity of the transition model and the reward:
%
\begin{assumption}[Lipschitz MDP]\label{ass:lipmdp}
	For all $s,\wt{s}\in\Sspace$ and $a,\wt{a}\in\Aspace$:
	\begin{align}
	&\Kant\left(P(\cdot|s,a), P(\cdot| \wt{s},\wt{a})\right) \leq L_{P}d_{\Sspace\Aspace}\left((s,a),(\wt{s},\wt{a})\right), \label{eq:liptf}\\
	&\left|R(s,a) - R(\wt{s},\wt{a})\right| \leq L_{R}d_{\Sspace\Aspace}\left((s,a),(\wt{s},\wt{a})\right),
	\end{align}
	for some positive real constants $L_{P}$ and $L_{R}$.
\end{assumption}
%
\noindent where $d_{\Sspace\Aspace}\left((s,a),(\wt{s},\wt{a})\right) = \norm{s-\wt{s}} + \norm{a-\wt{a}}$ is the taxicab norm on $\Sspace\times\Aspace$.
We also require our policy to be continuous both \wrt the input state and its parameters:
%
\begin{assumption}[Lipschitz Policies]\label{ass:lippol}
	For all $s,\wt{s}\in\Sspace$ and $\vtheta,\wt{\vtheta}\in\Theta$:
	\begin{align}
	&\Kant\Big(\pi_{\vtheta}(\cdot|s),\pi_{\vtheta}(\cdot|\wt{s})\Big) \leq L_{\pi_{\vtheta}}\norm{s-\wt{s}}, \label{eq:lp1}\\
	&\Kant\Big(\pi_{\vtheta}(\cdot|s), \pi_{\wt\vtheta}(\cdot|s)\Big) \leq L_{\Theta}\norm{\vtheta-\wt{\vtheta}}, \label{eq:lp2}
	\end{align}
	for some positive real constants $\{L_{\pi_{\vtheta}}\}_{\vtheta\in\Theta}$ and $L_{\Theta}$. In case $\pi_{\vtheta}$ is a deterministic policy, (\ref{eq:lp1}) and (\ref{eq:lp2}) are replaced with:
	\begin{align}
	&\norm{\pi_{\vtheta}(s) - \pi_{\vtheta}(\wt{s})} \leq L_{\pi_{\vtheta}}\norm{s-\wt{s}}, \\
	&\norm{\pi_{\vtheta}(s) - \pi_{\wt\vtheta}(s)} \leq L_{\Theta}\norm{\vtheta-\wt{\vtheta}}.
	\end{align}
\end{assumption}
%
\noindent We use the Euclidean norm to measure distances on $\Sspace$, $\Aspace$ and $\Theta$, but everything works for general metrics.
In the following, we will always assume that ${L_{P}(1+L_{\pi_{\vtheta}}) < \gamma^{-1}}$.
These assumptions are enough to guarantee the Lipschitz continuity of the value functions \wrt states and actions:
%
\begin{lemma}[\cite{rachelson2010locality}]\label{lem:lipval}
	Under Assumptions~\ref{ass:lipmdp} and~\ref{ass:lippol}, for all $s,\wt{s}\in\Sspace$, $a,\wt{a}\in\Aspace$ and $\vtheta\in\Theta$:
	\begin{align}
	&\left|V^{\vtheta}(s) - V^{\vtheta}(\wt{s})\right| \leq L_{V^{\vtheta}}\norm{s-\wt{s}}, \\
	&\left|Q^{\vtheta}(s,a) - Q^{\vtheta}(\wt{s},\wt{a})\right| \leq L_{Q^{\vtheta}}d_{\Sspace\Aspace}\left((s,a),(\wt{s},\wt{a})\right),
	\end{align}
	where $L_{Q^{\vtheta}} = \frac{L_{R}}{1-\gamma L_{P}(1+L_{\pi_{\vtheta}})}$ and $L_{V^{\vtheta}} = L_{Q^{\vtheta}}(1+L_{\pi_{\vtheta}})$,
\end{lemma}
%
\noindent and also of the future-state distributions \wrt policy parameters:
%
\begin{lemma}[\cite{pirotta2015policy}]\label{lem:lipfut}
	Under Assumptions~\ref{ass:lipmdp} and~\ref{ass:lippol}, for all $\vtheta,\wt\vtheta\in\Theta$:
	\begin{align}
	&\Kant\left(\delta^{\vtheta},\delta^{\wt{\vtheta}}\right) \leq L_{\delta^{\vtheta}}\norm{\vtheta-\wt{\vtheta}},
	\end{align}
	where $L_{\delta^{\vtheta}} = {\gamma L_{P}L_{\pi_{\vtheta}}}\big/{\left(1-\gamma L_{P}(1+L_{\pi_{\vtheta}})\right)}$.
\end{lemma}

\section{State abstraction} \label{sec:stdisc}

An abstraction is a mapping from one problem representation to a new simpler representation that preserves some properties of interest. State abstractions [\cite{lihong2006towards}] map \ac{MDPs} to \ac{MDPs} with simpler state spaces, typically for computational purposes. Let $ M = \langle \Sspace, \Aspace, P, R, \gamma \rangle$ be the ground \ac{MDP} and its abstract version be $\wt{M} = \langle \Xspace, \Aspace, \widetilde{P}, \widetilde{R}, \gamma \rangle$. We define the abstraction function as $\Gamma: \Sspace \to \Xspace$, so that $\Gamma(s)\in\Xspace$ is the abstract state corresponding to the ground state $s$ and $\Gamma^{-1}(X)\subseteq\Sspace$ denotes the inverse image of $X$ under $\Gamma$. $\widetilde{P}$ is the abstract transition function, with $\widetilde{P}(X'|X,a)$ denoting the probability of reaching $X'$ from $X$ by taking action $a$, and $\widetilde{R}$ is the reward function with $\widetilde{R}(X,a)$ denoting the expected reward from taking action $a$ in abstract state $X$.\\
\newline
To guarantee $\widetilde{P}$ and $\widetilde{R}$ are well-defined, we can use a weighting function  $w: \Sspace \to [0, 1]$ such that $\int_{\Gamma^{-1}(X)} w(s)\de s = 1$ for all $X \in \Xspace$. With these weights at hand, we can define the transition and reward functions of the abstract \ac{MDP} as follows:
%
\begin{align}
&\widetilde{R}(X,a) = \int_{\Gamma^{-1}(X)}w(s)R(s,a)\de s,\nonumber \\
&\widetilde{P}(X'|X,a) = \int_{\Gamma^{-1}(X)}w(s)\int_{\Gamma^{-1}(X')} P(s'|s,a)\de s\de s'.\nonumber
\end{align}
%
These definitions may be not enough to include in the abstraction definition the notion of state similarity because they are valid regardless of the abstraction function $\Gamma$ and the weighting function $w$ considered. Another drawback is that the abstract states $X \in \Xspace$ are often imperfect because representing the current environment in terms of abstract states necessarily neglects information. However, finding solution in the abstract state space $\Xspace$ is typically faster than in the ground state space $\Sspace$ because groups of states are treated as a unit. State abstraction allows to apply learning techniques in large, real-world environments. A central issue in the theory of abstraction is to distinguish between relevant and irrelevant information, in order to use the former to outline how the state abstraction is performed and discard the latter.\\
\newline
Several state abstractions schemes have been proposed, all of them aggregating the ground states $s$ that are similar according to some measure into the same abstract state $X$. For instance, in [\cite{ferns2012metrics}] a measure of similarity $d: \Sspace \times \Sspace \to \mathbb{R}$ between states is defined as follows:
\begin{align}
d(s, \wt{s}) = \max_{a \in \Aspace} \Big( c_{R} d_{R}(R(s,a), R(\wt{s}, a)) + c_{P} d_{P}(P(\cdot|s, a), P(\cdot|\wt{s}, a)) \Big), \label{eq:dismet}
\end{align}
where $c_{R}$ and $c_{P}$ are two constants such that $c_{R} + c_{P} = 1$, $d_{R}$ is a distance measure (usually the Euclidean distance) and $d_{P}$ is a distance measure for probability distributions (for instance, the Kantorovich distance defined in (\ref{eq:kant})). We can choose some seed states and cluster all the remaining states according to the distance metric in (\ref{eq:dismet}).\\
\newline 
In this work, we focus on \textit{state aggregation} where $\Xspace$ is a partition of $\Sspace$ and $s\in\Gamma(s)$. Hence, we will often identify $\Gamma^{-1}(X)$ with $X$ (as a set) itself. When it is possible, state abstraction focuses on the preservation of those properties that allow an agent to perform optimal behaviours.
\subsection{Irrelevance Abstractions}
In [\cite{lihong2006towards}], five different types of abstractions are presented, all of which preserve some information that is critical for solving the original \ac{MDP}. They can be ordered from the method that prescribes the finest abstraction to the method that prescribes the coarsest one. Here we report the five types of abstraction, called \emph{irrelevance abstractions}:
\begin{itemize}
	\item A \emph{model-irrelevance} abstraction $\Gamma_{model}$ is such that for any action $a$ and ay avstract state $X$, $\Gamma_{model}(s_1) = \Gamma_{model}(s_2)$ implies $R(s_1,a)=R(s_2,a)$ and $\int_{X'}P(s'|s_1,a) \de s'= \int_{X'}P(s'|s_2,a) \de s'$.
	\item A $Q^{\pi}$-irrelevance abstraction $\Gamma_{Q^{\pi}}$ is such that for any policy $\pi$ and any action $a$, $\Gamma_{Q^{\pi}}(s_1) = \Gamma_{Q^{\pi}}(s_2)$ implies $Q^{\pi}(s_1,a)=Q^{\pi}(s_2,a)$.
	\item A $Q^{*}$-irrelevance abstraction $\Gamma_{Q^{*}}$ is such that for any action $a$, $\Gamma_{Q^{*}}(s_1) = \Gamma_{Q^{*}}(s_2)$ implies $Q^{*}(s_1,a)=Q^{*}(s_2,a)$.
	\item An $a^{*}$-irrelevance abstraction $\Gamma_{a^{*}}$ is such that every abstract state $X$ has an action $a^{*}$ that is optimal for all the states $s \in X$, and $\Gamma_{a^{*}}(s_1) = \Gamma_{a^{*}}(s_2)$ implies that $Q^{*}(s_1,a^{*})=Q^{*}(s_2,a^{*})$.
	\item A $\pi^{*}$-irrelevance abstraction $\Gamma_{\pi^{*}}$ is such that every abstract state $X$ has an action $a^{*}$ that is optimal for all the states $s \in X$, and $\Gamma_{\pi^{*}}(s_1) = \Gamma_{\pi^{*}}(s_2)$ implies that $Q^{*}(s_1,a^{*})=\max_{a}Q^{*}(s_1,a)$ and $Q^{*}(s_2,a^{*})=\max_{a}Q^{*}(s_2,a)$.
\end{itemize}
\noindent However, even the coarsest irrelevance abstraction is impossible to be implemented in most of the \ac{MDPs}, obviously if we leave out the naive state abstraction in which the set of abstract states $\Xspace$ corresponds to the set of ground states $\Sspace$.\\
\newline
$\Gamma_{model}$ is a special case of state abstraction, called \emph{bisimulation}, where states with the same transition and reward functions are aggregated. The rule for aggregation is very strict and not applicable in most of the cases. Because of this, in approximated bisimulation the requirements of exact equivalence for the transition and reward functions are replaced with bounds that allow to aggregate in the same abstract state $X$ all the states $s$ for which the difference in the functions is below a certain value. From these bounds, it is possible to build a \ac{BMDP} and solve it with the method discussed in \mySubsec{subsec:bmdp} [\cite{dean2013model}].\\
\newline
The policies obtained by solving the abstract \ac{MDP} are a (possibly stochastic) mapping from abstract states to actions. Given an abstract policy $\rho$, it can be translated in the ground state space $\Sspace$ as: 
\begin{align}
	\pi(a|s) = \rho(a|\Gamma(s)) \label{eq:translation}
\end{align}
The intuitive explanation for \myEq{eq:translation} is that the action prescribed by $\rho$ in the abstract state $X$ is copied into all the ground states $s \in X$. However, this translation does not ensure to preserve the performance of $\rho$ in $\wt{M}$ for $\pi$ in $M$.\\
\newline
Coarser abstraction methods provide a better computational efficiency and generalization \wrt finer abstractions but have looser performance loss bounds. Indeed, in [\cite{lihong2006towards}] it is stated that in $\Gamma_{model}$, $\Gamma_{Q^{\pi}}$, $\Gamma_Q^{*}$ and $\Gamma_{a^{*}}$ abstractions, the optimal abstract policy $\rho^{*}$ is optimal also in the ground \ac{MDP}, while it can be suboptimal in $\Gamma_{\pi^{*}}$ abstractions. The optimal policy in the ground \ac{MDP} could require information that is not available in the abstract \ac{MDP}. For instance, when the agent is in the abstract state $X$, it does not know the exact ground state $s \in X$ in which it is. The agent in the abstract space $\Xspace$ can only partially observe the ground states $s$. A non-markovian abstract solution that keeps track of the entire agent's history may address the problem of finding an optimal policy in both the abstract and the ground state spaces, however it is more complicated to learn this kind of solution.

