%*****************************************************************
% Breve riassunto in italiano della tesi da cui si capisca tutto
% ****************************************************************
\newcommand{\estrattoname}{Estratto}
\addcontentsline{toc}{chapter}{\estrattoname}

\pdfbookmark[1]{Estratto}{Estratto}
\begingroup
\let\clearpage\relax
\let\cleardoublepage\relax
\let\cleardoublepage\relax

\chapter*{Estratto}
\sloppy L'apprendimento per rinforzo è un insieme di tecniche di apprendimento automatico che permettono a un agente autonomo che interagisce con un ambiente di imparare il miglior comportamento possibile, valutato rispetto al riscontro che l'agente riceve dall'ambiente. Nell'apprendimento per rinforzo, gli algoritmi che permettono di ottenere il miglior comportamento possibile solitamente richiedono all'agente di eseguire delle azioni casuali per facilitare la risoluzione del problema garantendo una sufficiente esplorazione delle possibili situazioni in cui l'agente si può trovare. Far eseguire azioni casuali all'agente permette di valutare un vasto numero di azioni, che altrimenti verrebbero trascurate dall'algoritmo di apprendimento. Tuttavia, questa tecnica può essere considerata inaccettabile in applicazioni reali dell'apprendimento per rinforzo, ad esempio nel campo industriale dove la sicurezza è un requisito importante ed è consigliabile evitare qualsiasi variazione dal comportamento usuale dell'agente. Ci sono molte possibilità, non esclusive tra loro, per definire il concetto di sicurezza in un problema di apprendimento per rinforzo. Per questo motivo, la letteratura propone numerose tecniche che affrontano la questione della sicurezza con approcci tra loro diversi. Noi consideriamo uno scenario in cui l'agente interagisce con un ambiente del mondo reale e deve imparare un comportamento ottimale senza poter effettuare alcuna azione casuale. Questa limitazione permette all'agente di esplorare l'ambiente in sicurezza durante l'apprendimento del comportamento ottimale. Per la prima volta, al meglio della nostra conoscenza, viene proposto un algoritmo di ottimizzazione del comportamento dell'agente in ambiente continuo che esegue solamente azioni deterministiche. Questo algoritmo richiede che siano valide alcune assunzioni sulla regolarità dell'ambiente, necessarie per avere una stima delle situazioni non più osservabili in mancanza di azioni casuali. Nelle applicazioni per cui l'algoritmo è stato pensato, consideriamo realistiche queste assunzioni. In questo algoritmo, inoltre, viene utilizzata la tecnica dell'aggregazione degli stati per costruire un modello astratto dell'ambiente e sfruttare una forma di esplorazione passiva, necessaria per migliorare il comportamento dell'agente. Abbiamo testato il metodo proposto in simulazioni di problemi di controllo continuo, nei casi di totale assenza di informazioni riguardo al problema e disponibilità di una conoscenza preliminare. Nei problemi considerati, affrontiamo un crescente livello di difficoltà dell'ambiente e della rappresentazione dello stato dell'agente: a partire da un ambiente deterministico monodimensionale, consideriamo prima un ambiente stocastico bidimensionale e poi un ambiente deterministico a nove dimensioni, rappresentato dai sensori con cui l'agente è equipaggiato. Negli esperimenti effettuati, nonostante i limiti imposti sull'esplorazione per ragioni di sicurezza, l'agente impara un comportamento ottimale che è equiparabile a quello appreso con altri algoritmi esistenti, dove la questione della sicurezza non è considerata. Questo documento contiene un'introduzione al lavoro svolto che presenta le motivazioni alla base di esso, una parte teorica relativa all'apprendimento per rinforzo e allo stato dell'arte, la presentazione dell'algoritmo, la sua giustificazione teorica e la descrizione degli esperimenti con cui l'algoritmo è stato testato. I risultati ottenuti dagli esperimenti sono promettenti e incoraggiano uno sviluppo futuro delle tecniche presentate in questo lavoro.

\endgroup

