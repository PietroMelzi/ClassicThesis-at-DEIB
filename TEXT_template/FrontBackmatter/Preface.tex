
\addcontentsline{toc}{chapter}{\prefacename}
\pdfbookmark[1]{Preface}{Preface}

\chapter*{Preface}
%rl
One of the challenges that arise in \acf{RL} is the trade-off between exploration and exploitation. To obtain an high reward, an agent must perform actions that it has found to be effective in producing reward. But to discover such actions, it has to try actions that it has not selected before. An intuitive approach to the issue is to perform random actions in the beginning of the task, so as to test as many actions as possible in a first phase and then exploit the most rewarding actions in the long term. By executing random actions in any state, the agent potentially executes and evaluates the entire range of feasible behaviours. An approach of this kind strongly supports exploration and provides future advantages to the agent.\\
\newline
%problem
If the agent evolves into a simulator there are no concerns with this method, however when the agent interacts with a real-life environment serious drawbacks come up. The lack of command on the agent's behaviour can lead to dangerous actions that damage the agent's hardware or, even worse, harm the humans that operate in the environment. Because of this safety issue, \ac{RL} techniques are scarcely used in fields where they could provide outstanding benefits as industrial robotics, surgery (\cite{baek2018PathPlanning}) or autonomous driving. Again, another important scenario in which the randomness of actions is unwanted is finance: certainly nobody wants to perform stochastic operations involving their money. In \ac{RL} several definitions of safety have been proposed (we discuss them in \mySec{sec:saferl}), however the safety problem due to randomness of actions is poorly addressed. The reason is that in \ac{RL} the stochasticity is essential for exploration. Indeed, if the agent always does the same action in every state, it cannot find any better action that improves its behaviour.\\
\newline
%solution
Driven by these motivations, we explore a procedure that ensure safeness in every instant of the agent-environment interaction by monitoring the explorative behaviour of the agent. The solution we propose is suitable for regular environments, \ie environments in which performing the same action in similar states produces similar effects. In these environments, once an action is executed in a certain state, we can evaluate its effect in similar states, without redoing the action. This assumption provides a sort of exploration that the agent can exploit instead of relying on stochasticity. As a result, in any state the agent performs a deterministic action and evaluates only the set of actions executed in similar states. Because of this, the learning ability of the agent is significantly reduced in this approach: learning an optimal behaviour becomes infeasible or requires an excessive amount of time. On the other hand, the agent is able to improve his behaviour without the need to perfor random actions.\\
\newline
%conclusion
We present our work... \todo{descrizione delle sezioni}
