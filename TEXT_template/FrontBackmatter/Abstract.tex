%*******************************************************
% Abstract
%*******************************************************
%\renewcommand{\abstractname}{Abstract}
\addcontentsline{toc}{chapter}{\abstractname}

\pdfbookmark[1]{Abstract}{Abstract}
\begingroup
\let\clearpage\relax
\let\cleardoublepage\relax
\let\cleardoublepage\relax

\chapter*{Abstract}
In reinforcement learning, policy optimization algorithms normally rely on action randomization to make the learning problem easier and to guarantee a sufficient exploration of all the possible situations in the task. Action randomization allows to execute and evaluate a wide range of actions that otherwise may be neglected by the algorithm. However, this practice may be unacceptable in real-life applications, such as industrial ones, where safety is a concern and deviations from usual behavior are not welcome by stakeholders. There exist multiple and not exclusive definitions of safety in reinforcement learning, hence safety aspects can be modeled and incorporated in the tasks in different ways. We consider the challenging scenario in which a learning agent is deployed in the real world and must be able to improve on-line without performing any random action, to ensure safe exploration throughout the learning process. For the first time, to the best of our knowledge, we propose a truly deterministic policy optimization algorithm for continuous domains. To design this algorithm, we require the validity of some assumptions on the regularity of the environment, which we deem easy to satisfy in the scenarios of interest. We also use state aggregation to build an abstract model of the environment and exploit passive exploration, necessary to allow successful policy optimization. The proposed approach is tested on simulated continuous control tasks, both in the case of learning from scratch and in the case of having some prior knowledge of the problem. The results obtained from the experiments are promising and encourage the future development of the techniques presented in this work.

\vfill
\newpage
\pdfbookmark[1]{Sommario}{Sommario}
\chapter*{Estratto}
\sloppy L'apprendimento per rinforzo è un insieme di tecniche di apprendimento automatico che permettono a un agente autonomo che interagisce con un ambiente di imparare il miglior comportamento possibile, valutato rispetto al riscontro che l'agente riceve dall'ambiente. Nell'apprendimento per rinforzo, gli algoritmi che permettono di ottenere il miglior comportamento possibile solitamente richiedono all'agente di eseguire delle azioni casuali per facilitare la risoluzione del problema garantendo una sufficiente esplorazione delle possibili situazioni in cui l'agente si può trovare. Far eseguire azioni casuali all'agente permette di valutare un vasto numero di azioni, che altrimenti verrebbero trascurate dall'algoritmo di apprendimento. Tuttavia, questa tecnica può essere considerata inaccettabile in applicazioni reali dell'apprendimento per rinforzo, ad esempio nel campo industriale dove la sicurezza è un requisito importante ed è consigliabile evitare qualsiasi variazione dal comportamento usuale dell'agente. Ci sono molte possibilità, non esclusive tra loro, per definire il concetto di sicurezza in un problema di apprendimento per rinforzo. Per questo motivo, la letteratura propone numerose tecniche che affrontano la questione della sicurezza con approcci tra loro diversi. Noi consideriamo uno scenario in cui l'agente interagisce con un ambiente del mondo reale e deve imparare un comportamento ottimale senza poter effettuare alcuna azione casuale. Questa limitazione permette all'agente di esplorare l'ambiente in sicurezza durante l'apprendimento del comportamento ottimale. Per la prima volta, al meglio della nostra conoscenza, viene proposto un algoritmo di ottimizzazione del comportamento dell'agente in ambiente continuo che esegue solamente azioni deterministiche. Questo algoritmo richiede che siano valide alcune assunzioni sulla regolarità dell'ambiente, necessarie per avere una stima delle situazioni non più osservabili in mancanza di azioni casuali. Nelle applicazioni per cui l'algoritmo è stato pensato, consideriamo realistiche queste assunzioni. In questo algoritmo, inoltre, viene utilizzata la tecnica dell'aggregazione degli stati per costruire un modello astratto dell'ambiente e sfruttare una forma di esplorazione passiva, necessaria per migliorare il comportamento dell'agente. Abbiamo testato il metodo proposto in simulazioni di problemi di controllo continuo, nei casi di totale assenza di informazioni riguardo al problema e disponibilità di una conoscenza preliminare. Nei problemi considerati, affrontiamo un crescente livello di difficoltà dell'ambiente e della rappresentazione dello stato dell'agente: a partire da un ambiente deterministico monodimensionale, consideriamo prima un ambiente stocastico bidimensionale e poi un ambiente deterministico a nove dimensioni, rappresentato dai sensori con cui l'agente è equipaggiato. Negli esperimenti effettuati, nonostante i limiti imposti sull'esplorazione per ragioni di sicurezza, l'agente impara un comportamento ottimale che è equiparabile a quello appreso con altri algoritmi esistenti, dove la questione della sicurezza non è considerata. Questo documento contiene un'introduzione al lavoro svolto che presenta le motivazioni alla base di esso, una parte teorica relativa all'apprendimento per rinforzo e allo stato dell'arte, la presentazione dell'algoritmo, la sua giustificazione teorica e la descrizione degli esperimenti con cui l'algoritmo è stato testato. I risultati ottenuti dagli esperimenti sono promettenti e incoraggiano uno sviluppo futuro delle tecniche presentate in questo lavoro.

\endgroup