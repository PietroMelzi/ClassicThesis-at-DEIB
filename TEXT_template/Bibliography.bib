@BOOK{bringhurst:2002,
  title = {{T}he {E}lements of {T}ypographic {S}tyle},
  publisher = {Hartley \& Marks Publishers},
  year = {2008},
  author = {Bringhurst, R. },
  series = {Version 3.2},
  address = {Point Roberts, WA, USA}
}

@ARTICLE{knuth:1974,
  author = {Knuth, D. E.},
  title = {{C}omputer {P}rogramming as an {A}rt},
  journal = {Communications of the ACM},
  year = {1974},
  volume = {17},
  pages = {667--673},
  number = {12},
  address = {New York, NY, USA},
  publisher = {ACM Press}
}

@BOOK{verne_journey:1957,
  title={Journey to the Center of the Earth},
  author={Verne, J.},
  isbn={9780758311993},
  series={Classics illustrated},
  year={1957},
  publisher={Huge Print Press}
}

@book{sutton2018reinforcement,
	title={Reinforcement learning: An introduction},
	author={Sutton, Richard S and Barto, Andrew G},
	year={2018},
	publisher={MIT press}
}

@book{puterman2014markov,
	title={Markov Decision Processes.: Discrete Stochastic Dynamic Programming},
	author={Puterman, Martin L},
	year={2014},
	publisher={John Wiley \& Sons}
}

@article{givan2000bounded,
	author    = {Robert Givan and
	Sonia M. Leach and
	Thomas L. Dean},
	title     = {Bounded-parameter Markov decision processes},
	journal   = {Artif. Intell.},
	volume    = {122},
	number    = {1-2},
	pages     = {71--109},
	year      = {2000}
}

@article{pirotta2015policy,
	author    = {Matteo Pirotta and
	Marcello Restelli and
	Luca Bascetta},
	title     = {Policy gradient in Lipschitz Markov Decision Processes},
	journal   = {Machine Learning},
	volume    = {100},
	number    = {2-3},
	pages     = {255--283},
	year      = {2015}
}

@inproceedings{rachelson2010locality,
	author    = {Emmanuel Rachelson and
	Michail G. Lagoudakis},
	title     = {On the locality of action domination in sequential decision making},
	booktitle = {{ISAIM}},
	year      = {2010}
}

@inproceedings{duan2016benchmarking,
	author    = {Yan Duan and
	Xi Chen and
	Rein Houthooft and
	John Schulman and
	Pieter Abbeel},
	title     = {Benchmarking Deep Reinforcement Learning for Continuous Control},
	booktitle = {{ICML}},
	series    = {{JMLR} Workshop and Conference Proceedings},
	volume    = {48},
	pages     = {1329--1338},
	publisher = {JMLR.org},
	year      = {2016}
}

@inproceedings{Sutton1999PolicyGM,
  title={Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  author={Richard S. Sutton and David A. McAllester and Satinder P. Singh and Yishay Mansour},
  booktitle={NIPS},
  year={1999}
}

@article{Williams1992SimpleSG,
  title={Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  author={Ronald J. Williams},
  journal={Machine Learning},
  year={1992},
  volume={8},
  pages={229-256}
}

@article{article,
author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
year = {2014},
month = {06},
pages = {},
title = {Deterministic Policy Gradient Algorithms},
volume = {1},
journal = {31st International Conference on Machine Learning, ICML 2014}
}

@inproceedings{sehnke2008PolicyGradient,
author = {Sehnke, Frank and Osendorfer, Christian and Rückstieß, Thomas and Graves, Alex and Peters, Jan and Schmidhuber, Jürgen},
year = {2008},
month = {09},
pages = {387-396},
title = {Policy Gradients with Parameter-Based Exploration for Control},
journal = {Artificial Neural Networks: ICANN 2008, 387-396 (2008)},
doi = {10.1007/978-3-540-87536-9_40}
}

@inproceedings{baek2018PathPlanning,
author = {Baek, Donghoon and Hwang, Minho and Kim, Hansoul and Kwon, Dong-Soo},
year = {2018},
month = {06},
pages = {342-347},
title = {Path Planning for Automation of Surgery Robot based on Probabilistic Roadmap and Reinforcement Learning},
doi = {10.1109/URAI.2018.8441801}
}

@book{deisenroth2013Survey,
author = {Deisenroth, Marc and Neumann, Gerhard and Peters, Jan},
year = {2013},
month = {08},
pages = {},
title = {A Survey on Policy Search for Robotics},
volume = {2}
}

@article{Peters2008ReinforcementLO,
  title={Reinforcement learning of motor skills with policy gradients},
  author={Jan Peters and Stefan Schaal},
  journal={Neural networks : the official journal of the International Neural Network Society},
  year={2008},
  volume={21 4},
  pages={
          682-97
        }
}

@article{garcia2015,
author = {García, J. and Fernández, F.},
year = {2015},
month = {08},
pages = {1437-1480},
title = {A comprehensive survey on safe reinforcement learning},
volume = {16}
}

@inproceedings{turchetta2016,
author = {Turchetta, Matteo and Berkenkamp, Felix and Krause, Andreas},
title = {Safe Exploration in Finite Markov Decision Processes with Gaussian Processes},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {4312–4320},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS’16}
}

@article{DBLP:journals/corr/abs-1905-11041,
  author    = {Chuheng Zhang and
               Yuanqi Li and
               Jian Li},
  title     = {Policy Search by Target Distribution Learning for Continuous Control},
  journal   = {CoRR},
  volume    = {abs/1905.11041},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.11041},
  archivePrefix = {arXiv},
  eprint    = {1905.11041},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-11041.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@unknown{papini2019,
author = {Papini, Matteo and Pirotta, Matteo and Restelli, Marcello},
year = {2019},
month = {05},
pages = {},
title = {Smoothing Policies and Safe Policy Gradients}
}

@inproceedings{Li2006TowardsAU,
  title={Towards a Unified Theory of State Abstraction for MDPs},
  author={Lihong Li and Thomas J. Walsh and Michael L. Littman},
  booktitle={ISAIM},
  year={2006}
}

@inproceedings{lihong2006towards,
	author    = {Lihong Li and
	Thomas J. Walsh and
	Michael L. Littman},
	title     = {Towards a Unified Theory of State Abstraction for MDPs},
	booktitle = {{ISAIM}},
	year      = {2006}
}

@article{givan2003equivalence,
	author    = {Robert Givan and
	Thomas L. Dean and
	Matthew Greig},
	title     = {Equivalence notions and model minimization in Markov decision processes},
	journal   = {Artif. Intell.},
	volume    = {147},
	number    = {1-2},
	pages     = {163--223},
	year      = {2003}
}

@book{bertsekas1996neuro,
	title={Neuro-dynamic programming},
	author={Bertsekas, Dimitri P and Tsitsiklis, John N},
	year={1996},
	publisher={Athena Scientific}
}

@techreport{peters2002policy,
	title={Policy gradient methods for control applications},
	author={Peters, Jan},
	year={2002}
}

@inproceedings{kakade2002approximately,
	author    = {Sham M. Kakade and
	John Langford},
	title     = {Approximately Optimal Approximate Reinforcement Learning},
	booktitle = {{ICML}},
	pages     = {267--274},
	publisher = {Morgan Kaufmann},
	year      = {2002}
}

@article{doro2019gradient,
	author    = {Pierluca D'Oro and
	Alberto Maria Metelli and
	Andrea Tirinzoni and
	Matteo Papini and
	Marcello Restelli},
	title     = {Gradient-Aware Model-based Policy Search},
	journal   = {CoRR},
	volume    = {abs/1909.04115},
	year      = {2019}
}

@article{recht2018tour,
	author    = {Benjamin Recht},
	title     = {A Tour of Reinforcement Learning: The View from Continuous Control},
	journal   = {CoRR},
	volume    = {abs/1806.09460},
	year      = {2018}
}

@inproceedings{sehnke2008policy,
	author    = {Frank Sehnke and
	Christian Osendorfer and
	Thomas R{\"{u}}ckstie{\ss} and
	Alex Graves and
	Jan Peters and
	J{\"{u}}rgen Schmidhuber},
	title     = {Policy Gradients with Parameter-Based Exploration for Control},
	booktitle = {{ICANN} {(1)}},
	series    = {Lecture Notes in Computer Science},
	volume    = {5163},
	pages     = {387--396},
	publisher = {Springer},
	year      = {2008}
}

@article{ray2019benchmarking,
	author    = {Alex Ray and
	Joshua Achiam and
	Dario Amodei},
	title     = {Benchmarking Safe Exploration in Deep Reinforcement Learning},
	year      = {2019},
	url = {https://openai.com/blog/safety-gym/}
}

@inproceedings{todorov2012mujoco,
	author    = {Emanuel Todorov and
	Tom Erez and
	Yuval Tassa},
	title     = {MuJoCo: {A} physics engine for model-based control},
	booktitle = {{IROS}},
	pages     = {5026--5033},
	publisher = {{IEEE}},
	year      = {2012}
}

@article{williams1992simple,
	author    = {Ronald J. Williams},
	title     = {Simple Statistical Gradient-Following Algorithms for Connectionist
	Reinforcement Learning},
	journal   = {Machine Learning},
	volume    = {8},
	pages     = {229--256},
	year      = {1992}
}

@article{peters2010pg,
author = {Peters, Jan},
year = {2010},
month = {01},
pages = {3698},
title = {Policy gradient methods},
volume = {5},
journal = {Scholarpedia},
doi = {10.4249/scholarpedia.3698}
}

@article{zhao2013efficient,
author = {Zhao, Tingting and Hachiya, Hirotaka and Tangkaratt, Voot and Morimoto, Jun and Sugiyama, Masashi},
year = {2013},
month = {03},
pages = {},
title = {Efficient Sample Reuse in Policy Gradients with Parameter-Based Exploration},
volume = {25},
journal = {Neural computation},
doi = {10.1162/NECO_a_00452}
}

@article{stulp2012pol,
author = {Stulp, Freek and Sigaud, Olivier},
year = {2012},
month = {10},
pages = {},
title = {Policy Improvement Methods: Between Black-Box Optimization and Episodic Reinforcement Learning}
}

@article{dean2013model,
author = {Dean, Thomas and Givan, Robert and Leach, Sonia},
year = {2013},
month = {02},
pages = {},
title = {Model Reduction Techniques for Computing Approximately Optimal Solutions
for Markov Decision Processes},
journal = {Proceedings of the 13th Annual Conference on Uncertainty in Artificial Intelligence (UAI-97)}
}

@article{ferns2012metrics,
author = {Ferns, Norman and Panangaden, Prakash and Precup, Doina},
year = {2012},
month = {07},
pages = {},
title = {Metrics for Finite Markov Decision Processes},
journal = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence}
}

@article{Geibel2005RiskSensitiveRL,
  title={Risk-Sensitive Reinforcement Learning Applied to Control under Constraints},
  author={Peter Geibel and Fritz Wysotzki},
  journal={ArXiv},
  year={2005},
  volume={abs/1109.2147}
}

@inproceedings{Heger1994ConsiderationOR,
  title={Consideration of Risk in Reinforcement Learning},
  author={Matthias Heger},
  booktitle={ICML},
  year={1994}
}

@article{moldovan2012safe,
author = {Moldovan, Teodor and Abbeel, Pieter},
year = {2012},
month = {05},
pages = {},
title = {Safe Exploration in Markov Decision Processes},
volume = {2},
journal = {Proceedings of the 29th International Conference on Machine Learning, ICML 2012}
}

@article{driessen2004integrating,
author = {Driessens, Kurt and Džeroski, Sašo},
year = {2004},
month = {12},
pages = {271-304},
title = {Integrating Guidance into Relational Reinforcement Learning},
volume = {57},
journal = {Machine Learning},
doi = {10.1023/B:MACH.0000039779.47329.3a}
}

@article{abbeel2010autonomous,
author = {Abbeel, Pieter and Coates, Adam and Ng, Andrew},
year = {2010},
month = {11},
pages = {1608-1639},
title = {Autonomous Helicopter Aerobatics through Apprenticeship Learning},
volume = {29},
journal = {I. J. Robotic Res.},
doi = {10.1177/0278364910371999}
}

@techreport{clouse1997on,
author = {Clouse, J.},
title = {On Integrating Apprentice Learning and Reinforcement Learning TITLE2:},
year = {1997},
publisher = {University of Massachusetts},
address = {USA}
}

@inproceedings{gehring2013smart,
author = {Gehring, Clement and Precup, Doina},
year = {2013},
month = {05},
pages = {1037-1044},
title = {Smart exploration in reinforcement learning using absolute temporal difference errors},
volume = {2},
journal = {12th International Conference on Autonomous Agents and Multiagent Systems 2013, AAMAS 2013}
}

@article{amodei2016concrete,
author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Mané, Dan},
year = {2016},
month = {06},
pages = {},
title = {Concrete Problems in AI Safety}
}

@article{thomas2019preventing,
author = {Thomas, Philip and da Silva, Bruno and Barto, Andrew and Giguere, Stephen and Brun, Yuriy and Brunskill, Emma},
year = {2019},
month = {11},
pages = {999-1004},
title = {Preventing undesirable behavior of intelligent machines},
volume = {366},
journal = {Science (New York, N.Y.)},
doi = {10.1126/science.aag3311}
}

@article{kober2013reinforcement,
	author    = {Jens Kober and
	J. Andrew Bagnell and
	Jan Peters},
	title     = {Reinforcement learning in robotics: {A} survey},
	journal   = {I. J. Robotics Res.},
	volume    = {32},
	number    = {11},
	pages     = {1238--1274},
	year      = {2013}
}

@incollection{NIPS1994_981,
title = {Reinforcement Learning with Soft State Aggregation},
author = {Satinder P. Singh and Jaakkola, Tommi and Michael I. Jordan},
booktitle = {Advances in Neural Information Processing Systems 7},
editor = {G. Tesauro and D. S. Touretzky and T. K. Leen},
pages = {361--368},
year = {1995},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/981-reinforcement-learning-with-soft-state-aggregation.pdf}
}

@unknown{yuxi2019,
author = {Li, Yuxi},
year = {2019},
month = {08},
pages = {},
title = {Reinforcement Learning Applications}
}
