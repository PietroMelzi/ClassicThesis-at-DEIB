@BOOK{bringhurst:2002,
  title = {{T}he {E}lements of {T}ypographic {S}tyle},
  publisher = {Hartley \& Marks Publishers},
  year = {2008},
  author = {Bringhurst, R. },
  series = {Version 3.2},
  address = {Point Roberts, WA, USA}
}

@ARTICLE{knuth:1974,
  author = {Knuth, D. E.},
  title = {{C}omputer {P}rogramming as an {A}rt},
  journal = {Communications of the ACM},
  year = {1974},
  volume = {17},
  pages = {667--673},
  number = {12},
  address = {New York, NY, USA},
  publisher = {ACM Press}
}

@BOOK{verne_journey:1957,
  title={Journey to the Center of the Earth},
  author={Verne, J.},
  isbn={9780758311993},
  series={Classics illustrated},
  year={1957},
  publisher={Huge Print Press}
}

@book{sutton2018reinforcement,
	title={Reinforcement learning: An introduction},
	author={Sutton, Richard S and Barto, Andrew G},
	year={2018},
	publisher={MIT press}
}

@book{puterman2014markov,
	title={Markov Decision Processes.: Discrete Stochastic Dynamic Programming},
	author={Puterman, Martin L},
	year={2014},
	publisher={John Wiley \& Sons}
}

@article{givan2000bounded,
	author    = {Robert Givan and
	Sonia M. Leach and
	Thomas L. Dean},
	title     = {Bounded-parameter Markov decision processes},
	journal   = {Artif. Intell.},
	volume    = {122},
	number    = {1-2},
	pages     = {71--109},
	year      = {2000}
}

@article{pirotta2015policy,
	author    = {Matteo Pirotta and
	Marcello Restelli and
	Luca Bascetta},
	title     = {Policy gradient in Lipschitz Markov Decision Processes},
	journal   = {Machine Learning},
	volume    = {100},
	number    = {2-3},
	pages     = {255--283},
	year      = {2015}
}

@inproceedings{rachelson2010locality,
	author    = {Emmanuel Rachelson and
	Michail G. Lagoudakis},
	title     = {On the locality of action domination in sequential decision making},
	booktitle = {{ISAIM}},
	year      = {2010}
}

@inproceedings{duan2016benchmarking,
	author    = {Yan Duan and
	Xi Chen and
	Rein Houthooft and
	John Schulman and
	Pieter Abbeel},
	title     = {Benchmarking Deep Reinforcement Learning for Continuous Control},
	booktitle = {{ICML}},
	series    = {{JMLR} Workshop and Conference Proceedings},
	volume    = {48},
	pages     = {1329--1338},
	publisher = {JMLR.org},
	year      = {2016}
}

@inproceedings{Sutton1999PolicyGM,
  title={Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  author={Richard S. Sutton and David A. McAllester and Satinder P. Singh and Yishay Mansour},
  booktitle={NIPS},
  year={1999}
}

@article{Williams1992SimpleSG,
  title={Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  author={Ronald J. Williams},
  journal={Machine Learning},
  year={1992},
  volume={8},
  pages={229-256}
}

@article{article,
author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
year = {2014},
month = {06},
pages = {},
title = {Deterministic Policy Gradient Algorithms},
volume = {1},
journal = {31st International Conference on Machine Learning, ICML 2014}
}

@inproceedings{sehnke2008PolicyGradient,
author = {Sehnke, Frank and Osendorfer, Christian and Rückstieß, Thomas and Graves, Alex and Peters, Jan and Schmidhuber, Jürgen},
year = {2008},
month = {09},
pages = {387-396},
title = {Policy Gradients with Parameter-Based Exploration for Control},
journal = {Artificial Neural Networks: ICANN 2008, 387-396 (2008)},
doi = {10.1007/978-3-540-87536-9_40}
}

@inproceedings{baek2018PathPlanning,
author = {Baek, Donghoon and Hwang, Minho and Kim, Hansoul and Kwon, Dong-Soo},
year = {2018},
month = {06},
pages = {342-347},
title = {Path Planning for Automation of Surgery Robot based on Probabilistic Roadmap and Reinforcement Learning},
doi = {10.1109/URAI.2018.8441801}
}

@book{deisenroth2013Survey,
author = {Deisenroth, Marc and Neumann, Gerhard and Peters, Jan},
year = {2013},
month = {08},
pages = {},
title = {A Survey on Policy Search for Robotics},
volume = {2}
}

@article{Peters2008ReinforcementLO,
  title={Reinforcement learning of motor skills with policy gradients},
  author={Jan Peters and Stefan Schaal},
  journal={Neural networks : the official journal of the International Neural Network Society},
  year={2008},
  volume={21 4},
  pages={
          682-97
        }
}

@article{JMLR:v16:garcia15a,
  author  = {Javier Garc{{\'i}}a and Fern and o Fern{{\'a}}ndez},
  title   = {A Comprehensive Survey on Safe Reinforcement Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2015},
  volume  = {16},
  number  = {42},
  pages   = {1437-1480},
  url     = {http://jmlr.org/papers/v16/garcia15a.html}
}

@inproceedings{turchetta2016,
author = {Turchetta, Matteo and Berkenkamp, Felix and Krause, Andreas},
title = {Safe Exploration in Finite Markov Decision Processes with Gaussian Processes},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {4312–4320},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS’16}
}

@article{DBLP:journals/corr/abs-1905-11041,
  author    = {Chuheng Zhang and
               Yuanqi Li and
               Jian Li},
  title     = {Policy Search by Target Distribution Learning for Continuous Control},
  journal   = {CoRR},
  volume    = {abs/1905.11041},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.11041},
  archivePrefix = {arXiv},
  eprint    = {1905.11041},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-11041.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@unknown{papini2019,
author = {Papini, Matteo and Pirotta, Matteo and Restelli, Marcello},
year = {2019},
month = {05},
pages = {},
title = {Smoothing Policies and Safe Policy Gradients}
}

@inproceedings{Li2006TowardsAU,
  title={Towards a Unified Theory of State Abstraction for MDPs},
  author={Lihong Li and Thomas J. Walsh and Michael L. Littman},
  booktitle={ISAIM},
  year={2006}
}

@inproceedings{lihong2006towards,
	author    = {Lihong Li and
	Thomas J. Walsh and
	Michael L. Littman},
	title     = {Towards a Unified Theory of State Abstraction for MDPs},
	booktitle = {{ISAIM}},
	year      = {2006}
}

@article{givan2003equivalence,
	author    = {Robert Givan and
	Thomas L. Dean and
	Matthew Greig},
	title     = {Equivalence notions and model minimization in Markov decision processes},
	journal   = {Artif. Intell.},
	volume    = {147},
	number    = {1-2},
	pages     = {163--223},
	year      = {2003}
}

@book{bertsekas1996neuro,
	title={Neuro-dynamic programming},
	author={Bertsekas, Dimitri P and Tsitsiklis, John N},
	year={1996},
	publisher={Athena Scientific}
}

@techreport{peters2002policy,
	title={Policy gradient methods for control applications},
	author={Peters, Jan},
	year={2002}
}
